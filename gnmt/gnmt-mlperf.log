_CUDA_COMPAT_STATUS=CUDA Driver UNAVAILABLE (cuInit(0) returned 100)
NVIDIA_PYTORCH_VERSION=19.05
MOFED_VERSION=4.4-1.0.0
COCOAPI_VERSION=2.0+nv0.3.1
CUDNN_VERSION=7.6.0.64
HOSTNAME=gnmt
DATADIR=/var/home/core/data/u1/mlperf/dataset/rnn_translator/data
NVIDIA_REQUIRE_CUDA=cuda>=5.0
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT=tcp://172.30.0.1:443
TERM=xterm
NSIGHT_SYSTEMS_VERSION=2019.3.1
CUBLAS_VERSION=10.2.0.163
LIBRARY_PATH=/usr/local/cuda/lib64/stubs:
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_HOST=172.30.0.1
NEXP=1
LC_ALL=C.UTF-8
PYTHONIOENCODING=utf-8
LD_LIBRARY_PATH=/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
NVIDIA_VISIBLE_DEVICES=all
ENV=/etc/shinit
_CUDA_COMPAT_PATH=/usr/local/cuda/compat
CUDA_CACHE_DISABLE=1
NVIDIA_DRIVER_CAPABILITIES=compute,utility
TRT_VERSION=5.1.5.0
CUDA_DRIVER_VERSION=418.67
NVIDIA_BUILD_ID=6411784
PATH=/opt/conda/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PREPROC_DATADIR=/var/home/core/data/u1/mlperf/dataset/gnmt_preproc
PWD=/workspace/rnn_translator
LANG=C.UTF-8
PYTORCH_VERSION=1.1.0a0+828a6a3
PYTORCH_BUILD_VERSION=1.1.0a0+828a6a3
CUDA_VERSION=10.1.163
OMPI_MCA_btl_vader_single_copy_mechanism=none
SHLVL=1
HOME=/root
DALI_VERSION=0.9.1
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_SERVICE_PORT_HTTPS=443
DALI_BUILD=719215
OPENMPI_VERSION=3.1.3
NCCL_VERSION=2.4.6
INSLURM=0
BASH_ENV=/etc/bash.bashrc
LOGDIR=/home/testuser/data/mlperf/logs
PYTORCH_BUILD_NUMBER=0
KUBERNETES_PORT_443_TCP_ADDR=172.30.0.1
KUBERNETES_PORT_443_TCP=tcp://172.30.0.1:443
_=/usr/bin/printenv
Run vars: id 10565 gpus 4 mparams 
STARTING TIMING RUN AT 2020-04-08 05:21:37 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.0e-3
+ TRAIN_BATCH_SIZE=256
+ TEST_BATCH_SIZE=128
+ WARMUP_STEPS=200
+ REMAIN_STEPS=6453
+ DECAY_INTERVAL=809
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --no-log-all-ranks    '
+ BIND_LAUNCH=1
+ MATH=amp_fp16
+ [[ 1 -eq 1 ]]
+ LAUNCH_OPT='bind_launch  --nsockets_per_node 2  --ncores_per_socket 16 --nproc_per_node 4 '
running benchmark
+ echo 'running benchmark'
+ python3 -m bind_launch --nsockets_per_node 2 --ncores_per_socket 16 --nproc_per_node 4 train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data --target-bleu 24.0 --epochs 8 --math amp_fp16 --max-length-train 75 --print-freq 10 --train-batch-size 256 --test-batch-size 128 --optimizer FusedAdam --lr 2.0e-3 --warmup-steps 200 --remain-steps 6453 --decay-interval 809 --fused-attention --fused-xentropy --no-log-all-ranks
:::MLL 1586323298.927 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1586323298.958 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1586323298.963 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
:::MLL 1586323298.968 init_start: {"value": null, "metadata": {"file": "train.py", "lineno": 287}}
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=809, decay_steps=4, dropout=0.2, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002, math='amp_fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='always', preproc_data_dir='/preproc_data', print_freq=10, rank=0, remain_steps=6453, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=128, test_loader_workers=0, train_batch_size=256, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3814678157
0: Worker 0 is using worker seed: 1073310145
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLL 1586323307.920 opt_base_learning_rate: {"value": 0.002, "metadata": {"file": "seq2seq/train/trainer.py", "lineno": 169}}
0: Executing preallocation
:::MLL 1586323308.786 init_stop: {"value": null, "metadata": {"file": "train.py", "lineno": 406}}
:::MLL 1586323308.786 run_start: {"value": null, "metadata": {"file": "train.py", "lineno": 408}}
:::MLL 1586323308.787 max_sequence_length: {"value": 75, "metadata": {"method": "discard", "file": "train.py", "lineno": 413}}
0: Reading preprocessed data file from /preproc_data/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75 
0: Opening preprocessed data /preproc_data/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLL 1586323309.266 global_batch_size: {"value": 1024, "metadata": {"file": "train.py", "lineno": 458}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 6453, 'decay_interval': 809, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 6453
0: Scheduler decay interval: 809
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLL 1586323309.268 opt_learning_rate_alt_decay_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLL 1586323309.268 opt_learning_rate_alt_warmup_func: {"value": true, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLL 1586323309.269 opt_learning_rate_decay_interval: {"value": 809, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLL 1586323309.269 opt_learning_rate_decay_factor: {"value": 0.5, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLL 1586323309.269 opt_learning_rate_decay_steps: {"value": 4, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLL 1586323309.270 opt_learning_rate_remain_steps: {"value": 6453, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLL 1586323309.270 opt_learning_rate_warmup_steps: {"value": 200, "metadata": {"file": "seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLL 1586323309.570 block_start: {"value": null, "metadata": {"first_epoch_num": 1, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1586323309.572 epoch_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 514}}
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 812456501
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][0/3880]	Time 0.808 (0.808)	Data 4.52e-01 (4.52e-01)	Tok/s 12903 (12903)	Loss/tok 10.6010 (10.6010)	LR 2.000e-05
0: TRAIN [0][10/3880]	Time 0.231 (0.248)	Data 1.17e-04 (4.15e-02)	Tok/s 100555 (77268)	Loss/tok 9.8425 (10.1409)	LR 2.518e-05
0: TRAIN [0][20/3880]	Time 0.235 (0.205)	Data 9.68e-05 (2.18e-02)	Tok/s 99496 (84313)	Loss/tok 9.4409 (9.8513)	LR 3.170e-05
0: TRAIN [0][30/3880]	Time 0.176 (0.191)	Data 1.06e-04 (1.48e-02)	Tok/s 95729 (86667)	Loss/tok 9.0554 (9.6301)	LR 3.991e-05
0: TRAIN [0][40/3880]	Time 0.120 (0.179)	Data 1.34e-04 (1.12e-02)	Tok/s 85353 (87317)	Loss/tok 8.7110 (9.4660)	LR 5.024e-05
0: TRAIN [0][50/3880]	Time 0.303 (0.179)	Data 1.16e-04 (9.03e-03)	Tok/s 98032 (88450)	Loss/tok 8.6878 (9.3007)	LR 6.325e-05
0: TRAIN [0][60/3880]	Time 0.177 (0.175)	Data 1.18e-04 (7.57e-03)	Tok/s 94258 (88796)	Loss/tok 8.4399 (9.1728)	LR 7.962e-05
0: TRAIN [0][70/3880]	Time 0.235 (0.171)	Data 1.52e-04 (6.52e-03)	Tok/s 97989 (88862)	Loss/tok 8.4351 (9.0642)	LR 1.002e-04
0: TRAIN [0][80/3880]	Time 0.132 (0.171)	Data 9.89e-05 (5.73e-03)	Tok/s 80293 (89189)	Loss/tok 8.0059 (8.9466)	LR 1.262e-04
0: TRAIN [0][90/3880]	Time 0.120 (0.172)	Data 9.73e-05 (5.11e-03)	Tok/s 85381 (89518)	Loss/tok 7.8444 (8.8412)	LR 1.589e-04
0: TRAIN [0][100/3880]	Time 0.235 (0.172)	Data 1.37e-04 (4.62e-03)	Tok/s 99689 (89669)	Loss/tok 8.0781 (8.7567)	LR 2.000e-04
0: TRAIN [0][110/3880]	Time 0.120 (0.171)	Data 1.26e-04 (4.21e-03)	Tok/s 85729 (89686)	Loss/tok 7.8440 (8.6867)	LR 2.518e-04
0: TRAIN [0][120/3880]	Time 0.120 (0.170)	Data 1.13e-04 (3.88e-03)	Tok/s 86428 (89725)	Loss/tok 7.7585 (8.6263)	LR 3.170e-04
0: TRAIN [0][130/3880]	Time 0.121 (0.168)	Data 1.35e-04 (3.59e-03)	Tok/s 85322 (89743)	Loss/tok 7.6471 (8.5810)	LR 3.991e-04
0: TRAIN [0][140/3880]	Time 0.122 (0.169)	Data 1.15e-04 (3.34e-03)	Tok/s 83696 (89901)	Loss/tok 7.6477 (8.5309)	LR 5.024e-04
0: TRAIN [0][150/3880]	Time 0.176 (0.171)	Data 1.21e-04 (3.13e-03)	Tok/s 93755 (90143)	Loss/tok 7.9282 (8.4793)	LR 6.325e-04
0: TRAIN [0][160/3880]	Time 0.177 (0.171)	Data 1.35e-04 (2.94e-03)	Tok/s 96424 (90216)	Loss/tok 7.8082 (8.4348)	LR 7.962e-04
0: TRAIN [0][170/3880]	Time 0.180 (0.170)	Data 1.25e-04 (2.78e-03)	Tok/s 94833 (90254)	Loss/tok 7.6242 (8.3880)	LR 1.002e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][180/3880]	Time 0.176 (0.170)	Data 1.07e-04 (2.63e-03)	Tok/s 94539 (90231)	Loss/tok 7.5028 (8.3421)	LR 1.233e-03
0: TRAIN [0][190/3880]	Time 0.176 (0.168)	Data 1.28e-04 (2.50e-03)	Tok/s 95253 (90023)	Loss/tok 7.3814 (8.2971)	LR 1.552e-03
0: TRAIN [0][200/3880]	Time 0.120 (0.168)	Data 1.16e-04 (2.38e-03)	Tok/s 84640 (89985)	Loss/tok 7.0365 (8.2477)	LR 1.954e-03
0: TRAIN [0][210/3880]	Time 0.123 (0.167)	Data 1.12e-04 (2.27e-03)	Tok/s 83230 (89936)	Loss/tok 6.8775 (8.1984)	LR 2.000e-03
0: TRAIN [0][220/3880]	Time 0.238 (0.167)	Data 1.50e-04 (2.18e-03)	Tok/s 98232 (90014)	Loss/tok 7.0641 (8.1393)	LR 2.000e-03
0: TRAIN [0][230/3880]	Time 0.123 (0.168)	Data 1.24e-04 (2.09e-03)	Tok/s 84439 (90016)	Loss/tok 6.6027 (8.0792)	LR 2.000e-03
0: TRAIN [0][240/3880]	Time 0.124 (0.168)	Data 1.08e-04 (2.00e-03)	Tok/s 84187 (90125)	Loss/tok 6.3963 (8.0166)	LR 2.000e-03
0: TRAIN [0][250/3880]	Time 0.180 (0.168)	Data 1.17e-04 (1.93e-03)	Tok/s 93442 (90115)	Loss/tok 6.6630 (7.9609)	LR 2.000e-03
0: TRAIN [0][260/3880]	Time 0.122 (0.169)	Data 1.24e-04 (1.86e-03)	Tok/s 82572 (90130)	Loss/tok 6.2172 (7.8976)	LR 2.000e-03
0: TRAIN [0][270/3880]	Time 0.125 (0.169)	Data 9.78e-05 (1.79e-03)	Tok/s 82289 (90095)	Loss/tok 6.0627 (7.8403)	LR 2.000e-03
0: TRAIN [0][280/3880]	Time 0.125 (0.169)	Data 1.21e-04 (1.73e-03)	Tok/s 81014 (90094)	Loss/tok 5.9606 (7.7834)	LR 2.000e-03
0: TRAIN [0][290/3880]	Time 0.122 (0.169)	Data 1.09e-04 (1.68e-03)	Tok/s 83782 (90069)	Loss/tok 5.9374 (7.7293)	LR 2.000e-03
0: TRAIN [0][300/3880]	Time 0.124 (0.169)	Data 1.04e-04 (1.63e-03)	Tok/s 82627 (90043)	Loss/tok 5.8643 (7.6786)	LR 2.000e-03
0: TRAIN [0][310/3880]	Time 0.123 (0.168)	Data 9.92e-05 (1.58e-03)	Tok/s 85046 (89934)	Loss/tok 5.7544 (7.6307)	LR 2.000e-03
0: TRAIN [0][320/3880]	Time 0.122 (0.167)	Data 9.58e-05 (1.53e-03)	Tok/s 85022 (89860)	Loss/tok 5.6564 (7.5829)	LR 2.000e-03
0: TRAIN [0][330/3880]	Time 0.123 (0.167)	Data 1.16e-04 (1.49e-03)	Tok/s 83215 (89861)	Loss/tok 5.5581 (7.5308)	LR 2.000e-03
0: TRAIN [0][340/3880]	Time 0.180 (0.168)	Data 1.01e-04 (1.45e-03)	Tok/s 93801 (89837)	Loss/tok 5.7605 (7.4792)	LR 2.000e-03
0: TRAIN [0][350/3880]	Time 0.238 (0.168)	Data 1.07e-04 (1.41e-03)	Tok/s 97239 (89842)	Loss/tok 5.8859 (7.4274)	LR 2.000e-03
0: TRAIN [0][360/3880]	Time 0.239 (0.167)	Data 8.96e-05 (1.37e-03)	Tok/s 96275 (89789)	Loss/tok 5.9218 (7.3809)	LR 2.000e-03
0: TRAIN [0][370/3880]	Time 0.238 (0.168)	Data 1.22e-04 (1.34e-03)	Tok/s 98312 (89781)	Loss/tok 5.8623 (7.3308)	LR 2.000e-03
0: TRAIN [0][380/3880]	Time 0.181 (0.167)	Data 9.49e-05 (1.31e-03)	Tok/s 93885 (89734)	Loss/tok 5.5363 (7.2879)	LR 2.000e-03
0: TRAIN [0][390/3880]	Time 0.123 (0.168)	Data 1.18e-04 (1.28e-03)	Tok/s 85980 (89765)	Loss/tok 5.3221 (7.2371)	LR 2.000e-03
0: TRAIN [0][400/3880]	Time 0.125 (0.168)	Data 1.29e-04 (1.25e-03)	Tok/s 82569 (89805)	Loss/tok 5.0780 (7.1860)	LR 2.000e-03
0: TRAIN [0][410/3880]	Time 0.237 (0.168)	Data 9.97e-05 (1.22e-03)	Tok/s 97386 (89712)	Loss/tok 5.7578 (7.1486)	LR 2.000e-03
0: TRAIN [0][420/3880]	Time 0.181 (0.167)	Data 1.02e-04 (1.19e-03)	Tok/s 92400 (89663)	Loss/tok 5.4073 (7.1077)	LR 2.000e-03
0: TRAIN [0][430/3880]	Time 0.123 (0.167)	Data 1.15e-04 (1.17e-03)	Tok/s 83474 (89568)	Loss/tok 4.8842 (7.0703)	LR 2.000e-03
0: TRAIN [0][440/3880]	Time 0.125 (0.167)	Data 9.85e-05 (1.14e-03)	Tok/s 82494 (89546)	Loss/tok 4.8068 (7.0274)	LR 2.000e-03
0: TRAIN [0][450/3880]	Time 0.123 (0.166)	Data 9.01e-05 (1.12e-03)	Tok/s 85947 (89474)	Loss/tok 4.7306 (6.9903)	LR 2.000e-03
0: TRAIN [0][460/3880]	Time 0.241 (0.166)	Data 9.89e-05 (1.10e-03)	Tok/s 96088 (89519)	Loss/tok 5.1619 (6.9443)	LR 2.000e-03
0: TRAIN [0][470/3880]	Time 0.241 (0.166)	Data 9.37e-05 (1.08e-03)	Tok/s 95594 (89507)	Loss/tok 5.1530 (6.9013)	LR 2.000e-03
0: TRAIN [0][480/3880]	Time 0.123 (0.167)	Data 8.65e-05 (1.06e-03)	Tok/s 84434 (89508)	Loss/tok 4.6497 (6.8613)	LR 2.000e-03
0: TRAIN [0][490/3880]	Time 0.123 (0.166)	Data 8.96e-05 (1.04e-03)	Tok/s 85088 (89440)	Loss/tok 4.5991 (6.8267)	LR 2.000e-03
0: TRAIN [0][500/3880]	Time 0.124 (0.166)	Data 1.01e-04 (1.02e-03)	Tok/s 85609 (89449)	Loss/tok 4.6024 (6.7839)	LR 2.000e-03
0: TRAIN [0][510/3880]	Time 0.243 (0.166)	Data 9.89e-05 (1.00e-03)	Tok/s 95356 (89426)	Loss/tok 5.0811 (6.7462)	LR 2.000e-03
0: TRAIN [0][520/3880]	Time 0.242 (0.167)	Data 1.40e-04 (9.83e-04)	Tok/s 97307 (89472)	Loss/tok 5.0458 (6.7046)	LR 2.000e-03
0: TRAIN [0][530/3880]	Time 0.124 (0.166)	Data 9.01e-05 (9.67e-04)	Tok/s 83417 (89402)	Loss/tok 4.3903 (6.6719)	LR 2.000e-03
0: TRAIN [0][540/3880]	Time 0.182 (0.166)	Data 9.68e-05 (9.51e-04)	Tok/s 91703 (89342)	Loss/tok 4.7375 (6.6404)	LR 2.000e-03
0: TRAIN [0][550/3880]	Time 0.124 (0.166)	Data 9.61e-05 (9.35e-04)	Tok/s 82090 (89383)	Loss/tok 4.4675 (6.5984)	LR 2.000e-03
0: TRAIN [0][560/3880]	Time 0.124 (0.166)	Data 9.18e-05 (9.20e-04)	Tok/s 83124 (89358)	Loss/tok 4.2783 (6.5649)	LR 2.000e-03
0: TRAIN [0][570/3880]	Time 0.067 (0.166)	Data 9.37e-05 (9.06e-04)	Tok/s 78211 (89267)	Loss/tok 3.4796 (6.5381)	LR 2.000e-03
0: TRAIN [0][580/3880]	Time 0.243 (0.166)	Data 1.09e-04 (8.92e-04)	Tok/s 96145 (89264)	Loss/tok 4.7602 (6.5041)	LR 2.000e-03
0: TRAIN [0][590/3880]	Time 0.125 (0.165)	Data 1.36e-04 (8.79e-04)	Tok/s 83720 (89209)	Loss/tok 4.2451 (6.4747)	LR 2.000e-03
0: TRAIN [0][600/3880]	Time 0.184 (0.165)	Data 1.06e-04 (8.66e-04)	Tok/s 91257 (89194)	Loss/tok 4.5993 (6.4433)	LR 2.000e-03
0: TRAIN [0][610/3880]	Time 0.125 (0.165)	Data 9.66e-05 (8.53e-04)	Tok/s 83254 (89138)	Loss/tok 4.2093 (6.4129)	LR 2.000e-03
0: TRAIN [0][620/3880]	Time 0.126 (0.165)	Data 9.47e-05 (8.41e-04)	Tok/s 81581 (89105)	Loss/tok 4.1579 (6.3831)	LR 2.000e-03
0: TRAIN [0][630/3880]	Time 0.181 (0.165)	Data 1.04e-04 (8.29e-04)	Tok/s 94666 (89072)	Loss/tok 4.4105 (6.3546)	LR 2.000e-03
0: TRAIN [0][640/3880]	Time 0.066 (0.164)	Data 1.02e-04 (8.18e-04)	Tok/s 80121 (88991)	Loss/tok 3.4821 (6.3303)	LR 2.000e-03
0: TRAIN [0][650/3880]	Time 0.312 (0.164)	Data 1.49e-04 (8.08e-04)	Tok/s 96549 (88953)	Loss/tok 4.7286 (6.3026)	LR 2.000e-03
0: TRAIN [0][660/3880]	Time 0.123 (0.164)	Data 1.19e-04 (7.97e-04)	Tok/s 83386 (88934)	Loss/tok 4.1261 (6.2751)	LR 2.000e-03
0: TRAIN [0][670/3880]	Time 0.243 (0.164)	Data 1.14e-04 (7.87e-04)	Tok/s 95931 (88916)	Loss/tok 4.7550 (6.2482)	LR 2.000e-03
0: TRAIN [0][680/3880]	Time 0.312 (0.164)	Data 1.26e-04 (7.77e-04)	Tok/s 95735 (88911)	Loss/tok 4.8596 (6.2194)	LR 2.000e-03
0: TRAIN [0][690/3880]	Time 0.125 (0.164)	Data 1.11e-04 (7.67e-04)	Tok/s 81236 (88842)	Loss/tok 4.0994 (6.1966)	LR 2.000e-03
0: TRAIN [0][700/3880]	Time 0.185 (0.163)	Data 1.50e-04 (7.58e-04)	Tok/s 88180 (88795)	Loss/tok 4.3931 (6.1740)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][710/3880]	Time 0.182 (0.164)	Data 1.28e-04 (7.49e-04)	Tok/s 92134 (88811)	Loss/tok 4.3587 (6.1451)	LR 2.000e-03
0: TRAIN [0][720/3880]	Time 0.182 (0.164)	Data 1.23e-04 (7.40e-04)	Tok/s 92451 (88802)	Loss/tok 4.3967 (6.1197)	LR 2.000e-03
0: TRAIN [0][730/3880]	Time 0.315 (0.164)	Data 1.50e-04 (7.32e-04)	Tok/s 92999 (88778)	Loss/tok 4.9178 (6.0938)	LR 2.000e-03
0: TRAIN [0][740/3880]	Time 0.067 (0.164)	Data 9.85e-05 (7.23e-04)	Tok/s 78072 (88744)	Loss/tok 3.4672 (6.0715)	LR 2.000e-03
0: TRAIN [0][750/3880]	Time 0.184 (0.164)	Data 1.13e-04 (7.15e-04)	Tok/s 91171 (88742)	Loss/tok 4.3086 (6.0468)	LR 2.000e-03
0: TRAIN [0][760/3880]	Time 0.246 (0.164)	Data 1.15e-04 (7.07e-04)	Tok/s 95668 (88735)	Loss/tok 4.4214 (6.0218)	LR 2.000e-03
0: TRAIN [0][770/3880]	Time 0.240 (0.164)	Data 1.18e-04 (7.00e-04)	Tok/s 96127 (88714)	Loss/tok 4.5404 (5.9991)	LR 2.000e-03
0: TRAIN [0][780/3880]	Time 0.245 (0.164)	Data 1.24e-04 (6.92e-04)	Tok/s 95268 (88667)	Loss/tok 4.5049 (5.9788)	LR 2.000e-03
0: TRAIN [0][790/3880]	Time 0.123 (0.164)	Data 1.11e-04 (6.85e-04)	Tok/s 81111 (88653)	Loss/tok 3.8933 (5.9563)	LR 2.000e-03
0: TRAIN [0][800/3880]	Time 0.066 (0.164)	Data 1.02e-04 (6.78e-04)	Tok/s 79473 (88654)	Loss/tok 3.3038 (5.9345)	LR 2.000e-03
0: TRAIN [0][810/3880]	Time 0.317 (0.164)	Data 1.05e-04 (6.71e-04)	Tok/s 94161 (88667)	Loss/tok 4.6556 (5.9102)	LR 2.000e-03
0: TRAIN [0][820/3880]	Time 0.124 (0.164)	Data 1.05e-04 (6.64e-04)	Tok/s 83932 (88657)	Loss/tok 4.0105 (5.8903)	LR 2.000e-03
0: TRAIN [0][830/3880]	Time 0.125 (0.164)	Data 1.15e-04 (6.57e-04)	Tok/s 82150 (88632)	Loss/tok 3.9407 (5.8717)	LR 2.000e-03
0: TRAIN [0][840/3880]	Time 0.180 (0.164)	Data 1.32e-04 (6.51e-04)	Tok/s 92407 (88619)	Loss/tok 4.2512 (5.8516)	LR 2.000e-03
0: TRAIN [0][850/3880]	Time 0.124 (0.164)	Data 1.38e-04 (6.45e-04)	Tok/s 82822 (88632)	Loss/tok 3.7743 (5.8307)	LR 2.000e-03
0: TRAIN [0][860/3880]	Time 0.126 (0.164)	Data 1.22e-04 (6.39e-04)	Tok/s 81901 (88628)	Loss/tok 3.8333 (5.8106)	LR 2.000e-03
0: TRAIN [0][870/3880]	Time 0.067 (0.164)	Data 1.26e-04 (6.33e-04)	Tok/s 81064 (88614)	Loss/tok 3.3278 (5.7923)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][880/3880]	Time 0.182 (0.165)	Data 1.18e-04 (6.27e-04)	Tok/s 91423 (88630)	Loss/tok 4.1050 (5.7717)	LR 2.000e-03
0: TRAIN [0][890/3880]	Time 0.123 (0.164)	Data 1.09e-04 (6.22e-04)	Tok/s 85245 (88636)	Loss/tok 3.8637 (5.7530)	LR 2.000e-03
0: TRAIN [0][900/3880]	Time 0.243 (0.165)	Data 1.09e-04 (6.16e-04)	Tok/s 95398 (88649)	Loss/tok 4.4164 (5.7338)	LR 2.000e-03
0: TRAIN [0][910/3880]	Time 0.181 (0.165)	Data 1.14e-04 (6.11e-04)	Tok/s 93978 (88645)	Loss/tok 4.0636 (5.7164)	LR 2.000e-03
0: TRAIN [0][920/3880]	Time 0.067 (0.165)	Data 1.16e-04 (6.05e-04)	Tok/s 77266 (88663)	Loss/tok 3.2869 (5.6970)	LR 2.000e-03
0: TRAIN [0][930/3880]	Time 0.243 (0.165)	Data 1.11e-04 (6.00e-04)	Tok/s 96759 (88673)	Loss/tok 4.3602 (5.6789)	LR 2.000e-03
0: TRAIN [0][940/3880]	Time 0.123 (0.165)	Data 1.18e-04 (5.95e-04)	Tok/s 84655 (88651)	Loss/tok 3.7772 (5.6630)	LR 2.000e-03
0: TRAIN [0][950/3880]	Time 0.125 (0.165)	Data 1.16e-04 (5.90e-04)	Tok/s 81140 (88657)	Loss/tok 3.8071 (5.6455)	LR 2.000e-03
0: TRAIN [0][960/3880]	Time 0.067 (0.165)	Data 1.12e-04 (5.85e-04)	Tok/s 79518 (88652)	Loss/tok 3.2673 (5.6289)	LR 2.000e-03
0: TRAIN [0][970/3880]	Time 0.244 (0.165)	Data 1.14e-04 (5.80e-04)	Tok/s 95101 (88670)	Loss/tok 4.3321 (5.6110)	LR 2.000e-03
0: TRAIN [0][980/3880]	Time 0.123 (0.165)	Data 1.15e-04 (5.75e-04)	Tok/s 84444 (88661)	Loss/tok 3.7647 (5.5945)	LR 2.000e-03
0: TRAIN [0][990/3880]	Time 0.243 (0.165)	Data 1.09e-04 (5.70e-04)	Tok/s 95618 (88678)	Loss/tok 4.2394 (5.5771)	LR 2.000e-03
0: TRAIN [0][1000/3880]	Time 0.246 (0.165)	Data 1.16e-04 (5.66e-04)	Tok/s 96386 (88674)	Loss/tok 4.2187 (5.5614)	LR 2.000e-03
0: TRAIN [0][1010/3880]	Time 0.124 (0.165)	Data 1.33e-04 (5.62e-04)	Tok/s 82930 (88665)	Loss/tok 3.7074 (5.5474)	LR 2.000e-03
0: TRAIN [0][1020/3880]	Time 0.125 (0.165)	Data 1.14e-04 (5.57e-04)	Tok/s 82328 (88661)	Loss/tok 3.8849 (5.5326)	LR 2.000e-03
0: TRAIN [0][1030/3880]	Time 0.123 (0.165)	Data 1.09e-04 (5.53e-04)	Tok/s 85069 (88657)	Loss/tok 3.7088 (5.5179)	LR 2.000e-03
0: TRAIN [0][1040/3880]	Time 0.185 (0.165)	Data 1.23e-04 (5.49e-04)	Tok/s 89447 (88638)	Loss/tok 4.0533 (5.5040)	LR 2.000e-03
0: TRAIN [0][1050/3880]	Time 0.123 (0.165)	Data 1.26e-04 (5.45e-04)	Tok/s 84326 (88643)	Loss/tok 3.7152 (5.4886)	LR 2.000e-03
0: TRAIN [0][1060/3880]	Time 0.241 (0.165)	Data 1.28e-04 (5.41e-04)	Tok/s 97351 (88649)	Loss/tok 4.1945 (5.4729)	LR 2.000e-03
0: TRAIN [0][1070/3880]	Time 0.185 (0.165)	Data 1.20e-04 (5.37e-04)	Tok/s 92176 (88641)	Loss/tok 3.9852 (5.4595)	LR 2.000e-03
0: TRAIN [0][1080/3880]	Time 0.126 (0.166)	Data 1.02e-04 (5.33e-04)	Tok/s 80957 (88670)	Loss/tok 3.7258 (5.4438)	LR 2.000e-03
0: TRAIN [0][1090/3880]	Time 0.180 (0.166)	Data 1.12e-04 (5.29e-04)	Tok/s 93213 (88660)	Loss/tok 3.9792 (5.4304)	LR 2.000e-03
0: TRAIN [0][1100/3880]	Time 0.126 (0.165)	Data 1.20e-04 (5.25e-04)	Tok/s 81880 (88650)	Loss/tok 3.7452 (5.4176)	LR 2.000e-03
0: TRAIN [0][1110/3880]	Time 0.241 (0.166)	Data 1.13e-04 (5.21e-04)	Tok/s 95673 (88667)	Loss/tok 4.2533 (5.4029)	LR 2.000e-03
0: TRAIN [0][1120/3880]	Time 0.124 (0.166)	Data 1.09e-04 (5.18e-04)	Tok/s 84398 (88650)	Loss/tok 3.7496 (5.3907)	LR 2.000e-03
0: TRAIN [0][1130/3880]	Time 0.183 (0.166)	Data 1.16e-04 (5.14e-04)	Tok/s 90479 (88643)	Loss/tok 4.0212 (5.3783)	LR 2.000e-03
0: TRAIN [0][1140/3880]	Time 0.124 (0.166)	Data 1.13e-04 (5.11e-04)	Tok/s 83234 (88648)	Loss/tok 3.6109 (5.3647)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1150/3880]	Time 0.124 (0.165)	Data 1.11e-04 (5.07e-04)	Tok/s 84837 (88638)	Loss/tok 3.6892 (5.3533)	LR 2.000e-03
0: TRAIN [0][1160/3880]	Time 0.242 (0.166)	Data 1.05e-04 (5.04e-04)	Tok/s 95640 (88667)	Loss/tok 4.2729 (5.3399)	LR 2.000e-03
0: TRAIN [0][1170/3880]	Time 0.313 (0.166)	Data 1.16e-04 (5.01e-04)	Tok/s 95806 (88668)	Loss/tok 4.3602 (5.3271)	LR 2.000e-03
0: TRAIN [0][1180/3880]	Time 0.123 (0.166)	Data 1.42e-04 (4.98e-04)	Tok/s 84226 (88655)	Loss/tok 3.5508 (5.3158)	LR 2.000e-03
0: TRAIN [0][1190/3880]	Time 0.068 (0.165)	Data 1.29e-04 (4.95e-04)	Tok/s 78062 (88645)	Loss/tok 3.0390 (5.3047)	LR 2.000e-03
0: TRAIN [0][1200/3880]	Time 0.315 (0.166)	Data 1.06e-04 (4.91e-04)	Tok/s 95811 (88666)	Loss/tok 4.2838 (5.2911)	LR 2.000e-03
0: TRAIN [0][1210/3880]	Time 0.243 (0.166)	Data 9.63e-05 (4.88e-04)	Tok/s 95449 (88647)	Loss/tok 4.3132 (5.2812)	LR 2.000e-03
0: TRAIN [0][1220/3880]	Time 0.182 (0.166)	Data 1.06e-04 (4.85e-04)	Tok/s 92476 (88646)	Loss/tok 3.8785 (5.2702)	LR 2.000e-03
0: TRAIN [0][1230/3880]	Time 0.123 (0.165)	Data 1.02e-04 (4.82e-04)	Tok/s 83739 (88629)	Loss/tok 3.5829 (5.2598)	LR 2.000e-03
0: TRAIN [0][1240/3880]	Time 0.181 (0.165)	Data 1.18e-04 (4.79e-04)	Tok/s 92765 (88617)	Loss/tok 4.0009 (5.2496)	LR 2.000e-03
0: TRAIN [0][1250/3880]	Time 0.123 (0.165)	Data 1.36e-04 (4.76e-04)	Tok/s 84337 (88615)	Loss/tok 3.6067 (5.2375)	LR 2.000e-03
0: TRAIN [0][1260/3880]	Time 0.123 (0.165)	Data 1.26e-04 (4.74e-04)	Tok/s 82940 (88594)	Loss/tok 3.5657 (5.2278)	LR 2.000e-03
0: TRAIN [0][1270/3880]	Time 0.124 (0.165)	Data 9.44e-05 (4.71e-04)	Tok/s 83244 (88592)	Loss/tok 3.7462 (5.2170)	LR 2.000e-03
0: TRAIN [0][1280/3880]	Time 0.125 (0.165)	Data 9.16e-05 (4.68e-04)	Tok/s 82657 (88563)	Loss/tok 3.6321 (5.2081)	LR 2.000e-03
0: TRAIN [0][1290/3880]	Time 0.123 (0.165)	Data 9.49e-05 (4.65e-04)	Tok/s 84186 (88568)	Loss/tok 3.5936 (5.1969)	LR 2.000e-03
0: TRAIN [0][1300/3880]	Time 0.125 (0.165)	Data 1.15e-04 (4.62e-04)	Tok/s 84229 (88564)	Loss/tok 3.5578 (5.1869)	LR 2.000e-03
0: TRAIN [0][1310/3880]	Time 0.125 (0.165)	Data 1.13e-04 (4.60e-04)	Tok/s 82438 (88533)	Loss/tok 3.6031 (5.1783)	LR 2.000e-03
0: TRAIN [0][1320/3880]	Time 0.316 (0.165)	Data 1.25e-04 (4.57e-04)	Tok/s 95146 (88500)	Loss/tok 4.1918 (5.1697)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1330/3880]	Time 0.126 (0.165)	Data 1.06e-04 (4.54e-04)	Tok/s 80757 (88470)	Loss/tok 3.5929 (5.1602)	LR 2.000e-03
0: TRAIN [0][1340/3880]	Time 0.125 (0.165)	Data 1.02e-04 (4.52e-04)	Tok/s 83614 (88453)	Loss/tok 3.5190 (5.1510)	LR 2.000e-03
0: TRAIN [0][1350/3880]	Time 0.182 (0.165)	Data 1.18e-04 (4.49e-04)	Tok/s 92926 (88457)	Loss/tok 3.9599 (5.1416)	LR 2.000e-03
0: TRAIN [0][1360/3880]	Time 0.124 (0.165)	Data 1.11e-04 (4.47e-04)	Tok/s 83462 (88449)	Loss/tok 3.4744 (5.1323)	LR 2.000e-03
0: TRAIN [0][1370/3880]	Time 0.123 (0.165)	Data 1.32e-04 (4.44e-04)	Tok/s 83606 (88441)	Loss/tok 3.7403 (5.1229)	LR 2.000e-03
0: TRAIN [0][1380/3880]	Time 0.180 (0.165)	Data 9.56e-05 (4.42e-04)	Tok/s 92907 (88454)	Loss/tok 3.8666 (5.1127)	LR 2.000e-03
0: TRAIN [0][1390/3880]	Time 0.124 (0.165)	Data 1.12e-04 (4.40e-04)	Tok/s 81645 (88440)	Loss/tok 3.7277 (5.1040)	LR 2.000e-03
0: TRAIN [0][1400/3880]	Time 0.182 (0.165)	Data 1.15e-04 (4.37e-04)	Tok/s 91472 (88438)	Loss/tok 3.8935 (5.0948)	LR 2.000e-03
0: TRAIN [0][1410/3880]	Time 0.066 (0.164)	Data 1.37e-04 (4.35e-04)	Tok/s 80048 (88411)	Loss/tok 2.8864 (5.0875)	LR 2.000e-03
0: TRAIN [0][1420/3880]	Time 0.184 (0.164)	Data 1.28e-04 (4.33e-04)	Tok/s 91317 (88420)	Loss/tok 4.0323 (5.0780)	LR 2.000e-03
0: TRAIN [0][1430/3880]	Time 0.064 (0.164)	Data 1.24e-04 (4.31e-04)	Tok/s 81026 (88401)	Loss/tok 2.9744 (5.0702)	LR 2.000e-03
0: TRAIN [0][1440/3880]	Time 0.123 (0.164)	Data 1.18e-04 (4.28e-04)	Tok/s 84669 (88387)	Loss/tok 3.5632 (5.0621)	LR 2.000e-03
0: TRAIN [0][1450/3880]	Time 0.181 (0.164)	Data 1.26e-04 (4.26e-04)	Tok/s 91936 (88378)	Loss/tok 3.8572 (5.0539)	LR 2.000e-03
0: TRAIN [0][1460/3880]	Time 0.187 (0.164)	Data 1.19e-04 (4.24e-04)	Tok/s 89575 (88392)	Loss/tok 3.7701 (5.0443)	LR 2.000e-03
0: TRAIN [0][1470/3880]	Time 0.125 (0.164)	Data 1.13e-04 (4.22e-04)	Tok/s 82345 (88371)	Loss/tok 3.5369 (5.0372)	LR 2.000e-03
0: TRAIN [0][1480/3880]	Time 0.125 (0.164)	Data 1.18e-04 (4.20e-04)	Tok/s 82756 (88371)	Loss/tok 3.4271 (5.0284)	LR 2.000e-03
0: TRAIN [0][1490/3880]	Time 0.182 (0.164)	Data 1.19e-04 (4.18e-04)	Tok/s 92286 (88364)	Loss/tok 3.8021 (5.0209)	LR 2.000e-03
0: TRAIN [0][1500/3880]	Time 0.183 (0.164)	Data 1.01e-04 (4.16e-04)	Tok/s 91926 (88360)	Loss/tok 3.7692 (5.0126)	LR 2.000e-03
0: TRAIN [0][1510/3880]	Time 0.181 (0.164)	Data 1.44e-04 (4.14e-04)	Tok/s 92646 (88384)	Loss/tok 3.8611 (5.0033)	LR 2.000e-03
0: TRAIN [0][1520/3880]	Time 0.183 (0.164)	Data 1.07e-04 (4.12e-04)	Tok/s 92242 (88406)	Loss/tok 3.7674 (4.9943)	LR 2.000e-03
0: TRAIN [0][1530/3880]	Time 0.247 (0.164)	Data 1.22e-04 (4.10e-04)	Tok/s 95538 (88398)	Loss/tok 3.9430 (4.9865)	LR 2.000e-03
0: TRAIN [0][1540/3880]	Time 0.126 (0.164)	Data 1.03e-04 (4.08e-04)	Tok/s 82578 (88384)	Loss/tok 3.5180 (4.9793)	LR 2.000e-03
0: TRAIN [0][1550/3880]	Time 0.124 (0.164)	Data 1.13e-04 (4.06e-04)	Tok/s 82388 (88381)	Loss/tok 3.6430 (4.9717)	LR 2.000e-03
0: TRAIN [0][1560/3880]	Time 0.314 (0.164)	Data 1.03e-04 (4.05e-04)	Tok/s 93920 (88397)	Loss/tok 4.2813 (4.9631)	LR 2.000e-03
0: TRAIN [0][1570/3880]	Time 0.126 (0.164)	Data 1.32e-04 (4.03e-04)	Tok/s 81918 (88386)	Loss/tok 3.5745 (4.9559)	LR 2.000e-03
0: TRAIN [0][1580/3880]	Time 0.124 (0.164)	Data 9.94e-05 (4.01e-04)	Tok/s 83615 (88359)	Loss/tok 3.4911 (4.9493)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1590/3880]	Time 0.124 (0.164)	Data 9.61e-05 (3.99e-04)	Tok/s 84216 (88367)	Loss/tok 3.4918 (4.9412)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][1600/3880]	Time 0.126 (0.164)	Data 9.75e-05 (3.97e-04)	Tok/s 81596 (88367)	Loss/tok 3.5588 (4.9337)	LR 2.000e-03
0: TRAIN [0][1610/3880]	Time 0.067 (0.164)	Data 1.18e-04 (3.96e-04)	Tok/s 79476 (88368)	Loss/tok 3.0197 (4.9265)	LR 2.000e-03
0: TRAIN [0][1620/3880]	Time 0.183 (0.165)	Data 1.40e-04 (3.94e-04)	Tok/s 92290 (88372)	Loss/tok 3.7852 (4.9189)	LR 2.000e-03
0: TRAIN [0][1630/3880]	Time 0.123 (0.164)	Data 1.02e-04 (3.92e-04)	Tok/s 83093 (88345)	Loss/tok 3.3655 (4.9125)	LR 2.000e-03
0: TRAIN [0][1640/3880]	Time 0.124 (0.164)	Data 9.58e-05 (3.90e-04)	Tok/s 82129 (88333)	Loss/tok 3.4312 (4.9058)	LR 2.000e-03
0: TRAIN [0][1650/3880]	Time 0.124 (0.164)	Data 9.56e-05 (3.89e-04)	Tok/s 81411 (88336)	Loss/tok 3.5038 (4.8984)	LR 2.000e-03
0: TRAIN [0][1660/3880]	Time 0.182 (0.164)	Data 9.20e-05 (3.87e-04)	Tok/s 92417 (88337)	Loss/tok 3.7013 (4.8912)	LR 2.000e-03
0: TRAIN [0][1670/3880]	Time 0.181 (0.164)	Data 1.00e-04 (3.85e-04)	Tok/s 93876 (88336)	Loss/tok 3.8989 (4.8844)	LR 2.000e-03
0: TRAIN [0][1680/3880]	Time 0.127 (0.164)	Data 1.15e-04 (3.84e-04)	Tok/s 81270 (88332)	Loss/tok 3.5256 (4.8774)	LR 2.000e-03
0: TRAIN [0][1690/3880]	Time 0.242 (0.164)	Data 1.32e-04 (3.82e-04)	Tok/s 96903 (88335)	Loss/tok 3.8974 (4.8702)	LR 2.000e-03
0: TRAIN [0][1700/3880]	Time 0.127 (0.164)	Data 1.03e-04 (3.80e-04)	Tok/s 81048 (88328)	Loss/tok 3.4881 (4.8636)	LR 2.000e-03
0: TRAIN [0][1710/3880]	Time 0.124 (0.164)	Data 9.63e-05 (3.79e-04)	Tok/s 84075 (88319)	Loss/tok 3.5568 (4.8572)	LR 2.000e-03
0: TRAIN [0][1720/3880]	Time 0.246 (0.165)	Data 1.50e-04 (3.77e-04)	Tok/s 95379 (88321)	Loss/tok 3.9441 (4.8506)	LR 2.000e-03
0: TRAIN [0][1730/3880]	Time 0.067 (0.165)	Data 9.97e-05 (3.76e-04)	Tok/s 75875 (88313)	Loss/tok 2.8847 (4.8443)	LR 2.000e-03
0: TRAIN [0][1740/3880]	Time 0.123 (0.165)	Data 9.23e-05 (3.74e-04)	Tok/s 83428 (88311)	Loss/tok 3.6413 (4.8381)	LR 2.000e-03
0: TRAIN [0][1750/3880]	Time 0.246 (0.165)	Data 1.19e-04 (3.73e-04)	Tok/s 95331 (88310)	Loss/tok 3.7694 (4.8316)	LR 2.000e-03
0: TRAIN [0][1760/3880]	Time 0.067 (0.164)	Data 1.08e-04 (3.71e-04)	Tok/s 78040 (88291)	Loss/tok 3.0297 (4.8260)	LR 2.000e-03
0: TRAIN [0][1770/3880]	Time 0.182 (0.165)	Data 1.16e-04 (3.70e-04)	Tok/s 91707 (88302)	Loss/tok 3.7465 (4.8193)	LR 2.000e-03
0: TRAIN [0][1780/3880]	Time 0.183 (0.165)	Data 8.96e-05 (3.68e-04)	Tok/s 93358 (88303)	Loss/tok 3.6123 (4.8132)	LR 2.000e-03
0: TRAIN [0][1790/3880]	Time 0.123 (0.165)	Data 1.04e-04 (3.67e-04)	Tok/s 84019 (88307)	Loss/tok 3.5213 (4.8068)	LR 2.000e-03
0: TRAIN [0][1800/3880]	Time 0.241 (0.165)	Data 1.27e-04 (3.65e-04)	Tok/s 96013 (88314)	Loss/tok 3.9331 (4.8007)	LR 2.000e-03
0: TRAIN [0][1810/3880]	Time 0.244 (0.165)	Data 1.06e-04 (3.64e-04)	Tok/s 95546 (88303)	Loss/tok 3.7900 (4.7951)	LR 2.000e-03
0: TRAIN [0][1820/3880]	Time 0.123 (0.165)	Data 1.16e-04 (3.62e-04)	Tok/s 82387 (88310)	Loss/tok 3.5702 (4.7891)	LR 2.000e-03
0: TRAIN [0][1830/3880]	Time 0.184 (0.165)	Data 1.02e-04 (3.61e-04)	Tok/s 91582 (88299)	Loss/tok 3.7501 (4.7840)	LR 2.000e-03
0: TRAIN [0][1840/3880]	Time 0.182 (0.165)	Data 1.08e-04 (3.60e-04)	Tok/s 91772 (88300)	Loss/tok 3.8153 (4.7781)	LR 2.000e-03
0: TRAIN [0][1850/3880]	Time 0.244 (0.165)	Data 1.08e-04 (3.58e-04)	Tok/s 95183 (88302)	Loss/tok 3.8902 (4.7721)	LR 2.000e-03
0: TRAIN [0][1860/3880]	Time 0.065 (0.165)	Data 9.44e-05 (3.57e-04)	Tok/s 82436 (88291)	Loss/tok 2.9743 (4.7667)	LR 2.000e-03
0: TRAIN [0][1870/3880]	Time 0.124 (0.165)	Data 1.22e-04 (3.56e-04)	Tok/s 83636 (88284)	Loss/tok 3.3806 (4.7610)	LR 2.000e-03
0: TRAIN [0][1880/3880]	Time 0.184 (0.165)	Data 1.11e-04 (3.55e-04)	Tok/s 91240 (88288)	Loss/tok 3.8965 (4.7550)	LR 2.000e-03
0: TRAIN [0][1890/3880]	Time 0.123 (0.164)	Data 1.16e-04 (3.53e-04)	Tok/s 81875 (88280)	Loss/tok 3.4913 (4.7498)	LR 2.000e-03
0: TRAIN [0][1900/3880]	Time 0.248 (0.164)	Data 1.12e-04 (3.52e-04)	Tok/s 93137 (88277)	Loss/tok 3.8810 (4.7440)	LR 2.000e-03
0: TRAIN [0][1910/3880]	Time 0.244 (0.164)	Data 1.33e-04 (3.51e-04)	Tok/s 95412 (88262)	Loss/tok 4.0152 (4.7393)	LR 2.000e-03
0: TRAIN [0][1920/3880]	Time 0.183 (0.164)	Data 1.09e-04 (3.50e-04)	Tok/s 91397 (88266)	Loss/tok 3.7052 (4.7338)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][1930/3880]	Time 0.126 (0.164)	Data 1.02e-04 (3.48e-04)	Tok/s 82570 (88264)	Loss/tok 3.4292 (4.7284)	LR 2.000e-03
0: TRAIN [0][1940/3880]	Time 0.182 (0.164)	Data 1.20e-04 (3.47e-04)	Tok/s 94195 (88268)	Loss/tok 3.6592 (4.7230)	LR 2.000e-03
0: TRAIN [0][1950/3880]	Time 0.181 (0.164)	Data 1.12e-04 (3.46e-04)	Tok/s 93904 (88264)	Loss/tok 3.6975 (4.7179)	LR 2.000e-03
0: TRAIN [0][1960/3880]	Time 0.181 (0.164)	Data 1.24e-04 (3.45e-04)	Tok/s 91750 (88260)	Loss/tok 3.7698 (4.7128)	LR 2.000e-03
0: TRAIN [0][1970/3880]	Time 0.065 (0.164)	Data 1.19e-04 (3.44e-04)	Tok/s 82119 (88261)	Loss/tok 2.8349 (4.7078)	LR 2.000e-03
0: TRAIN [0][1980/3880]	Time 0.125 (0.164)	Data 1.33e-04 (3.43e-04)	Tok/s 83928 (88261)	Loss/tok 3.4107 (4.7027)	LR 2.000e-03
0: TRAIN [0][1990/3880]	Time 0.182 (0.164)	Data 1.06e-04 (3.41e-04)	Tok/s 92395 (88266)	Loss/tok 3.6799 (4.6971)	LR 2.000e-03
0: TRAIN [0][2000/3880]	Time 0.123 (0.164)	Data 1.02e-04 (3.40e-04)	Tok/s 82723 (88260)	Loss/tok 3.3943 (4.6922)	LR 2.000e-03
0: TRAIN [0][2010/3880]	Time 0.181 (0.164)	Data 1.16e-04 (3.39e-04)	Tok/s 93768 (88254)	Loss/tok 3.6655 (4.6874)	LR 2.000e-03
0: TRAIN [0][2020/3880]	Time 0.240 (0.165)	Data 1.03e-04 (3.38e-04)	Tok/s 96300 (88267)	Loss/tok 3.9830 (4.6819)	LR 2.000e-03
0: TRAIN [0][2030/3880]	Time 0.241 (0.165)	Data 1.05e-04 (3.37e-04)	Tok/s 96978 (88270)	Loss/tok 3.8239 (4.6769)	LR 2.000e-03
0: TRAIN [0][2040/3880]	Time 0.126 (0.165)	Data 1.19e-04 (3.36e-04)	Tok/s 80853 (88259)	Loss/tok 3.4977 (4.6721)	LR 2.000e-03
0: TRAIN [0][2050/3880]	Time 0.124 (0.164)	Data 1.05e-04 (3.35e-04)	Tok/s 84298 (88251)	Loss/tok 3.3711 (4.6674)	LR 2.000e-03
0: TRAIN [0][2060/3880]	Time 0.181 (0.164)	Data 1.08e-04 (3.34e-04)	Tok/s 92832 (88253)	Loss/tok 3.7126 (4.6626)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2070/3880]	Time 0.185 (0.165)	Data 1.11e-04 (3.33e-04)	Tok/s 90203 (88263)	Loss/tok 3.7911 (4.6573)	LR 2.000e-03
0: TRAIN [0][2080/3880]	Time 0.068 (0.165)	Data 1.04e-04 (3.32e-04)	Tok/s 78603 (88262)	Loss/tok 2.8627 (4.6523)	LR 2.000e-03
0: TRAIN [0][2090/3880]	Time 0.123 (0.164)	Data 1.29e-04 (3.31e-04)	Tok/s 84597 (88254)	Loss/tok 3.5180 (4.6478)	LR 2.000e-03
0: TRAIN [0][2100/3880]	Time 0.124 (0.164)	Data 9.37e-05 (3.30e-04)	Tok/s 82381 (88246)	Loss/tok 3.3890 (4.6431)	LR 2.000e-03
0: TRAIN [0][2110/3880]	Time 0.067 (0.164)	Data 1.00e-04 (3.29e-04)	Tok/s 78370 (88241)	Loss/tok 2.9503 (4.6386)	LR 2.000e-03
0: TRAIN [0][2120/3880]	Time 0.067 (0.164)	Data 1.17e-04 (3.28e-04)	Tok/s 78203 (88237)	Loss/tok 2.9974 (4.6341)	LR 2.000e-03
0: TRAIN [0][2130/3880]	Time 0.066 (0.164)	Data 9.63e-05 (3.27e-04)	Tok/s 80047 (88233)	Loss/tok 2.9421 (4.6299)	LR 2.000e-03
0: TRAIN [0][2140/3880]	Time 0.182 (0.164)	Data 9.85e-05 (3.25e-04)	Tok/s 91013 (88227)	Loss/tok 3.7582 (4.6256)	LR 2.000e-03
0: TRAIN [0][2150/3880]	Time 0.241 (0.164)	Data 9.73e-05 (3.24e-04)	Tok/s 95965 (88226)	Loss/tok 3.9792 (4.6212)	LR 2.000e-03
0: TRAIN [0][2160/3880]	Time 0.183 (0.164)	Data 1.12e-04 (3.23e-04)	Tok/s 91276 (88225)	Loss/tok 3.6089 (4.6170)	LR 2.000e-03
0: TRAIN [0][2170/3880]	Time 0.182 (0.164)	Data 9.78e-05 (3.22e-04)	Tok/s 91953 (88232)	Loss/tok 3.5753 (4.6123)	LR 2.000e-03
0: TRAIN [0][2180/3880]	Time 0.249 (0.164)	Data 1.02e-04 (3.21e-04)	Tok/s 94241 (88250)	Loss/tok 3.8592 (4.6070)	LR 2.000e-03
0: TRAIN [0][2190/3880]	Time 0.066 (0.164)	Data 1.18e-04 (3.21e-04)	Tok/s 79484 (88251)	Loss/tok 3.0306 (4.6026)	LR 2.000e-03
0: TRAIN [0][2200/3880]	Time 0.247 (0.164)	Data 1.56e-04 (3.20e-04)	Tok/s 94645 (88258)	Loss/tok 3.8782 (4.5980)	LR 2.000e-03
0: TRAIN [0][2210/3880]	Time 0.182 (0.164)	Data 1.04e-04 (3.19e-04)	Tok/s 93300 (88259)	Loss/tok 3.7091 (4.5939)	LR 2.000e-03
0: TRAIN [0][2220/3880]	Time 0.124 (0.164)	Data 1.19e-04 (3.18e-04)	Tok/s 84695 (88259)	Loss/tok 3.4521 (4.5897)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2230/3880]	Time 0.124 (0.164)	Data 1.19e-04 (3.17e-04)	Tok/s 82469 (88245)	Loss/tok 3.3922 (4.5860)	LR 2.000e-03
0: TRAIN [0][2240/3880]	Time 0.245 (0.164)	Data 1.21e-04 (3.16e-04)	Tok/s 95000 (88240)	Loss/tok 3.8094 (4.5816)	LR 2.000e-03
0: TRAIN [0][2250/3880]	Time 0.244 (0.164)	Data 1.17e-04 (3.15e-04)	Tok/s 95716 (88245)	Loss/tok 3.7484 (4.5771)	LR 2.000e-03
0: TRAIN [0][2260/3880]	Time 0.182 (0.164)	Data 1.15e-04 (3.14e-04)	Tok/s 91337 (88236)	Loss/tok 3.7192 (4.5731)	LR 2.000e-03
0: TRAIN [0][2270/3880]	Time 0.066 (0.164)	Data 1.30e-04 (3.14e-04)	Tok/s 80899 (88226)	Loss/tok 2.9402 (4.5692)	LR 2.000e-03
0: TRAIN [0][2280/3880]	Time 0.127 (0.164)	Data 1.03e-04 (3.13e-04)	Tok/s 82382 (88217)	Loss/tok 3.5005 (4.5654)	LR 2.000e-03
0: TRAIN [0][2290/3880]	Time 0.243 (0.164)	Data 9.01e-05 (3.12e-04)	Tok/s 96778 (88216)	Loss/tok 3.7341 (4.5614)	LR 2.000e-03
0: TRAIN [0][2300/3880]	Time 0.122 (0.164)	Data 1.37e-04 (3.11e-04)	Tok/s 85997 (88212)	Loss/tok 3.3775 (4.5575)	LR 2.000e-03
0: TRAIN [0][2310/3880]	Time 0.125 (0.164)	Data 1.21e-04 (3.10e-04)	Tok/s 80717 (88196)	Loss/tok 3.4514 (4.5542)	LR 2.000e-03
0: TRAIN [0][2320/3880]	Time 0.184 (0.164)	Data 1.00e-04 (3.09e-04)	Tok/s 92109 (88202)	Loss/tok 3.6047 (4.5498)	LR 2.000e-03
0: TRAIN [0][2330/3880]	Time 0.124 (0.164)	Data 1.08e-04 (3.08e-04)	Tok/s 82401 (88189)	Loss/tok 3.3554 (4.5463)	LR 2.000e-03
0: TRAIN [0][2340/3880]	Time 0.066 (0.164)	Data 9.16e-05 (3.07e-04)	Tok/s 80604 (88179)	Loss/tok 2.8690 (4.5428)	LR 2.000e-03
0: TRAIN [0][2350/3880]	Time 0.183 (0.164)	Data 9.85e-05 (3.06e-04)	Tok/s 92384 (88179)	Loss/tok 3.6269 (4.5388)	LR 2.000e-03
0: TRAIN [0][2360/3880]	Time 0.181 (0.164)	Data 1.13e-04 (3.06e-04)	Tok/s 93526 (88184)	Loss/tok 3.6537 (4.5349)	LR 2.000e-03
0: TRAIN [0][2370/3880]	Time 0.182 (0.164)	Data 9.97e-05 (3.05e-04)	Tok/s 92079 (88170)	Loss/tok 3.5647 (4.5315)	LR 2.000e-03
0: TRAIN [0][2380/3880]	Time 0.184 (0.164)	Data 9.49e-05 (3.04e-04)	Tok/s 90285 (88171)	Loss/tok 3.6367 (4.5276)	LR 2.000e-03
0: TRAIN [0][2390/3880]	Time 0.182 (0.164)	Data 1.04e-04 (3.03e-04)	Tok/s 91712 (88166)	Loss/tok 3.6609 (4.5238)	LR 2.000e-03
0: TRAIN [0][2400/3880]	Time 0.184 (0.164)	Data 9.56e-05 (3.02e-04)	Tok/s 92225 (88164)	Loss/tok 3.7186 (4.5201)	LR 2.000e-03
0: TRAIN [0][2410/3880]	Time 0.124 (0.164)	Data 1.04e-04 (3.01e-04)	Tok/s 83642 (88173)	Loss/tok 3.3322 (4.5161)	LR 2.000e-03
0: TRAIN [0][2420/3880]	Time 0.125 (0.164)	Data 1.21e-04 (3.01e-04)	Tok/s 82529 (88176)	Loss/tok 3.3974 (4.5122)	LR 2.000e-03
0: TRAIN [0][2430/3880]	Time 0.126 (0.164)	Data 1.13e-04 (3.00e-04)	Tok/s 80981 (88167)	Loss/tok 3.3750 (4.5086)	LR 2.000e-03
0: TRAIN [0][2440/3880]	Time 0.124 (0.164)	Data 1.15e-04 (2.99e-04)	Tok/s 83136 (88171)	Loss/tok 3.3768 (4.5046)	LR 2.000e-03
0: TRAIN [0][2450/3880]	Time 0.242 (0.164)	Data 1.17e-04 (2.99e-04)	Tok/s 96063 (88182)	Loss/tok 3.8698 (4.5007)	LR 2.000e-03
0: TRAIN [0][2460/3880]	Time 0.123 (0.164)	Data 1.17e-04 (2.98e-04)	Tok/s 84532 (88166)	Loss/tok 3.4418 (4.4974)	LR 2.000e-03
0: TRAIN [0][2470/3880]	Time 0.182 (0.164)	Data 1.09e-04 (2.97e-04)	Tok/s 90469 (88172)	Loss/tok 3.7154 (4.4934)	LR 2.000e-03
0: TRAIN [0][2480/3880]	Time 0.067 (0.164)	Data 1.28e-04 (2.96e-04)	Tok/s 78209 (88167)	Loss/tok 2.9025 (4.4900)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][2490/3880]	Time 0.241 (0.164)	Data 1.23e-04 (2.96e-04)	Tok/s 97912 (88171)	Loss/tok 3.7962 (4.4862)	LR 2.000e-03
0: TRAIN [0][2500/3880]	Time 0.242 (0.164)	Data 9.97e-05 (2.95e-04)	Tok/s 96214 (88179)	Loss/tok 3.8421 (4.4824)	LR 2.000e-03
0: TRAIN [0][2510/3880]	Time 0.124 (0.164)	Data 1.29e-04 (2.94e-04)	Tok/s 83368 (88174)	Loss/tok 3.4272 (4.4790)	LR 2.000e-03
0: TRAIN [0][2520/3880]	Time 0.242 (0.164)	Data 1.01e-04 (2.93e-04)	Tok/s 96325 (88166)	Loss/tok 3.8807 (4.4758)	LR 2.000e-03
0: TRAIN [0][2530/3880]	Time 0.125 (0.164)	Data 1.18e-04 (2.93e-04)	Tok/s 82302 (88161)	Loss/tok 3.4637 (4.4726)	LR 2.000e-03
0: TRAIN [0][2540/3880]	Time 0.242 (0.164)	Data 1.06e-04 (2.92e-04)	Tok/s 97153 (88149)	Loss/tok 3.7306 (4.4696)	LR 2.000e-03
0: TRAIN [0][2550/3880]	Time 0.315 (0.164)	Data 1.40e-04 (2.91e-04)	Tok/s 93639 (88151)	Loss/tok 4.0792 (4.4661)	LR 2.000e-03
0: TRAIN [0][2560/3880]	Time 0.124 (0.164)	Data 1.16e-04 (2.91e-04)	Tok/s 82547 (88150)	Loss/tok 3.3242 (4.4626)	LR 2.000e-03
0: TRAIN [0][2570/3880]	Time 0.125 (0.164)	Data 1.12e-04 (2.90e-04)	Tok/s 80528 (88142)	Loss/tok 3.1826 (4.4591)	LR 2.000e-03
0: TRAIN [0][2580/3880]	Time 0.123 (0.164)	Data 1.27e-04 (2.89e-04)	Tok/s 83506 (88128)	Loss/tok 3.4310 (4.4559)	LR 2.000e-03
0: TRAIN [0][2590/3880]	Time 0.184 (0.164)	Data 1.11e-04 (2.89e-04)	Tok/s 91940 (88131)	Loss/tok 3.6262 (4.4524)	LR 2.000e-03
0: TRAIN [0][2600/3880]	Time 0.125 (0.164)	Data 1.05e-04 (2.88e-04)	Tok/s 83860 (88124)	Loss/tok 3.4312 (4.4491)	LR 2.000e-03
0: TRAIN [0][2610/3880]	Time 0.125 (0.164)	Data 1.23e-04 (2.87e-04)	Tok/s 82215 (88120)	Loss/tok 3.4357 (4.4459)	LR 2.000e-03
0: TRAIN [0][2620/3880]	Time 0.124 (0.163)	Data 1.02e-04 (2.87e-04)	Tok/s 83964 (88099)	Loss/tok 3.3911 (4.4430)	LR 2.000e-03
0: TRAIN [0][2630/3880]	Time 0.124 (0.163)	Data 1.19e-04 (2.86e-04)	Tok/s 82483 (88081)	Loss/tok 3.3915 (4.4401)	LR 2.000e-03
0: TRAIN [0][2640/3880]	Time 0.182 (0.163)	Data 1.16e-04 (2.85e-04)	Tok/s 93114 (88083)	Loss/tok 3.5352 (4.4369)	LR 2.000e-03
0: TRAIN [0][2650/3880]	Time 0.246 (0.163)	Data 1.30e-04 (2.85e-04)	Tok/s 96057 (88079)	Loss/tok 3.7214 (4.4336)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][2660/3880]	Time 0.247 (0.163)	Data 1.14e-04 (2.84e-04)	Tok/s 95204 (88084)	Loss/tok 3.7829 (4.4301)	LR 2.000e-03
0: TRAIN [0][2670/3880]	Time 0.125 (0.163)	Data 1.01e-04 (2.83e-04)	Tok/s 81492 (88081)	Loss/tok 3.2302 (4.4269)	LR 2.000e-03
0: TRAIN [0][2680/3880]	Time 0.245 (0.163)	Data 1.18e-04 (2.83e-04)	Tok/s 95823 (88076)	Loss/tok 3.8441 (4.4239)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][2690/3880]	Time 0.242 (0.163)	Data 1.10e-04 (2.82e-04)	Tok/s 95228 (88071)	Loss/tok 3.8130 (4.4211)	LR 2.000e-03
0: TRAIN [0][2700/3880]	Time 0.242 (0.163)	Data 1.17e-04 (2.82e-04)	Tok/s 95423 (88077)	Loss/tok 3.9099 (4.4178)	LR 2.000e-03
0: TRAIN [0][2710/3880]	Time 0.184 (0.163)	Data 1.16e-04 (2.81e-04)	Tok/s 91864 (88078)	Loss/tok 3.4735 (4.4145)	LR 2.000e-03
0: TRAIN [0][2720/3880]	Time 0.124 (0.163)	Data 1.20e-04 (2.80e-04)	Tok/s 84380 (88085)	Loss/tok 3.3667 (4.4112)	LR 2.000e-03
0: TRAIN [0][2730/3880]	Time 0.182 (0.163)	Data 9.87e-05 (2.80e-04)	Tok/s 92179 (88083)	Loss/tok 3.5819 (4.4081)	LR 2.000e-03
0: TRAIN [0][2740/3880]	Time 0.124 (0.163)	Data 1.37e-04 (2.79e-04)	Tok/s 81951 (88066)	Loss/tok 3.3123 (4.4054)	LR 2.000e-03
0: TRAIN [0][2750/3880]	Time 0.241 (0.163)	Data 1.26e-04 (2.79e-04)	Tok/s 96354 (88075)	Loss/tok 3.8329 (4.4022)	LR 2.000e-03
0: TRAIN [0][2760/3880]	Time 0.125 (0.163)	Data 1.10e-04 (2.78e-04)	Tok/s 81946 (88073)	Loss/tok 3.3918 (4.3993)	LR 2.000e-03
0: TRAIN [0][2770/3880]	Time 0.124 (0.163)	Data 1.33e-04 (2.77e-04)	Tok/s 81751 (88067)	Loss/tok 3.3453 (4.3963)	LR 2.000e-03
0: TRAIN [0][2780/3880]	Time 0.124 (0.163)	Data 1.48e-04 (2.77e-04)	Tok/s 82116 (88069)	Loss/tok 3.4273 (4.3932)	LR 2.000e-03
0: TRAIN [0][2790/3880]	Time 0.316 (0.163)	Data 1.21e-04 (2.76e-04)	Tok/s 93515 (88068)	Loss/tok 4.0842 (4.3906)	LR 2.000e-03
0: TRAIN [0][2800/3880]	Time 0.068 (0.163)	Data 1.30e-04 (2.76e-04)	Tok/s 78056 (88063)	Loss/tok 2.9070 (4.3878)	LR 2.000e-03
0: TRAIN [0][2810/3880]	Time 0.184 (0.163)	Data 1.47e-04 (2.75e-04)	Tok/s 90383 (88062)	Loss/tok 3.5016 (4.3846)	LR 2.000e-03
0: TRAIN [0][2820/3880]	Time 0.124 (0.163)	Data 9.32e-05 (2.74e-04)	Tok/s 83436 (88063)	Loss/tok 3.3669 (4.3817)	LR 2.000e-03
0: TRAIN [0][2830/3880]	Time 0.123 (0.163)	Data 9.97e-05 (2.74e-04)	Tok/s 84532 (88057)	Loss/tok 3.2199 (4.3788)	LR 2.000e-03
0: TRAIN [0][2840/3880]	Time 0.184 (0.163)	Data 1.15e-04 (2.73e-04)	Tok/s 90049 (88050)	Loss/tok 3.6128 (4.3761)	LR 2.000e-03
0: TRAIN [0][2850/3880]	Time 0.242 (0.163)	Data 1.12e-04 (2.73e-04)	Tok/s 95852 (88051)	Loss/tok 3.7594 (4.3735)	LR 2.000e-03
0: TRAIN [0][2860/3880]	Time 0.124 (0.163)	Data 1.16e-04 (2.72e-04)	Tok/s 82746 (88044)	Loss/tok 3.2339 (4.3709)	LR 2.000e-03
0: TRAIN [0][2870/3880]	Time 0.183 (0.163)	Data 1.24e-04 (2.72e-04)	Tok/s 91443 (88046)	Loss/tok 3.4834 (4.3679)	LR 2.000e-03
0: TRAIN [0][2880/3880]	Time 0.312 (0.163)	Data 9.75e-05 (2.71e-04)	Tok/s 95059 (88051)	Loss/tok 3.9833 (4.3650)	LR 2.000e-03
0: TRAIN [0][2890/3880]	Time 0.125 (0.163)	Data 1.03e-04 (2.71e-04)	Tok/s 83563 (88042)	Loss/tok 3.3697 (4.3624)	LR 2.000e-03
0: TRAIN [0][2900/3880]	Time 0.066 (0.163)	Data 1.09e-04 (2.70e-04)	Tok/s 77563 (88040)	Loss/tok 2.7589 (4.3596)	LR 2.000e-03
0: TRAIN [0][2910/3880]	Time 0.123 (0.163)	Data 9.82e-05 (2.70e-04)	Tok/s 84797 (88032)	Loss/tok 3.2888 (4.3570)	LR 2.000e-03
0: TRAIN [0][2920/3880]	Time 0.243 (0.163)	Data 1.53e-04 (2.69e-04)	Tok/s 96995 (88031)	Loss/tok 3.6426 (4.3540)	LR 2.000e-03
0: TRAIN [0][2930/3880]	Time 0.066 (0.163)	Data 1.11e-04 (2.69e-04)	Tok/s 78088 (88021)	Loss/tok 2.7511 (4.3515)	LR 2.000e-03
0: TRAIN [0][2940/3880]	Time 0.185 (0.163)	Data 1.29e-04 (2.68e-04)	Tok/s 91815 (88024)	Loss/tok 3.5539 (4.3488)	LR 2.000e-03
0: TRAIN [0][2950/3880]	Time 0.126 (0.163)	Data 1.49e-04 (2.67e-04)	Tok/s 82758 (88024)	Loss/tok 3.3344 (4.3461)	LR 2.000e-03
0: TRAIN [0][2960/3880]	Time 0.126 (0.163)	Data 1.16e-04 (2.67e-04)	Tok/s 83580 (88022)	Loss/tok 3.4118 (4.3433)	LR 2.000e-03
0: TRAIN [0][2970/3880]	Time 0.124 (0.163)	Data 1.35e-04 (2.66e-04)	Tok/s 83637 (88016)	Loss/tok 3.3322 (4.3407)	LR 2.000e-03
0: TRAIN [0][2980/3880]	Time 0.123 (0.163)	Data 1.15e-04 (2.66e-04)	Tok/s 83541 (88014)	Loss/tok 3.2533 (4.3382)	LR 2.000e-03
0: TRAIN [0][2990/3880]	Time 0.184 (0.163)	Data 1.44e-04 (2.66e-04)	Tok/s 93055 (88015)	Loss/tok 3.4623 (4.3355)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3000/3880]	Time 0.185 (0.163)	Data 1.27e-04 (2.65e-04)	Tok/s 91648 (88013)	Loss/tok 3.5382 (4.3328)	LR 2.000e-03
0: TRAIN [0][3010/3880]	Time 0.124 (0.163)	Data 1.04e-04 (2.65e-04)	Tok/s 84067 (88014)	Loss/tok 3.3093 (4.3300)	LR 2.000e-03
0: TRAIN [0][3020/3880]	Time 0.243 (0.163)	Data 1.15e-04 (2.64e-04)	Tok/s 95584 (88017)	Loss/tok 3.7607 (4.3272)	LR 2.000e-03
0: TRAIN [0][3030/3880]	Time 0.244 (0.163)	Data 1.41e-04 (2.64e-04)	Tok/s 96486 (88018)	Loss/tok 3.5491 (4.3246)	LR 2.000e-03
0: TRAIN [0][3040/3880]	Time 0.066 (0.163)	Data 8.92e-05 (2.63e-04)	Tok/s 79914 (88006)	Loss/tok 2.8584 (4.3222)	LR 2.000e-03
0: TRAIN [0][3050/3880]	Time 0.184 (0.163)	Data 1.33e-04 (2.63e-04)	Tok/s 90455 (88005)	Loss/tok 3.5108 (4.3196)	LR 2.000e-03
0: TRAIN [0][3060/3880]	Time 0.126 (0.163)	Data 1.03e-04 (2.62e-04)	Tok/s 81540 (88004)	Loss/tok 3.3930 (4.3170)	LR 2.000e-03
0: TRAIN [0][3070/3880]	Time 0.125 (0.163)	Data 9.16e-05 (2.62e-04)	Tok/s 83041 (87998)	Loss/tok 3.3349 (4.3146)	LR 2.000e-03
0: TRAIN [0][3080/3880]	Time 0.124 (0.163)	Data 9.94e-05 (2.61e-04)	Tok/s 81709 (87987)	Loss/tok 3.3156 (4.3123)	LR 2.000e-03
0: TRAIN [0][3090/3880]	Time 0.244 (0.163)	Data 1.39e-04 (2.61e-04)	Tok/s 96170 (87999)	Loss/tok 3.7522 (4.3094)	LR 2.000e-03
0: TRAIN [0][3100/3880]	Time 0.315 (0.163)	Data 1.00e-04 (2.60e-04)	Tok/s 94750 (88000)	Loss/tok 3.9288 (4.3069)	LR 2.000e-03
0: TRAIN [0][3110/3880]	Time 0.183 (0.163)	Data 1.06e-04 (2.60e-04)	Tok/s 93380 (88004)	Loss/tok 3.5218 (4.3043)	LR 2.000e-03
0: TRAIN [0][3120/3880]	Time 0.125 (0.163)	Data 9.54e-05 (2.59e-04)	Tok/s 83727 (88002)	Loss/tok 3.3380 (4.3018)	LR 2.000e-03
0: TRAIN [0][3130/3880]	Time 0.125 (0.163)	Data 1.13e-04 (2.59e-04)	Tok/s 82103 (88011)	Loss/tok 3.3326 (4.2991)	LR 2.000e-03
0: TRAIN [0][3140/3880]	Time 0.182 (0.163)	Data 9.27e-05 (2.58e-04)	Tok/s 91659 (88006)	Loss/tok 3.5159 (4.2966)	LR 2.000e-03
0: TRAIN [0][3150/3880]	Time 0.312 (0.163)	Data 9.37e-05 (2.58e-04)	Tok/s 95301 (88009)	Loss/tok 3.9410 (4.2943)	LR 2.000e-03
0: TRAIN [0][3160/3880]	Time 0.183 (0.163)	Data 1.16e-04 (2.57e-04)	Tok/s 90367 (88004)	Loss/tok 3.5143 (4.2919)	LR 2.000e-03
0: TRAIN [0][3170/3880]	Time 0.124 (0.163)	Data 1.21e-04 (2.57e-04)	Tok/s 82796 (87997)	Loss/tok 3.3851 (4.2896)	LR 2.000e-03
0: TRAIN [0][3180/3880]	Time 0.125 (0.163)	Data 1.23e-04 (2.56e-04)	Tok/s 83516 (87990)	Loss/tok 3.2685 (4.2873)	LR 2.000e-03
0: TRAIN [0][3190/3880]	Time 0.129 (0.163)	Data 9.82e-05 (2.56e-04)	Tok/s 81130 (87984)	Loss/tok 3.2575 (4.2849)	LR 2.000e-03
0: TRAIN [0][3200/3880]	Time 0.125 (0.163)	Data 1.08e-04 (2.56e-04)	Tok/s 84220 (87984)	Loss/tok 3.3067 (4.2826)	LR 2.000e-03
0: TRAIN [0][3210/3880]	Time 0.127 (0.163)	Data 1.09e-04 (2.55e-04)	Tok/s 80713 (87988)	Loss/tok 3.2981 (4.2800)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3220/3880]	Time 0.183 (0.163)	Data 1.02e-04 (2.55e-04)	Tok/s 92272 (87995)	Loss/tok 3.4989 (4.2773)	LR 2.000e-03
0: TRAIN [0][3230/3880]	Time 0.124 (0.163)	Data 9.82e-05 (2.54e-04)	Tok/s 83296 (87985)	Loss/tok 3.2217 (4.2751)	LR 2.000e-03
0: TRAIN [0][3240/3880]	Time 0.185 (0.163)	Data 1.05e-04 (2.54e-04)	Tok/s 89931 (87993)	Loss/tok 3.6513 (4.2726)	LR 2.000e-03
0: TRAIN [0][3250/3880]	Time 0.242 (0.163)	Data 1.12e-04 (2.53e-04)	Tok/s 96045 (87999)	Loss/tok 3.8180 (4.2702)	LR 2.000e-03
0: TRAIN [0][3260/3880]	Time 0.181 (0.163)	Data 1.15e-04 (2.53e-04)	Tok/s 93201 (88001)	Loss/tok 3.3930 (4.2678)	LR 2.000e-03
0: TRAIN [0][3270/3880]	Time 0.068 (0.163)	Data 1.18e-04 (2.53e-04)	Tok/s 77588 (87995)	Loss/tok 2.7701 (4.2658)	LR 2.000e-03
0: TRAIN [0][3280/3880]	Time 0.245 (0.163)	Data 1.13e-04 (2.52e-04)	Tok/s 96050 (87996)	Loss/tok 3.6290 (4.2635)	LR 2.000e-03
0: TRAIN [0][3290/3880]	Time 0.183 (0.163)	Data 9.87e-05 (2.52e-04)	Tok/s 93357 (87988)	Loss/tok 3.5867 (4.2614)	LR 2.000e-03
0: TRAIN [0][3300/3880]	Time 0.184 (0.163)	Data 1.19e-04 (2.51e-04)	Tok/s 91531 (87979)	Loss/tok 3.5325 (4.2593)	LR 2.000e-03
0: TRAIN [0][3310/3880]	Time 0.312 (0.163)	Data 1.17e-04 (2.51e-04)	Tok/s 94806 (87982)	Loss/tok 3.8835 (4.2571)	LR 2.000e-03
0: TRAIN [0][3320/3880]	Time 0.126 (0.163)	Data 9.27e-05 (2.50e-04)	Tok/s 82125 (87986)	Loss/tok 3.2631 (4.2547)	LR 2.000e-03
0: TRAIN [0][3330/3880]	Time 0.124 (0.163)	Data 1.16e-04 (2.50e-04)	Tok/s 83388 (87987)	Loss/tok 3.2680 (4.2523)	LR 2.000e-03
0: TRAIN [0][3340/3880]	Time 0.124 (0.163)	Data 1.11e-04 (2.50e-04)	Tok/s 82573 (87981)	Loss/tok 3.3147 (4.2504)	LR 2.000e-03
0: TRAIN [0][3350/3880]	Time 0.124 (0.163)	Data 1.05e-04 (2.49e-04)	Tok/s 82713 (87976)	Loss/tok 3.2428 (4.2481)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3360/3880]	Time 0.244 (0.163)	Data 1.02e-04 (2.49e-04)	Tok/s 97540 (87984)	Loss/tok 3.5929 (4.2456)	LR 2.000e-03
0: TRAIN [0][3370/3880]	Time 0.244 (0.163)	Data 9.80e-05 (2.48e-04)	Tok/s 95309 (87980)	Loss/tok 3.6319 (4.2435)	LR 2.000e-03
0: TRAIN [0][3380/3880]	Time 0.124 (0.163)	Data 1.06e-04 (2.48e-04)	Tok/s 83882 (87975)	Loss/tok 3.3618 (4.2414)	LR 2.000e-03
0: TRAIN [0][3390/3880]	Time 0.126 (0.163)	Data 1.08e-04 (2.47e-04)	Tok/s 80974 (87965)	Loss/tok 3.4122 (4.2395)	LR 2.000e-03
0: TRAIN [0][3400/3880]	Time 0.124 (0.163)	Data 1.13e-04 (2.47e-04)	Tok/s 82402 (87960)	Loss/tok 3.2786 (4.2373)	LR 2.000e-03
0: TRAIN [0][3410/3880]	Time 0.182 (0.163)	Data 1.17e-04 (2.47e-04)	Tok/s 92402 (87956)	Loss/tok 3.5063 (4.2353)	LR 2.000e-03
0: TRAIN [0][3420/3880]	Time 0.124 (0.163)	Data 1.03e-04 (2.46e-04)	Tok/s 84108 (87960)	Loss/tok 3.1267 (4.2330)	LR 2.000e-03
0: TRAIN [0][3430/3880]	Time 0.127 (0.163)	Data 1.13e-04 (2.46e-04)	Tok/s 82058 (87960)	Loss/tok 3.2523 (4.2308)	LR 2.000e-03
0: TRAIN [0][3440/3880]	Time 0.182 (0.163)	Data 1.08e-04 (2.45e-04)	Tok/s 92850 (87968)	Loss/tok 3.4904 (4.2285)	LR 2.000e-03
0: TRAIN [0][3450/3880]	Time 0.181 (0.163)	Data 9.51e-05 (2.45e-04)	Tok/s 91773 (87969)	Loss/tok 3.4511 (4.2263)	LR 2.000e-03
0: TRAIN [0][3460/3880]	Time 0.186 (0.163)	Data 1.20e-04 (2.45e-04)	Tok/s 89751 (87968)	Loss/tok 3.5272 (4.2241)	LR 2.000e-03
0: TRAIN [0][3470/3880]	Time 0.124 (0.163)	Data 1.09e-04 (2.44e-04)	Tok/s 84300 (87960)	Loss/tok 3.1703 (4.2220)	LR 2.000e-03
0: TRAIN [0][3480/3880]	Time 0.125 (0.163)	Data 1.09e-04 (2.44e-04)	Tok/s 82340 (87957)	Loss/tok 3.2294 (4.2201)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3490/3880]	Time 0.184 (0.163)	Data 9.82e-05 (2.43e-04)	Tok/s 90935 (87957)	Loss/tok 3.5555 (4.2182)	LR 2.000e-03
0: TRAIN [0][3500/3880]	Time 0.125 (0.163)	Data 1.19e-04 (2.43e-04)	Tok/s 82854 (87950)	Loss/tok 3.4383 (4.2162)	LR 2.000e-03
0: TRAIN [0][3510/3880]	Time 0.183 (0.163)	Data 1.12e-04 (2.43e-04)	Tok/s 92910 (87945)	Loss/tok 3.5927 (4.2142)	LR 2.000e-03
0: TRAIN [0][3520/3880]	Time 0.067 (0.163)	Data 9.97e-05 (2.42e-04)	Tok/s 77906 (87935)	Loss/tok 2.7969 (4.2124)	LR 2.000e-03
0: TRAIN [0][3530/3880]	Time 0.067 (0.163)	Data 8.70e-05 (2.42e-04)	Tok/s 78878 (87933)	Loss/tok 2.8259 (4.2105)	LR 2.000e-03
0: TRAIN [0][3540/3880]	Time 0.125 (0.163)	Data 1.06e-04 (2.42e-04)	Tok/s 82235 (87932)	Loss/tok 3.3106 (4.2086)	LR 2.000e-03
0: TRAIN [0][3550/3880]	Time 0.124 (0.163)	Data 1.12e-04 (2.41e-04)	Tok/s 84362 (87925)	Loss/tok 3.3718 (4.2067)	LR 2.000e-03
0: TRAIN [0][3560/3880]	Time 0.187 (0.163)	Data 1.12e-04 (2.41e-04)	Tok/s 90576 (87925)	Loss/tok 3.5454 (4.2046)	LR 2.000e-03
0: TRAIN [0][3570/3880]	Time 0.246 (0.163)	Data 1.44e-04 (2.40e-04)	Tok/s 95566 (87922)	Loss/tok 3.7057 (4.2025)	LR 2.000e-03
0: TRAIN [0][3580/3880]	Time 0.067 (0.163)	Data 1.11e-04 (2.40e-04)	Tok/s 79008 (87911)	Loss/tok 2.7179 (4.2008)	LR 2.000e-03
0: TRAIN [0][3590/3880]	Time 0.067 (0.163)	Data 1.34e-04 (2.40e-04)	Tok/s 77713 (87916)	Loss/tok 2.7939 (4.1988)	LR 2.000e-03
0: TRAIN [0][3600/3880]	Time 0.182 (0.163)	Data 1.22e-04 (2.39e-04)	Tok/s 91119 (87921)	Loss/tok 3.5653 (4.1967)	LR 2.000e-03
0: TRAIN [0][3610/3880]	Time 0.126 (0.163)	Data 1.23e-04 (2.39e-04)	Tok/s 80325 (87916)	Loss/tok 3.3235 (4.1948)	LR 2.000e-03
0: TRAIN [0][3620/3880]	Time 0.245 (0.163)	Data 1.21e-04 (2.39e-04)	Tok/s 94905 (87909)	Loss/tok 3.5810 (4.1929)	LR 2.000e-03
0: TRAIN [0][3630/3880]	Time 0.124 (0.163)	Data 1.14e-04 (2.38e-04)	Tok/s 82733 (87896)	Loss/tok 3.3242 (4.1912)	LR 2.000e-03
0: TRAIN [0][3640/3880]	Time 0.068 (0.163)	Data 1.32e-04 (2.38e-04)	Tok/s 76657 (87891)	Loss/tok 2.8711 (4.1895)	LR 2.000e-03
0: TRAIN [0][3650/3880]	Time 0.183 (0.163)	Data 1.25e-04 (2.38e-04)	Tok/s 92187 (87889)	Loss/tok 3.5459 (4.1877)	LR 2.000e-03
0: TRAIN [0][3660/3880]	Time 0.125 (0.163)	Data 1.10e-04 (2.38e-04)	Tok/s 83262 (87879)	Loss/tok 3.3055 (4.1860)	LR 2.000e-03
0: TRAIN [0][3670/3880]	Time 0.184 (0.163)	Data 1.37e-04 (2.37e-04)	Tok/s 92664 (87876)	Loss/tok 3.5019 (4.1840)	LR 2.000e-03
0: TRAIN [0][3680/3880]	Time 0.243 (0.163)	Data 1.23e-04 (2.37e-04)	Tok/s 97004 (87874)	Loss/tok 3.7379 (4.1822)	LR 2.000e-03
0: TRAIN [0][3690/3880]	Time 0.182 (0.163)	Data 1.19e-04 (2.37e-04)	Tok/s 92518 (87880)	Loss/tok 3.5157 (4.1802)	LR 2.000e-03
0: TRAIN [0][3700/3880]	Time 0.124 (0.163)	Data 1.41e-04 (2.36e-04)	Tok/s 82178 (87877)	Loss/tok 3.2547 (4.1783)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][3710/3880]	Time 0.123 (0.163)	Data 1.26e-04 (2.36e-04)	Tok/s 84884 (87881)	Loss/tok 3.1476 (4.1763)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][3720/3880]	Time 0.123 (0.163)	Data 1.23e-04 (2.36e-04)	Tok/s 84726 (87880)	Loss/tok 3.2835 (4.1746)	LR 2.000e-03
0: TRAIN [0][3730/3880]	Time 0.182 (0.163)	Data 1.41e-04 (2.35e-04)	Tok/s 91838 (87886)	Loss/tok 3.4815 (4.1729)	LR 2.000e-03
0: TRAIN [0][3740/3880]	Time 0.125 (0.163)	Data 1.16e-04 (2.35e-04)	Tok/s 83521 (87872)	Loss/tok 3.2655 (4.1712)	LR 2.000e-03
0: TRAIN [0][3750/3880]	Time 0.183 (0.163)	Data 1.24e-04 (2.35e-04)	Tok/s 90887 (87866)	Loss/tok 3.7179 (4.1695)	LR 2.000e-03
0: TRAIN [0][3760/3880]	Time 0.123 (0.162)	Data 1.07e-04 (2.34e-04)	Tok/s 85271 (87859)	Loss/tok 3.1459 (4.1678)	LR 2.000e-03
0: TRAIN [0][3770/3880]	Time 0.241 (0.162)	Data 1.04e-04 (2.34e-04)	Tok/s 97800 (87855)	Loss/tok 3.6141 (4.1660)	LR 2.000e-03
0: TRAIN [0][3780/3880]	Time 0.128 (0.163)	Data 1.50e-04 (2.34e-04)	Tok/s 81723 (87858)	Loss/tok 3.2791 (4.1642)	LR 2.000e-03
0: TRAIN [0][3790/3880]	Time 0.243 (0.163)	Data 1.01e-04 (2.33e-04)	Tok/s 96357 (87854)	Loss/tok 3.7276 (4.1625)	LR 2.000e-03
0: TRAIN [0][3800/3880]	Time 0.181 (0.163)	Data 1.31e-04 (2.33e-04)	Tok/s 93180 (87855)	Loss/tok 3.5582 (4.1606)	LR 2.000e-03
0: TRAIN [0][3810/3880]	Time 0.128 (0.163)	Data 1.13e-04 (2.33e-04)	Tok/s 80091 (87852)	Loss/tok 3.1627 (4.1589)	LR 2.000e-03
0: TRAIN [0][3820/3880]	Time 0.242 (0.163)	Data 1.37e-04 (2.33e-04)	Tok/s 97680 (87849)	Loss/tok 3.5857 (4.1572)	LR 2.000e-03
0: TRAIN [0][3830/3880]	Time 0.127 (0.163)	Data 1.22e-04 (2.32e-04)	Tok/s 81890 (87855)	Loss/tok 3.2334 (4.1551)	LR 2.000e-03
0: TRAIN [0][3840/3880]	Time 0.124 (0.163)	Data 1.00e-04 (2.32e-04)	Tok/s 84485 (87855)	Loss/tok 3.2443 (4.1534)	LR 2.000e-03
0: TRAIN [0][3850/3880]	Time 0.128 (0.163)	Data 1.60e-04 (2.32e-04)	Tok/s 81750 (87862)	Loss/tok 3.2568 (4.1516)	LR 2.000e-03
0: TRAIN [0][3860/3880]	Time 0.126 (0.163)	Data 1.08e-04 (2.31e-04)	Tok/s 81867 (87853)	Loss/tok 3.1398 (4.1500)	LR 2.000e-03
0: TRAIN [0][3870/3880]	Time 0.125 (0.163)	Data 1.18e-04 (2.31e-04)	Tok/s 82346 (87850)	Loss/tok 3.4709 (4.1483)	LR 2.000e-03
:::MLL 1586323941.626 epoch_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 524}}
:::MLL 1586323941.626 eval_start: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [0][0/6]	Time 0.727 (0.727)	Decoder iters 149.0 (149.0)	Tok/s 22446 (22446)
0: Running moses detokenizer
0: BLEU(score=20.64273660493536, counts=[35215, 16413, 8924, 5031], totals=[66079, 63076, 60073, 57076], precisions=[53.292271372145464, 26.020990551081237, 14.855259434354869, 8.814563038755344], bp=1.0, sys_len=66079, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586323944.666 eval_accuracy: {"value": 20.64, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 535}}
:::MLL 1586323944.667 eval_stop: {"value": null, "metadata": {"epoch_num": 1, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 0	Training Loss: 4.1479	Test BLEU: 20.64
0: Performance: Epoch: 0	Training: 351460 Tok/s
0: Finished epoch 0
:::MLL 1586323944.667 block_stop: {"value": null, "metadata": {"first_epoch_num": 1, "file": "train.py", "lineno": 557}}
:::MLL 1586323944.668 block_start: {"value": null, "metadata": {"first_epoch_num": 2, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1586323944.668 epoch_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 514}}
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2103025043
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][0/3880]	Time 0.867 (0.867)	Data 2.80e-01 (2.80e-01)	Tok/s 11746 (11746)	Loss/tok 3.2328 (3.2328)	LR 2.000e-03
0: TRAIN [1][10/3880]	Time 0.124 (0.214)	Data 1.49e-04 (2.56e-02)	Tok/s 85468 (79325)	Loss/tok 3.1971 (3.3778)	LR 2.000e-03
0: TRAIN [1][20/3880]	Time 0.244 (0.200)	Data 1.05e-04 (1.34e-02)	Tok/s 95722 (84029)	Loss/tok 3.5639 (3.4357)	LR 2.000e-03
0: TRAIN [1][30/3880]	Time 0.242 (0.183)	Data 9.27e-05 (9.14e-03)	Tok/s 95752 (84534)	Loss/tok 3.6550 (3.4092)	LR 2.000e-03
0: TRAIN [1][40/3880]	Time 0.244 (0.175)	Data 9.75e-05 (6.94e-03)	Tok/s 94293 (84941)	Loss/tok 3.5757 (3.3952)	LR 2.000e-03
0: TRAIN [1][50/3880]	Time 0.124 (0.170)	Data 8.85e-05 (5.60e-03)	Tok/s 82911 (85304)	Loss/tok 3.1681 (3.3795)	LR 2.000e-03
0: TRAIN [1][60/3880]	Time 0.183 (0.166)	Data 9.01e-05 (4.70e-03)	Tok/s 91324 (85604)	Loss/tok 3.4462 (3.3736)	LR 2.000e-03
0: TRAIN [1][70/3880]	Time 0.314 (0.169)	Data 1.06e-04 (4.05e-03)	Tok/s 94932 (86306)	Loss/tok 3.7518 (3.3841)	LR 2.000e-03
0: TRAIN [1][80/3880]	Time 0.126 (0.169)	Data 1.12e-04 (3.56e-03)	Tok/s 83583 (86377)	Loss/tok 3.2748 (3.3907)	LR 2.000e-03
0: TRAIN [1][90/3880]	Time 0.126 (0.170)	Data 1.08e-04 (3.18e-03)	Tok/s 81937 (86457)	Loss/tok 3.1788 (3.4024)	LR 2.000e-03
0: TRAIN [1][100/3880]	Time 0.183 (0.167)	Data 1.54e-04 (2.88e-03)	Tok/s 91818 (86445)	Loss/tok 3.5414 (3.3955)	LR 2.000e-03
0: TRAIN [1][110/3880]	Time 0.182 (0.165)	Data 1.15e-04 (2.63e-03)	Tok/s 93377 (86514)	Loss/tok 3.3299 (3.3874)	LR 2.000e-03
0: TRAIN [1][120/3880]	Time 0.124 (0.166)	Data 1.03e-04 (2.42e-03)	Tok/s 82662 (86729)	Loss/tok 3.1399 (3.4018)	LR 2.000e-03
0: TRAIN [1][130/3880]	Time 0.242 (0.165)	Data 1.16e-04 (2.25e-03)	Tok/s 97039 (86715)	Loss/tok 3.5061 (3.3956)	LR 2.000e-03
0: TRAIN [1][140/3880]	Time 0.122 (0.164)	Data 1.26e-04 (2.10e-03)	Tok/s 82470 (86830)	Loss/tok 3.1387 (3.3913)	LR 2.000e-03
0: TRAIN [1][150/3880]	Time 0.124 (0.163)	Data 1.21e-04 (1.97e-03)	Tok/s 84050 (86876)	Loss/tok 3.2216 (3.3879)	LR 2.000e-03
0: TRAIN [1][160/3880]	Time 0.245 (0.164)	Data 1.32e-04 (1.85e-03)	Tok/s 96311 (87029)	Loss/tok 3.5180 (3.3923)	LR 2.000e-03
0: TRAIN [1][170/3880]	Time 0.123 (0.164)	Data 1.10e-04 (1.75e-03)	Tok/s 82796 (87177)	Loss/tok 3.1220 (3.3974)	LR 2.000e-03
0: TRAIN [1][180/3880]	Time 0.124 (0.162)	Data 1.04e-04 (1.66e-03)	Tok/s 81533 (86997)	Loss/tok 3.3143 (3.3911)	LR 2.000e-03
0: TRAIN [1][190/3880]	Time 0.124 (0.162)	Data 1.17e-04 (1.58e-03)	Tok/s 83091 (87016)	Loss/tok 3.0987 (3.3889)	LR 2.000e-03
0: TRAIN [1][200/3880]	Time 0.181 (0.162)	Data 1.26e-04 (1.50e-03)	Tok/s 92015 (87078)	Loss/tok 3.2753 (3.3851)	LR 2.000e-03
0: TRAIN [1][210/3880]	Time 0.124 (0.162)	Data 9.85e-05 (1.44e-03)	Tok/s 83578 (87093)	Loss/tok 3.1682 (3.3859)	LR 2.000e-03
0: TRAIN [1][220/3880]	Time 0.182 (0.160)	Data 1.18e-04 (1.38e-03)	Tok/s 92169 (86964)	Loss/tok 3.3915 (3.3799)	LR 2.000e-03
0: TRAIN [1][230/3880]	Time 0.124 (0.158)	Data 1.22e-04 (1.32e-03)	Tok/s 84397 (86892)	Loss/tok 3.2630 (3.3757)	LR 2.000e-03
0: TRAIN [1][240/3880]	Time 0.247 (0.160)	Data 1.27e-04 (1.27e-03)	Tok/s 95234 (87043)	Loss/tok 3.6478 (3.3817)	LR 2.000e-03
0: TRAIN [1][250/3880]	Time 0.126 (0.161)	Data 1.02e-04 (1.23e-03)	Tok/s 81887 (87128)	Loss/tok 3.2546 (3.3844)	LR 2.000e-03
0: TRAIN [1][260/3880]	Time 0.246 (0.160)	Data 1.38e-04 (1.19e-03)	Tok/s 96059 (87132)	Loss/tok 3.4612 (3.3818)	LR 2.000e-03
0: TRAIN [1][270/3880]	Time 0.125 (0.161)	Data 1.13e-04 (1.15e-03)	Tok/s 84489 (87195)	Loss/tok 3.1184 (3.3831)	LR 2.000e-03
0: TRAIN [1][280/3880]	Time 0.124 (0.161)	Data 1.08e-04 (1.11e-03)	Tok/s 84244 (87193)	Loss/tok 3.2204 (3.3838)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][290/3880]	Time 0.180 (0.161)	Data 1.64e-04 (1.08e-03)	Tok/s 92782 (87167)	Loss/tok 3.3530 (3.3844)	LR 2.000e-03
0: TRAIN [1][300/3880]	Time 0.126 (0.162)	Data 1.34e-04 (1.04e-03)	Tok/s 83111 (87228)	Loss/tok 3.1927 (3.3904)	LR 2.000e-03
0: TRAIN [1][310/3880]	Time 0.314 (0.162)	Data 1.01e-04 (1.01e-03)	Tok/s 94756 (87234)	Loss/tok 3.8009 (3.3942)	LR 2.000e-03
0: TRAIN [1][320/3880]	Time 0.066 (0.162)	Data 1.01e-04 (9.86e-04)	Tok/s 77581 (87224)	Loss/tok 2.6090 (3.3948)	LR 2.000e-03
0: TRAIN [1][330/3880]	Time 0.245 (0.163)	Data 1.08e-04 (9.60e-04)	Tok/s 95862 (87302)	Loss/tok 3.5570 (3.3991)	LR 2.000e-03
0: TRAIN [1][340/3880]	Time 0.183 (0.163)	Data 9.75e-05 (9.35e-04)	Tok/s 90052 (87350)	Loss/tok 3.4857 (3.4004)	LR 2.000e-03
0: TRAIN [1][350/3880]	Time 0.182 (0.163)	Data 1.07e-04 (9.11e-04)	Tok/s 92902 (87387)	Loss/tok 3.4550 (3.4012)	LR 2.000e-03
0: TRAIN [1][360/3880]	Time 0.184 (0.164)	Data 1.03e-04 (8.89e-04)	Tok/s 91406 (87483)	Loss/tok 3.3391 (3.4038)	LR 2.000e-03
0: TRAIN [1][370/3880]	Time 0.184 (0.164)	Data 9.94e-05 (8.68e-04)	Tok/s 90471 (87462)	Loss/tok 3.4734 (3.4015)	LR 2.000e-03
0: TRAIN [1][380/3880]	Time 0.124 (0.163)	Data 9.85e-05 (8.48e-04)	Tok/s 84289 (87462)	Loss/tok 3.1954 (3.4002)	LR 2.000e-03
0: TRAIN [1][390/3880]	Time 0.124 (0.163)	Data 1.13e-04 (8.29e-04)	Tok/s 84619 (87453)	Loss/tok 3.1336 (3.3991)	LR 2.000e-03
0: TRAIN [1][400/3880]	Time 0.244 (0.164)	Data 1.26e-04 (8.12e-04)	Tok/s 95097 (87561)	Loss/tok 3.6704 (3.4031)	LR 2.000e-03
0: TRAIN [1][410/3880]	Time 0.317 (0.165)	Data 1.20e-04 (7.95e-04)	Tok/s 94327 (87600)	Loss/tok 3.7804 (3.4090)	LR 2.000e-03
0: TRAIN [1][420/3880]	Time 0.246 (0.165)	Data 1.34e-04 (7.79e-04)	Tok/s 93555 (87606)	Loss/tok 3.6103 (3.4080)	LR 2.000e-03
0: TRAIN [1][430/3880]	Time 0.184 (0.165)	Data 1.28e-04 (7.64e-04)	Tok/s 91965 (87634)	Loss/tok 3.2831 (3.4075)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][440/3880]	Time 0.067 (0.166)	Data 1.57e-04 (7.49e-04)	Tok/s 79821 (87672)	Loss/tok 2.7712 (3.4101)	LR 2.000e-03
0: TRAIN [1][450/3880]	Time 0.066 (0.165)	Data 1.26e-04 (7.36e-04)	Tok/s 79169 (87646)	Loss/tok 2.8355 (3.4091)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][460/3880]	Time 0.124 (0.166)	Data 1.27e-04 (7.22e-04)	Tok/s 84792 (87639)	Loss/tok 3.1834 (3.4119)	LR 2.000e-03
0: TRAIN [1][470/3880]	Time 0.125 (0.166)	Data 1.13e-04 (7.10e-04)	Tok/s 82057 (87695)	Loss/tok 3.1651 (3.4150)	LR 2.000e-03
0: TRAIN [1][480/3880]	Time 0.127 (0.166)	Data 1.22e-04 (6.97e-04)	Tok/s 81410 (87640)	Loss/tok 3.1590 (3.4133)	LR 2.000e-03
0: TRAIN [1][490/3880]	Time 0.186 (0.166)	Data 1.15e-04 (6.85e-04)	Tok/s 91328 (87653)	Loss/tok 3.3731 (3.4139)	LR 2.000e-03
0: TRAIN [1][500/3880]	Time 0.124 (0.166)	Data 1.15e-04 (6.74e-04)	Tok/s 81726 (87647)	Loss/tok 3.1566 (3.4139)	LR 2.000e-03
0: TRAIN [1][510/3880]	Time 0.125 (0.166)	Data 1.27e-04 (6.63e-04)	Tok/s 81827 (87654)	Loss/tok 3.1060 (3.4126)	LR 2.000e-03
0: TRAIN [1][520/3880]	Time 0.183 (0.165)	Data 1.12e-04 (6.53e-04)	Tok/s 91978 (87632)	Loss/tok 3.4121 (3.4110)	LR 2.000e-03
0: TRAIN [1][530/3880]	Time 0.124 (0.165)	Data 1.02e-04 (6.43e-04)	Tok/s 84094 (87623)	Loss/tok 3.2741 (3.4108)	LR 2.000e-03
0: TRAIN [1][540/3880]	Time 0.126 (0.165)	Data 1.27e-04 (6.33e-04)	Tok/s 82541 (87596)	Loss/tok 3.1997 (3.4098)	LR 2.000e-03
0: TRAIN [1][550/3880]	Time 0.125 (0.165)	Data 1.34e-04 (6.24e-04)	Tok/s 82314 (87605)	Loss/tok 3.1652 (3.4092)	LR 2.000e-03
0: TRAIN [1][560/3880]	Time 0.183 (0.165)	Data 1.30e-04 (6.15e-04)	Tok/s 92390 (87620)	Loss/tok 3.2684 (3.4072)	LR 2.000e-03
0: TRAIN [1][570/3880]	Time 0.123 (0.164)	Data 1.20e-04 (6.06e-04)	Tok/s 83449 (87589)	Loss/tok 3.1898 (3.4057)	LR 2.000e-03
0: TRAIN [1][580/3880]	Time 0.125 (0.164)	Data 1.30e-04 (5.98e-04)	Tok/s 82336 (87567)	Loss/tok 3.1826 (3.4046)	LR 2.000e-03
0: TRAIN [1][590/3880]	Time 0.125 (0.164)	Data 1.26e-04 (5.90e-04)	Tok/s 82841 (87582)	Loss/tok 3.2042 (3.4048)	LR 2.000e-03
0: TRAIN [1][600/3880]	Time 0.125 (0.164)	Data 1.12e-04 (5.82e-04)	Tok/s 83314 (87544)	Loss/tok 3.0482 (3.4038)	LR 2.000e-03
0: TRAIN [1][610/3880]	Time 0.067 (0.164)	Data 1.04e-04 (5.74e-04)	Tok/s 80258 (87545)	Loss/tok 2.7864 (3.4032)	LR 2.000e-03
0: TRAIN [1][620/3880]	Time 0.127 (0.164)	Data 1.32e-04 (5.67e-04)	Tok/s 82437 (87548)	Loss/tok 3.2561 (3.4016)	LR 2.000e-03
0: TRAIN [1][630/3880]	Time 0.184 (0.163)	Data 1.13e-04 (5.60e-04)	Tok/s 90757 (87515)	Loss/tok 3.3640 (3.3999)	LR 2.000e-03
0: TRAIN [1][640/3880]	Time 0.125 (0.163)	Data 1.04e-04 (5.53e-04)	Tok/s 82825 (87465)	Loss/tok 3.1385 (3.3987)	LR 2.000e-03
0: TRAIN [1][650/3880]	Time 0.182 (0.162)	Data 1.02e-04 (5.46e-04)	Tok/s 92005 (87437)	Loss/tok 3.4011 (3.3969)	LR 2.000e-03
0: TRAIN [1][660/3880]	Time 0.125 (0.162)	Data 1.13e-04 (5.39e-04)	Tok/s 83290 (87420)	Loss/tok 3.1823 (3.3971)	LR 2.000e-03
0: TRAIN [1][670/3880]	Time 0.125 (0.162)	Data 1.07e-04 (5.33e-04)	Tok/s 83087 (87412)	Loss/tok 3.2096 (3.3956)	LR 2.000e-03
0: TRAIN [1][680/3880]	Time 0.183 (0.162)	Data 9.99e-05 (5.27e-04)	Tok/s 92623 (87421)	Loss/tok 3.3634 (3.3956)	LR 2.000e-03
0: TRAIN [1][690/3880]	Time 0.243 (0.163)	Data 1.02e-04 (5.21e-04)	Tok/s 96411 (87465)	Loss/tok 3.5021 (3.3974)	LR 2.000e-03
0: TRAIN [1][700/3880]	Time 0.068 (0.163)	Data 1.42e-04 (5.15e-04)	Tok/s 75797 (87476)	Loss/tok 2.6128 (3.3992)	LR 2.000e-03
0: TRAIN [1][710/3880]	Time 0.123 (0.163)	Data 1.04e-04 (5.09e-04)	Tok/s 82843 (87499)	Loss/tok 3.0947 (3.3999)	LR 2.000e-03
0: TRAIN [1][720/3880]	Time 0.124 (0.163)	Data 9.99e-05 (5.04e-04)	Tok/s 84712 (87505)	Loss/tok 3.2027 (3.3989)	LR 2.000e-03
0: TRAIN [1][730/3880]	Time 0.184 (0.163)	Data 1.12e-04 (4.98e-04)	Tok/s 89855 (87495)	Loss/tok 3.5178 (3.3994)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][740/3880]	Time 0.313 (0.163)	Data 1.19e-04 (4.93e-04)	Tok/s 96324 (87506)	Loss/tok 3.6574 (3.4005)	LR 2.000e-03
0: TRAIN [1][750/3880]	Time 0.124 (0.163)	Data 1.20e-04 (4.88e-04)	Tok/s 83118 (87477)	Loss/tok 3.1783 (3.3993)	LR 2.000e-03
0: TRAIN [1][760/3880]	Time 0.243 (0.163)	Data 9.16e-05 (4.83e-04)	Tok/s 96549 (87471)	Loss/tok 3.5211 (3.3997)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][770/3880]	Time 0.242 (0.163)	Data 9.75e-05 (4.78e-04)	Tok/s 95142 (87467)	Loss/tok 3.6488 (3.4009)	LR 2.000e-03
0: TRAIN [1][780/3880]	Time 0.126 (0.163)	Data 1.05e-04 (4.74e-04)	Tok/s 81941 (87448)	Loss/tok 3.1810 (3.3997)	LR 2.000e-03
0: TRAIN [1][790/3880]	Time 0.182 (0.163)	Data 9.18e-05 (4.69e-04)	Tok/s 93245 (87459)	Loss/tok 3.4766 (3.3998)	LR 2.000e-03
0: TRAIN [1][800/3880]	Time 0.125 (0.163)	Data 9.85e-05 (4.64e-04)	Tok/s 81725 (87465)	Loss/tok 3.1904 (3.4010)	LR 2.000e-03
0: TRAIN [1][810/3880]	Time 0.244 (0.163)	Data 9.61e-05 (4.60e-04)	Tok/s 95824 (87465)	Loss/tok 3.5888 (3.4024)	LR 2.000e-03
0: TRAIN [1][820/3880]	Time 0.124 (0.163)	Data 1.05e-04 (4.56e-04)	Tok/s 82064 (87454)	Loss/tok 3.1878 (3.4031)	LR 2.000e-03
0: TRAIN [1][830/3880]	Time 0.123 (0.163)	Data 9.51e-05 (4.51e-04)	Tok/s 83648 (87442)	Loss/tok 3.1682 (3.4021)	LR 2.000e-03
0: TRAIN [1][840/3880]	Time 0.182 (0.163)	Data 1.16e-04 (4.47e-04)	Tok/s 93032 (87468)	Loss/tok 3.3438 (3.4022)	LR 2.000e-03
0: TRAIN [1][850/3880]	Time 0.126 (0.163)	Data 9.42e-05 (4.43e-04)	Tok/s 83358 (87455)	Loss/tok 3.2209 (3.4007)	LR 2.000e-03
0: TRAIN [1][860/3880]	Time 0.183 (0.163)	Data 1.10e-04 (4.39e-04)	Tok/s 92259 (87456)	Loss/tok 3.3268 (3.3994)	LR 2.000e-03
0: TRAIN [1][870/3880]	Time 0.127 (0.162)	Data 1.28e-04 (4.36e-04)	Tok/s 79688 (87444)	Loss/tok 3.2192 (3.3990)	LR 2.000e-03
0: TRAIN [1][880/3880]	Time 0.123 (0.162)	Data 1.39e-04 (4.32e-04)	Tok/s 83597 (87446)	Loss/tok 3.1733 (3.3987)	LR 2.000e-03
0: TRAIN [1][890/3880]	Time 0.182 (0.162)	Data 1.13e-04 (4.29e-04)	Tok/s 91342 (87424)	Loss/tok 3.2987 (3.3982)	LR 2.000e-03
0: TRAIN [1][900/3880]	Time 0.182 (0.162)	Data 1.17e-04 (4.25e-04)	Tok/s 92224 (87437)	Loss/tok 3.4401 (3.3978)	LR 2.000e-03
0: TRAIN [1][910/3880]	Time 0.244 (0.162)	Data 1.21e-04 (4.22e-04)	Tok/s 95072 (87452)	Loss/tok 3.6089 (3.3977)	LR 2.000e-03
0: TRAIN [1][920/3880]	Time 0.124 (0.163)	Data 1.18e-04 (4.19e-04)	Tok/s 84562 (87481)	Loss/tok 3.2220 (3.3994)	LR 2.000e-03
0: TRAIN [1][930/3880]	Time 0.319 (0.162)	Data 1.21e-04 (4.15e-04)	Tok/s 93664 (87477)	Loss/tok 3.7423 (3.3989)	LR 2.000e-03
0: TRAIN [1][940/3880]	Time 0.125 (0.162)	Data 1.35e-04 (4.12e-04)	Tok/s 82771 (87453)	Loss/tok 3.0755 (3.3979)	LR 2.000e-03
0: TRAIN [1][950/3880]	Time 0.184 (0.162)	Data 1.10e-04 (4.09e-04)	Tok/s 91292 (87445)	Loss/tok 3.3731 (3.3978)	LR 2.000e-03
0: TRAIN [1][960/3880]	Time 0.126 (0.162)	Data 1.24e-04 (4.06e-04)	Tok/s 83159 (87452)	Loss/tok 3.1618 (3.3980)	LR 2.000e-03
0: TRAIN [1][970/3880]	Time 0.184 (0.162)	Data 1.22e-04 (4.03e-04)	Tok/s 91130 (87427)	Loss/tok 3.4553 (3.3965)	LR 2.000e-03
0: TRAIN [1][980/3880]	Time 0.244 (0.162)	Data 1.14e-04 (4.00e-04)	Tok/s 95737 (87467)	Loss/tok 3.5745 (3.3978)	LR 2.000e-03
0: TRAIN [1][990/3880]	Time 0.124 (0.162)	Data 1.18e-04 (3.98e-04)	Tok/s 82197 (87431)	Loss/tok 3.2767 (3.3963)	LR 2.000e-03
0: TRAIN [1][1000/3880]	Time 0.317 (0.162)	Data 1.35e-04 (3.95e-04)	Tok/s 94995 (87447)	Loss/tok 3.7658 (3.3974)	LR 2.000e-03
0: TRAIN [1][1010/3880]	Time 0.245 (0.163)	Data 1.37e-04 (3.92e-04)	Tok/s 96019 (87471)	Loss/tok 3.5757 (3.3978)	LR 2.000e-03
0: TRAIN [1][1020/3880]	Time 0.067 (0.163)	Data 1.02e-04 (3.89e-04)	Tok/s 80807 (87471)	Loss/tok 2.6452 (3.3975)	LR 2.000e-03
0: TRAIN [1][1030/3880]	Time 0.183 (0.163)	Data 1.12e-04 (3.87e-04)	Tok/s 92987 (87487)	Loss/tok 3.3306 (3.3975)	LR 2.000e-03
0: TRAIN [1][1040/3880]	Time 0.067 (0.163)	Data 9.61e-05 (3.84e-04)	Tok/s 78258 (87453)	Loss/tok 2.7775 (3.3964)	LR 2.000e-03
0: TRAIN [1][1050/3880]	Time 0.124 (0.163)	Data 1.09e-04 (3.81e-04)	Tok/s 82258 (87453)	Loss/tok 3.1797 (3.3969)	LR 2.000e-03
0: TRAIN [1][1060/3880]	Time 0.183 (0.163)	Data 1.24e-04 (3.79e-04)	Tok/s 91527 (87469)	Loss/tok 3.4168 (3.3966)	LR 2.000e-03
0: TRAIN [1][1070/3880]	Time 0.246 (0.163)	Data 1.18e-04 (3.76e-04)	Tok/s 95111 (87486)	Loss/tok 3.5595 (3.3981)	LR 2.000e-03
0: TRAIN [1][1080/3880]	Time 0.125 (0.163)	Data 1.02e-04 (3.74e-04)	Tok/s 83250 (87484)	Loss/tok 3.1331 (3.3973)	LR 2.000e-03
0: TRAIN [1][1090/3880]	Time 0.186 (0.163)	Data 1.19e-04 (3.71e-04)	Tok/s 90547 (87511)	Loss/tok 3.3431 (3.3981)	LR 2.000e-03
0: TRAIN [1][1100/3880]	Time 0.185 (0.163)	Data 1.28e-04 (3.69e-04)	Tok/s 90873 (87498)	Loss/tok 3.4416 (3.3973)	LR 2.000e-03
0: TRAIN [1][1110/3880]	Time 0.184 (0.163)	Data 1.04e-04 (3.67e-04)	Tok/s 90788 (87511)	Loss/tok 3.4189 (3.3969)	LR 2.000e-03
0: TRAIN [1][1120/3880]	Time 0.183 (0.163)	Data 1.22e-04 (3.65e-04)	Tok/s 91244 (87521)	Loss/tok 3.3781 (3.3970)	LR 2.000e-03
0: TRAIN [1][1130/3880]	Time 0.184 (0.163)	Data 1.17e-04 (3.62e-04)	Tok/s 92006 (87525)	Loss/tok 3.4270 (3.3964)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1140/3880]	Time 0.126 (0.163)	Data 1.04e-04 (3.60e-04)	Tok/s 80556 (87483)	Loss/tok 3.0985 (3.3957)	LR 2.000e-03
0: TRAIN [1][1150/3880]	Time 0.067 (0.163)	Data 1.13e-04 (3.58e-04)	Tok/s 78660 (87469)	Loss/tok 2.7868 (3.3947)	LR 2.000e-03
0: TRAIN [1][1160/3880]	Time 0.244 (0.163)	Data 1.26e-04 (3.56e-04)	Tok/s 95097 (87463)	Loss/tok 3.6183 (3.3946)	LR 2.000e-03
0: TRAIN [1][1170/3880]	Time 0.313 (0.163)	Data 1.02e-04 (3.54e-04)	Tok/s 93437 (87456)	Loss/tok 3.8380 (3.3942)	LR 2.000e-03
0: TRAIN [1][1180/3880]	Time 0.068 (0.163)	Data 1.20e-04 (3.52e-04)	Tok/s 74115 (87460)	Loss/tok 2.6993 (3.3944)	LR 2.000e-03
0: TRAIN [1][1190/3880]	Time 0.181 (0.163)	Data 1.07e-04 (3.50e-04)	Tok/s 95118 (87455)	Loss/tok 3.4881 (3.3937)	LR 2.000e-03
0: TRAIN [1][1200/3880]	Time 0.126 (0.163)	Data 1.29e-04 (3.48e-04)	Tok/s 80939 (87442)	Loss/tok 3.1109 (3.3930)	LR 2.000e-03
0: TRAIN [1][1210/3880]	Time 0.315 (0.163)	Data 1.31e-04 (3.46e-04)	Tok/s 94321 (87425)	Loss/tok 3.7438 (3.3929)	LR 2.000e-03
0: TRAIN [1][1220/3880]	Time 0.124 (0.162)	Data 1.24e-04 (3.44e-04)	Tok/s 82983 (87403)	Loss/tok 3.1408 (3.3920)	LR 2.000e-03
0: TRAIN [1][1230/3880]	Time 0.185 (0.163)	Data 1.14e-04 (3.42e-04)	Tok/s 90441 (87407)	Loss/tok 3.4094 (3.3923)	LR 2.000e-03
0: TRAIN [1][1240/3880]	Time 0.127 (0.163)	Data 1.38e-04 (3.41e-04)	Tok/s 80180 (87416)	Loss/tok 3.1441 (3.3926)	LR 2.000e-03
0: TRAIN [1][1250/3880]	Time 0.184 (0.163)	Data 1.18e-04 (3.39e-04)	Tok/s 92230 (87427)	Loss/tok 3.2993 (3.3928)	LR 2.000e-03
0: TRAIN [1][1260/3880]	Time 0.243 (0.163)	Data 1.27e-04 (3.37e-04)	Tok/s 96559 (87473)	Loss/tok 3.4952 (3.3937)	LR 2.000e-03
0: TRAIN [1][1270/3880]	Time 0.185 (0.163)	Data 1.33e-04 (3.36e-04)	Tok/s 90606 (87473)	Loss/tok 3.2947 (3.3932)	LR 2.000e-03
0: TRAIN [1][1280/3880]	Time 0.124 (0.163)	Data 1.31e-04 (3.34e-04)	Tok/s 84345 (87467)	Loss/tok 3.2513 (3.3937)	LR 2.000e-03
0: TRAIN [1][1290/3880]	Time 0.125 (0.163)	Data 1.11e-04 (3.32e-04)	Tok/s 83131 (87474)	Loss/tok 3.1388 (3.3940)	LR 2.000e-03
0: TRAIN [1][1300/3880]	Time 0.126 (0.163)	Data 1.35e-04 (3.31e-04)	Tok/s 80847 (87477)	Loss/tok 3.1668 (3.3937)	LR 2.000e-03
0: TRAIN [1][1310/3880]	Time 0.246 (0.163)	Data 1.34e-04 (3.29e-04)	Tok/s 95112 (87489)	Loss/tok 3.6510 (3.3939)	LR 2.000e-03
0: TRAIN [1][1320/3880]	Time 0.127 (0.163)	Data 1.34e-04 (3.28e-04)	Tok/s 81596 (87466)	Loss/tok 3.0767 (3.3926)	LR 2.000e-03
0: TRAIN [1][1330/3880]	Time 0.243 (0.163)	Data 1.13e-04 (3.26e-04)	Tok/s 95728 (87459)	Loss/tok 3.6266 (3.3929)	LR 2.000e-03
0: TRAIN [1][1340/3880]	Time 0.126 (0.163)	Data 1.28e-04 (3.25e-04)	Tok/s 81174 (87451)	Loss/tok 3.1882 (3.3928)	LR 2.000e-03
0: TRAIN [1][1350/3880]	Time 0.125 (0.163)	Data 1.06e-04 (3.23e-04)	Tok/s 83856 (87446)	Loss/tok 3.1141 (3.3929)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1360/3880]	Time 0.183 (0.163)	Data 1.13e-04 (3.21e-04)	Tok/s 91052 (87458)	Loss/tok 3.2587 (3.3931)	LR 2.000e-03
0: TRAIN [1][1370/3880]	Time 0.125 (0.163)	Data 1.07e-04 (3.20e-04)	Tok/s 83660 (87441)	Loss/tok 3.1063 (3.3924)	LR 2.000e-03
0: TRAIN [1][1380/3880]	Time 0.183 (0.163)	Data 1.06e-04 (3.18e-04)	Tok/s 92041 (87469)	Loss/tok 3.4198 (3.3937)	LR 2.000e-03
0: TRAIN [1][1390/3880]	Time 0.123 (0.163)	Data 1.06e-04 (3.17e-04)	Tok/s 85077 (87464)	Loss/tok 3.1037 (3.3931)	LR 2.000e-03
0: TRAIN [1][1400/3880]	Time 0.123 (0.163)	Data 1.12e-04 (3.16e-04)	Tok/s 83175 (87460)	Loss/tok 3.2248 (3.3933)	LR 2.000e-03
0: TRAIN [1][1410/3880]	Time 0.182 (0.163)	Data 1.24e-04 (3.14e-04)	Tok/s 92116 (87467)	Loss/tok 3.3672 (3.3928)	LR 2.000e-03
0: TRAIN [1][1420/3880]	Time 0.182 (0.163)	Data 1.10e-04 (3.13e-04)	Tok/s 90754 (87473)	Loss/tok 3.4386 (3.3923)	LR 2.000e-03
0: TRAIN [1][1430/3880]	Time 0.183 (0.163)	Data 1.10e-04 (3.11e-04)	Tok/s 92231 (87459)	Loss/tok 3.2870 (3.3915)	LR 2.000e-03
0: TRAIN [1][1440/3880]	Time 0.244 (0.163)	Data 1.35e-04 (3.10e-04)	Tok/s 96027 (87469)	Loss/tok 3.5049 (3.3915)	LR 2.000e-03
0: TRAIN [1][1450/3880]	Time 0.124 (0.163)	Data 1.56e-04 (3.09e-04)	Tok/s 81993 (87472)	Loss/tok 3.1377 (3.3915)	LR 2.000e-03
0: TRAIN [1][1460/3880]	Time 0.181 (0.163)	Data 1.31e-04 (3.08e-04)	Tok/s 93954 (87469)	Loss/tok 3.4130 (3.3910)	LR 2.000e-03
0: TRAIN [1][1470/3880]	Time 0.123 (0.163)	Data 1.12e-04 (3.06e-04)	Tok/s 84124 (87455)	Loss/tok 3.1715 (3.3904)	LR 2.000e-03
0: TRAIN [1][1480/3880]	Time 0.182 (0.163)	Data 1.08e-04 (3.05e-04)	Tok/s 91750 (87448)	Loss/tok 3.3227 (3.3902)	LR 2.000e-03
0: TRAIN [1][1490/3880]	Time 0.124 (0.163)	Data 1.26e-04 (3.04e-04)	Tok/s 84697 (87452)	Loss/tok 3.2093 (3.3901)	LR 2.000e-03
0: TRAIN [1][1500/3880]	Time 0.181 (0.163)	Data 1.30e-04 (3.03e-04)	Tok/s 92852 (87460)	Loss/tok 3.4038 (3.3898)	LR 2.000e-03
0: TRAIN [1][1510/3880]	Time 0.126 (0.163)	Data 1.14e-04 (3.01e-04)	Tok/s 81350 (87464)	Loss/tok 3.2390 (3.3901)	LR 2.000e-03
0: TRAIN [1][1520/3880]	Time 0.127 (0.163)	Data 1.20e-04 (3.00e-04)	Tok/s 81299 (87481)	Loss/tok 3.1468 (3.3901)	LR 2.000e-03
0: TRAIN [1][1530/3880]	Time 0.243 (0.163)	Data 1.28e-04 (2.99e-04)	Tok/s 96350 (87469)	Loss/tok 3.5546 (3.3896)	LR 2.000e-03
0: TRAIN [1][1540/3880]	Time 0.243 (0.163)	Data 1.06e-04 (2.98e-04)	Tok/s 95463 (87460)	Loss/tok 3.4465 (3.3891)	LR 2.000e-03
0: TRAIN [1][1550/3880]	Time 0.125 (0.163)	Data 1.48e-04 (2.97e-04)	Tok/s 80367 (87446)	Loss/tok 3.1986 (3.3886)	LR 2.000e-03
0: TRAIN [1][1560/3880]	Time 0.247 (0.163)	Data 1.43e-04 (2.96e-04)	Tok/s 94540 (87463)	Loss/tok 3.6275 (3.3890)	LR 2.000e-03
0: TRAIN [1][1570/3880]	Time 0.126 (0.163)	Data 1.19e-04 (2.95e-04)	Tok/s 82980 (87482)	Loss/tok 3.0808 (3.3898)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1580/3880]	Time 0.125 (0.163)	Data 1.23e-04 (2.93e-04)	Tok/s 80543 (87492)	Loss/tok 3.1854 (3.3906)	LR 2.000e-03
0: TRAIN [1][1590/3880]	Time 0.066 (0.163)	Data 1.18e-04 (2.92e-04)	Tok/s 79340 (87510)	Loss/tok 2.6703 (3.3912)	LR 2.000e-03
0: TRAIN [1][1600/3880]	Time 0.066 (0.163)	Data 1.16e-04 (2.91e-04)	Tok/s 77886 (87489)	Loss/tok 2.6839 (3.3904)	LR 2.000e-03
0: TRAIN [1][1610/3880]	Time 0.181 (0.163)	Data 1.27e-04 (2.90e-04)	Tok/s 92365 (87484)	Loss/tok 3.3231 (3.3899)	LR 2.000e-03
0: TRAIN [1][1620/3880]	Time 0.070 (0.163)	Data 1.40e-04 (2.89e-04)	Tok/s 76088 (87482)	Loss/tok 2.7267 (3.3894)	LR 2.000e-03
0: TRAIN [1][1630/3880]	Time 0.181 (0.163)	Data 1.15e-04 (2.88e-04)	Tok/s 93233 (87492)	Loss/tok 3.4080 (3.3900)	LR 2.000e-03
0: TRAIN [1][1640/3880]	Time 0.317 (0.163)	Data 1.02e-04 (2.87e-04)	Tok/s 93314 (87505)	Loss/tok 3.5527 (3.3908)	LR 2.000e-03
0: TRAIN [1][1650/3880]	Time 0.126 (0.163)	Data 1.30e-04 (2.86e-04)	Tok/s 80049 (87518)	Loss/tok 3.1334 (3.3907)	LR 2.000e-03
0: TRAIN [1][1660/3880]	Time 0.126 (0.163)	Data 1.05e-04 (2.85e-04)	Tok/s 81565 (87512)	Loss/tok 3.1116 (3.3901)	LR 2.000e-03
0: TRAIN [1][1670/3880]	Time 0.124 (0.163)	Data 1.08e-04 (2.84e-04)	Tok/s 84239 (87531)	Loss/tok 3.1731 (3.3905)	LR 2.000e-03
0: TRAIN [1][1680/3880]	Time 0.185 (0.163)	Data 1.18e-04 (2.83e-04)	Tok/s 90369 (87526)	Loss/tok 3.2804 (3.3899)	LR 2.000e-03
0: TRAIN [1][1690/3880]	Time 0.126 (0.163)	Data 1.05e-04 (2.82e-04)	Tok/s 83716 (87503)	Loss/tok 3.0878 (3.3890)	LR 2.000e-03
0: TRAIN [1][1700/3880]	Time 0.240 (0.163)	Data 1.09e-04 (2.81e-04)	Tok/s 96301 (87478)	Loss/tok 3.6637 (3.3883)	LR 2.000e-03
0: TRAIN [1][1710/3880]	Time 0.245 (0.163)	Data 1.20e-04 (2.80e-04)	Tok/s 96189 (87488)	Loss/tok 3.4570 (3.3884)	LR 2.000e-03
0: TRAIN [1][1720/3880]	Time 0.124 (0.163)	Data 1.04e-04 (2.79e-04)	Tok/s 84047 (87473)	Loss/tok 3.1656 (3.3877)	LR 2.000e-03
0: TRAIN [1][1730/3880]	Time 0.128 (0.163)	Data 1.51e-04 (2.78e-04)	Tok/s 81114 (87466)	Loss/tok 3.1226 (3.3878)	LR 2.000e-03
0: TRAIN [1][1740/3880]	Time 0.124 (0.163)	Data 1.08e-04 (2.77e-04)	Tok/s 83021 (87456)	Loss/tok 3.1061 (3.3872)	LR 2.000e-03
0: TRAIN [1][1750/3880]	Time 0.242 (0.163)	Data 1.07e-04 (2.76e-04)	Tok/s 95659 (87449)	Loss/tok 3.5780 (3.3866)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][1760/3880]	Time 0.183 (0.163)	Data 1.24e-04 (2.75e-04)	Tok/s 91226 (87458)	Loss/tok 3.3788 (3.3872)	LR 2.000e-03
0: TRAIN [1][1770/3880]	Time 0.127 (0.163)	Data 1.13e-04 (2.74e-04)	Tok/s 81432 (87453)	Loss/tok 3.1876 (3.3874)	LR 2.000e-03
0: TRAIN [1][1780/3880]	Time 0.124 (0.163)	Data 1.07e-04 (2.74e-04)	Tok/s 83231 (87456)	Loss/tok 3.0913 (3.3876)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][1790/3880]	Time 0.184 (0.163)	Data 1.08e-04 (2.73e-04)	Tok/s 90537 (87459)	Loss/tok 3.3345 (3.3883)	LR 2.000e-03
0: TRAIN [1][1800/3880]	Time 0.181 (0.163)	Data 1.05e-04 (2.72e-04)	Tok/s 92460 (87466)	Loss/tok 3.3895 (3.3884)	LR 2.000e-03
0: TRAIN [1][1810/3880]	Time 0.126 (0.163)	Data 1.25e-04 (2.71e-04)	Tok/s 81933 (87458)	Loss/tok 3.0830 (3.3881)	LR 2.000e-03
0: TRAIN [1][1820/3880]	Time 0.243 (0.163)	Data 1.43e-04 (2.70e-04)	Tok/s 95009 (87449)	Loss/tok 3.6159 (3.3876)	LR 2.000e-03
0: TRAIN [1][1830/3880]	Time 0.124 (0.163)	Data 1.04e-04 (2.69e-04)	Tok/s 82228 (87449)	Loss/tok 3.1338 (3.3872)	LR 2.000e-03
0: TRAIN [1][1840/3880]	Time 0.315 (0.163)	Data 1.36e-04 (2.68e-04)	Tok/s 94194 (87449)	Loss/tok 3.7606 (3.3873)	LR 2.000e-03
0: TRAIN [1][1850/3880]	Time 0.068 (0.163)	Data 1.08e-04 (2.68e-04)	Tok/s 74456 (87445)	Loss/tok 2.7206 (3.3871)	LR 2.000e-03
0: TRAIN [1][1860/3880]	Time 0.068 (0.163)	Data 1.32e-04 (2.67e-04)	Tok/s 78257 (87459)	Loss/tok 2.6526 (3.3879)	LR 2.000e-03
0: TRAIN [1][1870/3880]	Time 0.124 (0.163)	Data 1.29e-04 (2.66e-04)	Tok/s 83254 (87453)	Loss/tok 3.1570 (3.3873)	LR 2.000e-03
0: TRAIN [1][1880/3880]	Time 0.125 (0.163)	Data 1.21e-04 (2.65e-04)	Tok/s 81365 (87442)	Loss/tok 3.0905 (3.3872)	LR 2.000e-03
0: TRAIN [1][1890/3880]	Time 0.245 (0.163)	Data 1.07e-04 (2.64e-04)	Tok/s 94725 (87457)	Loss/tok 3.4843 (3.3872)	LR 2.000e-03
0: TRAIN [1][1900/3880]	Time 0.186 (0.163)	Data 1.38e-04 (2.64e-04)	Tok/s 90313 (87449)	Loss/tok 3.2947 (3.3867)	LR 2.000e-03
0: TRAIN [1][1910/3880]	Time 0.127 (0.163)	Data 1.45e-04 (2.63e-04)	Tok/s 80960 (87461)	Loss/tok 3.1719 (3.3872)	LR 2.000e-03
0: TRAIN [1][1920/3880]	Time 0.124 (0.163)	Data 1.29e-04 (2.62e-04)	Tok/s 85444 (87454)	Loss/tok 3.1641 (3.3866)	LR 2.000e-03
0: TRAIN [1][1930/3880]	Time 0.313 (0.163)	Data 1.31e-04 (2.62e-04)	Tok/s 96397 (87462)	Loss/tok 3.5943 (3.3867)	LR 2.000e-03
0: TRAIN [1][1940/3880]	Time 0.125 (0.163)	Data 1.39e-04 (2.61e-04)	Tok/s 83115 (87459)	Loss/tok 3.2118 (3.3865)	LR 2.000e-03
0: TRAIN [1][1950/3880]	Time 0.243 (0.163)	Data 1.43e-04 (2.60e-04)	Tok/s 97190 (87457)	Loss/tok 3.4388 (3.3863)	LR 2.000e-03
0: TRAIN [1][1960/3880]	Time 0.068 (0.163)	Data 1.39e-04 (2.59e-04)	Tok/s 77920 (87442)	Loss/tok 2.7064 (3.3857)	LR 2.000e-03
0: TRAIN [1][1970/3880]	Time 0.183 (0.163)	Data 1.23e-04 (2.59e-04)	Tok/s 91904 (87457)	Loss/tok 3.3860 (3.3860)	LR 2.000e-03
0: TRAIN [1][1980/3880]	Time 0.125 (0.163)	Data 1.38e-04 (2.58e-04)	Tok/s 83289 (87447)	Loss/tok 3.0358 (3.3856)	LR 2.000e-03
0: TRAIN [1][1990/3880]	Time 0.124 (0.163)	Data 1.31e-04 (2.57e-04)	Tok/s 84019 (87453)	Loss/tok 3.0792 (3.3855)	LR 2.000e-03
0: TRAIN [1][2000/3880]	Time 0.182 (0.163)	Data 1.38e-04 (2.57e-04)	Tok/s 92979 (87458)	Loss/tok 3.2924 (3.3859)	LR 2.000e-03
0: TRAIN [1][2010/3880]	Time 0.125 (0.163)	Data 1.32e-04 (2.56e-04)	Tok/s 82293 (87443)	Loss/tok 2.9964 (3.3852)	LR 2.000e-03
0: TRAIN [1][2020/3880]	Time 0.183 (0.163)	Data 1.28e-04 (2.56e-04)	Tok/s 90458 (87433)	Loss/tok 3.3847 (3.3844)	LR 2.000e-03
0: TRAIN [1][2030/3880]	Time 0.245 (0.163)	Data 1.35e-04 (2.55e-04)	Tok/s 96075 (87433)	Loss/tok 3.4315 (3.3849)	LR 2.000e-03
0: TRAIN [1][2040/3880]	Time 0.066 (0.163)	Data 1.34e-04 (2.54e-04)	Tok/s 80674 (87424)	Loss/tok 2.6895 (3.3848)	LR 2.000e-03
0: TRAIN [1][2050/3880]	Time 0.124 (0.163)	Data 1.14e-04 (2.54e-04)	Tok/s 83298 (87414)	Loss/tok 3.1480 (3.3845)	LR 2.000e-03
0: TRAIN [1][2060/3880]	Time 0.183 (0.163)	Data 1.19e-04 (2.53e-04)	Tok/s 91776 (87407)	Loss/tok 3.3001 (3.3844)	LR 2.000e-03
0: TRAIN [1][2070/3880]	Time 0.246 (0.163)	Data 1.12e-04 (2.52e-04)	Tok/s 96353 (87416)	Loss/tok 3.4351 (3.3843)	LR 2.000e-03
0: TRAIN [1][2080/3880]	Time 0.246 (0.163)	Data 1.27e-04 (2.52e-04)	Tok/s 95328 (87417)	Loss/tok 3.5508 (3.3841)	LR 2.000e-03
0: TRAIN [1][2090/3880]	Time 0.244 (0.163)	Data 1.11e-04 (2.51e-04)	Tok/s 95994 (87410)	Loss/tok 3.5893 (3.3837)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2100/3880]	Time 0.319 (0.163)	Data 1.28e-04 (2.51e-04)	Tok/s 91444 (87409)	Loss/tok 3.8273 (3.3840)	LR 2.000e-03
0: TRAIN [1][2110/3880]	Time 0.124 (0.163)	Data 1.18e-04 (2.50e-04)	Tok/s 85402 (87393)	Loss/tok 3.1770 (3.3832)	LR 2.000e-03
0: TRAIN [1][2120/3880]	Time 0.125 (0.163)	Data 1.34e-04 (2.49e-04)	Tok/s 83125 (87386)	Loss/tok 3.2008 (3.3829)	LR 2.000e-03
0: TRAIN [1][2130/3880]	Time 0.067 (0.163)	Data 1.44e-04 (2.49e-04)	Tok/s 79063 (87387)	Loss/tok 2.7193 (3.3825)	LR 2.000e-03
0: TRAIN [1][2140/3880]	Time 0.124 (0.163)	Data 1.30e-04 (2.48e-04)	Tok/s 83543 (87388)	Loss/tok 3.1543 (3.3823)	LR 2.000e-03
0: TRAIN [1][2150/3880]	Time 0.183 (0.163)	Data 1.33e-04 (2.48e-04)	Tok/s 91198 (87390)	Loss/tok 3.4982 (3.3824)	LR 2.000e-03
0: TRAIN [1][2160/3880]	Time 0.183 (0.163)	Data 1.22e-04 (2.47e-04)	Tok/s 92390 (87390)	Loss/tok 3.3829 (3.3818)	LR 2.000e-03
0: TRAIN [1][2170/3880]	Time 0.124 (0.163)	Data 1.15e-04 (2.46e-04)	Tok/s 81863 (87381)	Loss/tok 3.0948 (3.3812)	LR 2.000e-03
0: TRAIN [1][2180/3880]	Time 0.121 (0.163)	Data 1.18e-04 (2.46e-04)	Tok/s 84589 (87391)	Loss/tok 3.1185 (3.3811)	LR 2.000e-03
0: TRAIN [1][2190/3880]	Time 0.314 (0.163)	Data 1.15e-04 (2.45e-04)	Tok/s 95770 (87396)	Loss/tok 3.7018 (3.3818)	LR 2.000e-03
0: TRAIN [1][2200/3880]	Time 0.124 (0.163)	Data 1.33e-04 (2.45e-04)	Tok/s 82292 (87414)	Loss/tok 3.2281 (3.3820)	LR 2.000e-03
0: TRAIN [1][2210/3880]	Time 0.183 (0.163)	Data 1.22e-04 (2.44e-04)	Tok/s 90872 (87428)	Loss/tok 3.3544 (3.3826)	LR 2.000e-03
0: TRAIN [1][2220/3880]	Time 0.317 (0.163)	Data 1.63e-04 (2.44e-04)	Tok/s 95123 (87445)	Loss/tok 3.6036 (3.3830)	LR 2.000e-03
0: TRAIN [1][2230/3880]	Time 0.182 (0.163)	Data 1.37e-04 (2.43e-04)	Tok/s 92882 (87440)	Loss/tok 3.3002 (3.3825)	LR 2.000e-03
0: TRAIN [1][2240/3880]	Time 0.124 (0.163)	Data 1.32e-04 (2.43e-04)	Tok/s 83583 (87452)	Loss/tok 3.2528 (3.3829)	LR 2.000e-03
0: TRAIN [1][2250/3880]	Time 0.246 (0.163)	Data 1.24e-04 (2.42e-04)	Tok/s 94377 (87448)	Loss/tok 3.4224 (3.3826)	LR 2.000e-03
0: TRAIN [1][2260/3880]	Time 0.123 (0.163)	Data 1.16e-04 (2.42e-04)	Tok/s 84712 (87450)	Loss/tok 2.9765 (3.3828)	LR 2.000e-03
0: TRAIN [1][2270/3880]	Time 0.126 (0.163)	Data 1.21e-04 (2.41e-04)	Tok/s 82024 (87447)	Loss/tok 3.1520 (3.3828)	LR 2.000e-03
0: TRAIN [1][2280/3880]	Time 0.181 (0.163)	Data 1.45e-04 (2.41e-04)	Tok/s 92454 (87460)	Loss/tok 3.4662 (3.3832)	LR 2.000e-03
0: TRAIN [1][2290/3880]	Time 0.126 (0.163)	Data 1.49e-04 (2.40e-04)	Tok/s 80283 (87452)	Loss/tok 3.1905 (3.3827)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2300/3880]	Time 0.244 (0.163)	Data 1.29e-04 (2.40e-04)	Tok/s 96624 (87451)	Loss/tok 3.4232 (3.3832)	LR 2.000e-03
0: TRAIN [1][2310/3880]	Time 0.183 (0.163)	Data 1.21e-04 (2.39e-04)	Tok/s 91623 (87459)	Loss/tok 3.5014 (3.3834)	LR 2.000e-03
0: TRAIN [1][2320/3880]	Time 0.128 (0.163)	Data 1.24e-04 (2.39e-04)	Tok/s 80739 (87459)	Loss/tok 3.1507 (3.3831)	LR 2.000e-03
0: TRAIN [1][2330/3880]	Time 0.067 (0.163)	Data 1.13e-04 (2.38e-04)	Tok/s 78723 (87464)	Loss/tok 2.6414 (3.3827)	LR 2.000e-03
0: TRAIN [1][2340/3880]	Time 0.247 (0.164)	Data 1.26e-04 (2.38e-04)	Tok/s 94592 (87470)	Loss/tok 3.5306 (3.3827)	LR 2.000e-03
0: TRAIN [1][2350/3880]	Time 0.068 (0.164)	Data 1.30e-04 (2.37e-04)	Tok/s 76178 (87478)	Loss/tok 2.7062 (3.3827)	LR 2.000e-03
0: TRAIN [1][2360/3880]	Time 0.317 (0.164)	Data 1.41e-04 (2.37e-04)	Tok/s 92678 (87482)	Loss/tok 3.7298 (3.3829)	LR 2.000e-03
0: TRAIN [1][2370/3880]	Time 0.183 (0.164)	Data 1.14e-04 (2.36e-04)	Tok/s 93145 (87483)	Loss/tok 3.2120 (3.3826)	LR 2.000e-03
0: TRAIN [1][2380/3880]	Time 0.244 (0.164)	Data 1.52e-04 (2.36e-04)	Tok/s 96068 (87493)	Loss/tok 3.5050 (3.3825)	LR 2.000e-03
0: TRAIN [1][2390/3880]	Time 0.243 (0.164)	Data 1.17e-04 (2.35e-04)	Tok/s 96009 (87502)	Loss/tok 3.5442 (3.3821)	LR 2.000e-03
0: TRAIN [1][2400/3880]	Time 0.184 (0.164)	Data 1.44e-04 (2.35e-04)	Tok/s 91015 (87507)	Loss/tok 3.4381 (3.3822)	LR 2.000e-03
0: TRAIN [1][2410/3880]	Time 0.125 (0.164)	Data 1.18e-04 (2.35e-04)	Tok/s 83028 (87509)	Loss/tok 3.2943 (3.3819)	LR 2.000e-03
0: TRAIN [1][2420/3880]	Time 0.123 (0.164)	Data 1.15e-04 (2.34e-04)	Tok/s 84568 (87496)	Loss/tok 3.0908 (3.3811)	LR 2.000e-03
0: TRAIN [1][2430/3880]	Time 0.181 (0.164)	Data 1.12e-04 (2.34e-04)	Tok/s 92279 (87494)	Loss/tok 3.2932 (3.3813)	LR 2.000e-03
0: TRAIN [1][2440/3880]	Time 0.184 (0.163)	Data 1.32e-04 (2.33e-04)	Tok/s 92305 (87485)	Loss/tok 3.2749 (3.3807)	LR 2.000e-03
0: TRAIN [1][2450/3880]	Time 0.122 (0.163)	Data 1.20e-04 (2.33e-04)	Tok/s 82526 (87471)	Loss/tok 3.1639 (3.3802)	LR 2.000e-03
0: TRAIN [1][2460/3880]	Time 0.124 (0.163)	Data 1.14e-04 (2.32e-04)	Tok/s 83149 (87470)	Loss/tok 3.1538 (3.3800)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2470/3880]	Time 0.313 (0.164)	Data 1.37e-04 (2.32e-04)	Tok/s 97186 (87478)	Loss/tok 3.6486 (3.3806)	LR 2.000e-03
0: TRAIN [1][2480/3880]	Time 0.182 (0.164)	Data 1.20e-04 (2.32e-04)	Tok/s 93042 (87480)	Loss/tok 3.4901 (3.3805)	LR 2.000e-03
0: TRAIN [1][2490/3880]	Time 0.067 (0.164)	Data 1.27e-04 (2.31e-04)	Tok/s 78952 (87482)	Loss/tok 2.7020 (3.3804)	LR 2.000e-03
0: TRAIN [1][2500/3880]	Time 0.244 (0.164)	Data 1.24e-04 (2.31e-04)	Tok/s 95893 (87486)	Loss/tok 3.5401 (3.3803)	LR 2.000e-03
0: TRAIN [1][2510/3880]	Time 0.124 (0.164)	Data 1.38e-04 (2.30e-04)	Tok/s 82234 (87484)	Loss/tok 3.1959 (3.3802)	LR 2.000e-03
0: TRAIN [1][2520/3880]	Time 0.181 (0.164)	Data 1.33e-04 (2.30e-04)	Tok/s 91853 (87480)	Loss/tok 3.3366 (3.3799)	LR 2.000e-03
0: TRAIN [1][2530/3880]	Time 0.123 (0.163)	Data 1.26e-04 (2.29e-04)	Tok/s 84114 (87476)	Loss/tok 2.9439 (3.3795)	LR 2.000e-03
0: TRAIN [1][2540/3880]	Time 0.181 (0.163)	Data 1.18e-04 (2.29e-04)	Tok/s 92557 (87473)	Loss/tok 3.4204 (3.3793)	LR 2.000e-03
0: TRAIN [1][2550/3880]	Time 0.123 (0.163)	Data 1.18e-04 (2.29e-04)	Tok/s 84281 (87474)	Loss/tok 3.1099 (3.3792)	LR 2.000e-03
0: TRAIN [1][2560/3880]	Time 0.123 (0.163)	Data 1.25e-04 (2.28e-04)	Tok/s 83781 (87467)	Loss/tok 3.0738 (3.3788)	LR 2.000e-03
0: TRAIN [1][2570/3880]	Time 0.126 (0.163)	Data 1.32e-04 (2.28e-04)	Tok/s 80231 (87455)	Loss/tok 3.0632 (3.3783)	LR 2.000e-03
0: TRAIN [1][2580/3880]	Time 0.067 (0.163)	Data 1.21e-04 (2.27e-04)	Tok/s 78797 (87446)	Loss/tok 2.6878 (3.3780)	LR 2.000e-03
0: TRAIN [1][2590/3880]	Time 0.126 (0.163)	Data 1.48e-04 (2.27e-04)	Tok/s 84337 (87451)	Loss/tok 3.1911 (3.3781)	LR 2.000e-03
0: TRAIN [1][2600/3880]	Time 0.125 (0.163)	Data 1.24e-04 (2.27e-04)	Tok/s 82931 (87437)	Loss/tok 3.2426 (3.3776)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][2610/3880]	Time 0.127 (0.163)	Data 1.15e-04 (2.26e-04)	Tok/s 82064 (87449)	Loss/tok 3.1211 (3.3779)	LR 1.000e-03
0: TRAIN [1][2620/3880]	Time 0.316 (0.163)	Data 1.13e-04 (2.26e-04)	Tok/s 93203 (87454)	Loss/tok 3.7352 (3.3779)	LR 1.000e-03
0: TRAIN [1][2630/3880]	Time 0.126 (0.163)	Data 1.24e-04 (2.25e-04)	Tok/s 81714 (87453)	Loss/tok 3.0967 (3.3778)	LR 1.000e-03
0: TRAIN [1][2640/3880]	Time 0.185 (0.163)	Data 1.13e-04 (2.25e-04)	Tok/s 89401 (87442)	Loss/tok 3.4730 (3.3772)	LR 1.000e-03
0: TRAIN [1][2650/3880]	Time 0.242 (0.163)	Data 1.19e-04 (2.25e-04)	Tok/s 96805 (87444)	Loss/tok 3.6062 (3.3769)	LR 1.000e-03
0: TRAIN [1][2660/3880]	Time 0.181 (0.163)	Data 1.16e-04 (2.24e-04)	Tok/s 92842 (87450)	Loss/tok 3.2561 (3.3766)	LR 1.000e-03
0: TRAIN [1][2670/3880]	Time 0.242 (0.163)	Data 1.21e-04 (2.24e-04)	Tok/s 96423 (87446)	Loss/tok 3.4084 (3.3763)	LR 1.000e-03
0: TRAIN [1][2680/3880]	Time 0.124 (0.163)	Data 1.21e-04 (2.24e-04)	Tok/s 82881 (87454)	Loss/tok 3.0538 (3.3760)	LR 1.000e-03
0: TRAIN [1][2690/3880]	Time 0.312 (0.163)	Data 1.20e-04 (2.23e-04)	Tok/s 95601 (87462)	Loss/tok 3.6835 (3.3761)	LR 1.000e-03
0: TRAIN [1][2700/3880]	Time 0.183 (0.163)	Data 1.17e-04 (2.23e-04)	Tok/s 91196 (87473)	Loss/tok 3.3624 (3.3763)	LR 1.000e-03
0: TRAIN [1][2710/3880]	Time 0.180 (0.163)	Data 1.13e-04 (2.22e-04)	Tok/s 93760 (87462)	Loss/tok 3.4976 (3.3758)	LR 1.000e-03
0: TRAIN [1][2720/3880]	Time 0.242 (0.163)	Data 1.10e-04 (2.22e-04)	Tok/s 96302 (87465)	Loss/tok 3.5684 (3.3756)	LR 1.000e-03
0: TRAIN [1][2730/3880]	Time 0.124 (0.163)	Data 1.11e-04 (2.22e-04)	Tok/s 82703 (87461)	Loss/tok 3.1674 (3.3752)	LR 1.000e-03
0: TRAIN [1][2740/3880]	Time 0.243 (0.163)	Data 1.15e-04 (2.21e-04)	Tok/s 96777 (87466)	Loss/tok 3.5208 (3.3748)	LR 1.000e-03
0: TRAIN [1][2750/3880]	Time 0.124 (0.163)	Data 1.34e-04 (2.21e-04)	Tok/s 82251 (87458)	Loss/tok 3.1409 (3.3743)	LR 1.000e-03
0: TRAIN [1][2760/3880]	Time 0.181 (0.163)	Data 1.30e-04 (2.21e-04)	Tok/s 93738 (87465)	Loss/tok 3.2766 (3.3738)	LR 1.000e-03
0: TRAIN [1][2770/3880]	Time 0.182 (0.163)	Data 1.39e-04 (2.20e-04)	Tok/s 91839 (87468)	Loss/tok 3.4605 (3.3735)	LR 1.000e-03
0: TRAIN [1][2780/3880]	Time 0.067 (0.163)	Data 1.33e-04 (2.20e-04)	Tok/s 80101 (87466)	Loss/tok 2.7814 (3.3731)	LR 1.000e-03
0: TRAIN [1][2790/3880]	Time 0.183 (0.163)	Data 1.27e-04 (2.20e-04)	Tok/s 92471 (87473)	Loss/tok 3.3358 (3.3729)	LR 1.000e-03
0: TRAIN [1][2800/3880]	Time 0.123 (0.163)	Data 1.13e-04 (2.19e-04)	Tok/s 85866 (87482)	Loss/tok 3.1770 (3.3733)	LR 1.000e-03
0: TRAIN [1][2810/3880]	Time 0.314 (0.163)	Data 1.40e-04 (2.19e-04)	Tok/s 95061 (87483)	Loss/tok 3.6749 (3.3731)	LR 1.000e-03
0: TRAIN [1][2820/3880]	Time 0.067 (0.163)	Data 1.03e-04 (2.19e-04)	Tok/s 78895 (87486)	Loss/tok 2.6620 (3.3728)	LR 1.000e-03
0: TRAIN [1][2830/3880]	Time 0.185 (0.163)	Data 1.07e-04 (2.18e-04)	Tok/s 91547 (87485)	Loss/tok 3.2977 (3.3728)	LR 1.000e-03
0: TRAIN [1][2840/3880]	Time 0.066 (0.163)	Data 1.17e-04 (2.18e-04)	Tok/s 79261 (87478)	Loss/tok 2.6251 (3.3726)	LR 1.000e-03
0: TRAIN [1][2850/3880]	Time 0.123 (0.163)	Data 1.09e-04 (2.18e-04)	Tok/s 82678 (87472)	Loss/tok 3.1590 (3.3720)	LR 1.000e-03
0: TRAIN [1][2860/3880]	Time 0.067 (0.163)	Data 1.16e-04 (2.17e-04)	Tok/s 78888 (87469)	Loss/tok 2.6236 (3.3716)	LR 1.000e-03
0: TRAIN [1][2870/3880]	Time 0.246 (0.163)	Data 1.25e-04 (2.17e-04)	Tok/s 93918 (87472)	Loss/tok 3.5438 (3.3713)	LR 1.000e-03
0: TRAIN [1][2880/3880]	Time 0.125 (0.163)	Data 1.23e-04 (2.16e-04)	Tok/s 83495 (87476)	Loss/tok 3.0921 (3.3710)	LR 1.000e-03
0: TRAIN [1][2890/3880]	Time 0.186 (0.163)	Data 1.38e-04 (2.16e-04)	Tok/s 89590 (87475)	Loss/tok 3.3413 (3.3705)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][2900/3880]	Time 0.123 (0.163)	Data 1.22e-04 (2.16e-04)	Tok/s 84291 (87464)	Loss/tok 3.0265 (3.3699)	LR 1.000e-03
0: TRAIN [1][2910/3880]	Time 0.184 (0.163)	Data 1.09e-04 (2.15e-04)	Tok/s 90975 (87455)	Loss/tok 3.2723 (3.3693)	LR 1.000e-03
0: TRAIN [1][2920/3880]	Time 0.068 (0.163)	Data 1.10e-04 (2.15e-04)	Tok/s 78249 (87455)	Loss/tok 2.7319 (3.3692)	LR 1.000e-03
0: TRAIN [1][2930/3880]	Time 0.126 (0.163)	Data 1.40e-04 (2.15e-04)	Tok/s 80191 (87457)	Loss/tok 3.0924 (3.3694)	LR 1.000e-03
0: TRAIN [1][2940/3880]	Time 0.185 (0.163)	Data 1.11e-04 (2.14e-04)	Tok/s 91870 (87462)	Loss/tok 3.3584 (3.3693)	LR 1.000e-03
0: TRAIN [1][2950/3880]	Time 0.125 (0.163)	Data 1.24e-04 (2.14e-04)	Tok/s 83578 (87453)	Loss/tok 3.1205 (3.3689)	LR 1.000e-03
0: TRAIN [1][2960/3880]	Time 0.183 (0.163)	Data 1.25e-04 (2.14e-04)	Tok/s 91303 (87447)	Loss/tok 3.2765 (3.3684)	LR 1.000e-03
0: TRAIN [1][2970/3880]	Time 0.242 (0.163)	Data 1.12e-04 (2.13e-04)	Tok/s 96528 (87447)	Loss/tok 3.4775 (3.3682)	LR 1.000e-03
0: TRAIN [1][2980/3880]	Time 0.245 (0.163)	Data 1.09e-04 (2.13e-04)	Tok/s 95204 (87455)	Loss/tok 3.4517 (3.3681)	LR 1.000e-03
0: TRAIN [1][2990/3880]	Time 0.128 (0.163)	Data 1.15e-04 (2.13e-04)	Tok/s 79535 (87456)	Loss/tok 3.1571 (3.3682)	LR 1.000e-03
0: TRAIN [1][3000/3880]	Time 0.183 (0.163)	Data 1.34e-04 (2.13e-04)	Tok/s 91371 (87458)	Loss/tok 3.2371 (3.3678)	LR 1.000e-03
0: TRAIN [1][3010/3880]	Time 0.314 (0.163)	Data 1.11e-04 (2.12e-04)	Tok/s 95198 (87452)	Loss/tok 3.6229 (3.3677)	LR 1.000e-03
0: TRAIN [1][3020/3880]	Time 0.123 (0.163)	Data 1.10e-04 (2.12e-04)	Tok/s 84228 (87440)	Loss/tok 3.0051 (3.3670)	LR 1.000e-03
0: TRAIN [1][3030/3880]	Time 0.184 (0.163)	Data 1.12e-04 (2.12e-04)	Tok/s 90322 (87447)	Loss/tok 3.2939 (3.3673)	LR 1.000e-03
0: TRAIN [1][3040/3880]	Time 0.182 (0.163)	Data 1.22e-04 (2.11e-04)	Tok/s 90973 (87439)	Loss/tok 3.2710 (3.3667)	LR 1.000e-03
0: TRAIN [1][3050/3880]	Time 0.068 (0.163)	Data 1.27e-04 (2.11e-04)	Tok/s 77308 (87445)	Loss/tok 2.7685 (3.3664)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3060/3880]	Time 0.185 (0.163)	Data 1.22e-04 (2.11e-04)	Tok/s 90940 (87445)	Loss/tok 3.2391 (3.3660)	LR 1.000e-03
0: TRAIN [1][3070/3880]	Time 0.312 (0.163)	Data 1.10e-04 (2.10e-04)	Tok/s 94394 (87447)	Loss/tok 3.6069 (3.3658)	LR 1.000e-03
0: TRAIN [1][3080/3880]	Time 0.246 (0.163)	Data 1.06e-04 (2.10e-04)	Tok/s 95086 (87448)	Loss/tok 3.4593 (3.3656)	LR 1.000e-03
0: TRAIN [1][3090/3880]	Time 0.315 (0.163)	Data 1.11e-04 (2.10e-04)	Tok/s 93717 (87453)	Loss/tok 3.6053 (3.3656)	LR 1.000e-03
0: TRAIN [1][3100/3880]	Time 0.318 (0.163)	Data 1.13e-04 (2.10e-04)	Tok/s 92430 (87457)	Loss/tok 3.6788 (3.3655)	LR 1.000e-03
0: TRAIN [1][3110/3880]	Time 0.181 (0.163)	Data 1.05e-04 (2.09e-04)	Tok/s 93352 (87457)	Loss/tok 3.3626 (3.3651)	LR 1.000e-03
0: TRAIN [1][3120/3880]	Time 0.242 (0.163)	Data 1.24e-04 (2.09e-04)	Tok/s 96240 (87455)	Loss/tok 3.4024 (3.3648)	LR 1.000e-03
0: TRAIN [1][3130/3880]	Time 0.068 (0.163)	Data 1.12e-04 (2.09e-04)	Tok/s 79143 (87459)	Loss/tok 2.6733 (3.3649)	LR 1.000e-03
0: TRAIN [1][3140/3880]	Time 0.123 (0.163)	Data 1.45e-04 (2.08e-04)	Tok/s 82815 (87452)	Loss/tok 3.1092 (3.3647)	LR 1.000e-03
0: TRAIN [1][3150/3880]	Time 0.125 (0.163)	Data 1.05e-04 (2.08e-04)	Tok/s 82039 (87451)	Loss/tok 3.1019 (3.3643)	LR 1.000e-03
0: TRAIN [1][3160/3880]	Time 0.068 (0.163)	Data 1.28e-04 (2.08e-04)	Tok/s 78539 (87456)	Loss/tok 2.7158 (3.3643)	LR 1.000e-03
0: TRAIN [1][3170/3880]	Time 0.124 (0.163)	Data 1.09e-04 (2.07e-04)	Tok/s 81254 (87452)	Loss/tok 3.0261 (3.3640)	LR 1.000e-03
0: TRAIN [1][3180/3880]	Time 0.123 (0.163)	Data 1.12e-04 (2.07e-04)	Tok/s 83361 (87446)	Loss/tok 3.1420 (3.3635)	LR 1.000e-03
0: TRAIN [1][3190/3880]	Time 0.125 (0.163)	Data 1.10e-04 (2.07e-04)	Tok/s 83720 (87438)	Loss/tok 3.0302 (3.3631)	LR 1.000e-03
0: TRAIN [1][3200/3880]	Time 0.125 (0.163)	Data 1.08e-04 (2.07e-04)	Tok/s 82538 (87436)	Loss/tok 3.1057 (3.3629)	LR 1.000e-03
0: TRAIN [1][3210/3880]	Time 0.125 (0.163)	Data 1.27e-04 (2.06e-04)	Tok/s 82054 (87430)	Loss/tok 3.0541 (3.3624)	LR 1.000e-03
0: TRAIN [1][3220/3880]	Time 0.184 (0.163)	Data 1.29e-04 (2.06e-04)	Tok/s 94562 (87439)	Loss/tok 3.1299 (3.3621)	LR 1.000e-03
0: TRAIN [1][3230/3880]	Time 0.242 (0.163)	Data 1.16e-04 (2.06e-04)	Tok/s 95887 (87448)	Loss/tok 3.4620 (3.3621)	LR 1.000e-03
0: TRAIN [1][3240/3880]	Time 0.181 (0.163)	Data 1.05e-04 (2.05e-04)	Tok/s 93005 (87445)	Loss/tok 3.3517 (3.3618)	LR 1.000e-03
0: TRAIN [1][3250/3880]	Time 0.241 (0.163)	Data 1.06e-04 (2.05e-04)	Tok/s 95529 (87447)	Loss/tok 3.5360 (3.3615)	LR 1.000e-03
0: TRAIN [1][3260/3880]	Time 0.245 (0.163)	Data 1.11e-04 (2.05e-04)	Tok/s 94689 (87452)	Loss/tok 3.4813 (3.3613)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3270/3880]	Time 0.068 (0.163)	Data 1.11e-04 (2.05e-04)	Tok/s 77445 (87447)	Loss/tok 2.7252 (3.3609)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [1][3280/3880]	Time 0.246 (0.163)	Data 1.08e-04 (2.04e-04)	Tok/s 95271 (87452)	Loss/tok 3.4264 (3.3606)	LR 1.000e-03
0: TRAIN [1][3290/3880]	Time 0.183 (0.163)	Data 1.17e-04 (2.04e-04)	Tok/s 91246 (87443)	Loss/tok 3.2775 (3.3602)	LR 1.000e-03
0: TRAIN [1][3300/3880]	Time 0.127 (0.163)	Data 1.14e-04 (2.04e-04)	Tok/s 81497 (87439)	Loss/tok 3.0596 (3.3603)	LR 1.000e-03
0: TRAIN [1][3310/3880]	Time 0.245 (0.163)	Data 1.12e-04 (2.03e-04)	Tok/s 94794 (87448)	Loss/tok 3.4588 (3.3603)	LR 1.000e-03
0: TRAIN [1][3320/3880]	Time 0.069 (0.163)	Data 1.27e-04 (2.03e-04)	Tok/s 76349 (87445)	Loss/tok 2.7249 (3.3599)	LR 1.000e-03
0: TRAIN [1][3330/3880]	Time 0.183 (0.163)	Data 1.24e-04 (2.03e-04)	Tok/s 93042 (87454)	Loss/tok 3.3170 (3.3598)	LR 1.000e-03
0: TRAIN [1][3340/3880]	Time 0.243 (0.163)	Data 1.11e-04 (2.03e-04)	Tok/s 96242 (87461)	Loss/tok 3.2989 (3.3596)	LR 1.000e-03
0: TRAIN [1][3350/3880]	Time 0.124 (0.163)	Data 1.10e-04 (2.02e-04)	Tok/s 81705 (87459)	Loss/tok 3.1107 (3.3593)	LR 1.000e-03
0: TRAIN [1][3360/3880]	Time 0.068 (0.163)	Data 1.19e-04 (2.02e-04)	Tok/s 77104 (87464)	Loss/tok 2.6702 (3.3589)	LR 1.000e-03
0: TRAIN [1][3370/3880]	Time 0.182 (0.163)	Data 1.06e-04 (2.02e-04)	Tok/s 91999 (87462)	Loss/tok 3.2637 (3.3585)	LR 1.000e-03
0: TRAIN [1][3380/3880]	Time 0.123 (0.164)	Data 1.28e-04 (2.02e-04)	Tok/s 83685 (87467)	Loss/tok 3.0534 (3.3588)	LR 1.000e-03
0: TRAIN [1][3390/3880]	Time 0.319 (0.164)	Data 1.29e-04 (2.02e-04)	Tok/s 94198 (87478)	Loss/tok 3.6077 (3.3591)	LR 1.000e-03
0: TRAIN [1][3400/3880]	Time 0.182 (0.164)	Data 1.09e-04 (2.01e-04)	Tok/s 93169 (87478)	Loss/tok 3.1999 (3.3588)	LR 1.000e-03
0: TRAIN [1][3410/3880]	Time 0.066 (0.164)	Data 1.06e-04 (2.01e-04)	Tok/s 78166 (87477)	Loss/tok 2.6439 (3.3587)	LR 1.000e-03
0: TRAIN [1][3420/3880]	Time 0.185 (0.164)	Data 1.16e-04 (2.01e-04)	Tok/s 92305 (87478)	Loss/tok 3.2020 (3.3583)	LR 5.000e-04
0: TRAIN [1][3430/3880]	Time 0.185 (0.164)	Data 1.13e-04 (2.00e-04)	Tok/s 90091 (87481)	Loss/tok 3.3792 (3.3581)	LR 5.000e-04
0: TRAIN [1][3440/3880]	Time 0.126 (0.164)	Data 1.37e-04 (2.00e-04)	Tok/s 82724 (87489)	Loss/tok 3.0049 (3.3578)	LR 5.000e-04
0: TRAIN [1][3450/3880]	Time 0.243 (0.164)	Data 1.03e-04 (2.00e-04)	Tok/s 96178 (87493)	Loss/tok 3.4205 (3.3576)	LR 5.000e-04
0: TRAIN [1][3460/3880]	Time 0.182 (0.164)	Data 1.08e-04 (2.00e-04)	Tok/s 92019 (87494)	Loss/tok 3.2644 (3.3573)	LR 5.000e-04
0: TRAIN [1][3470/3880]	Time 0.247 (0.164)	Data 1.05e-04 (1.99e-04)	Tok/s 94459 (87498)	Loss/tok 3.4800 (3.3570)	LR 5.000e-04
0: TRAIN [1][3480/3880]	Time 0.067 (0.164)	Data 1.06e-04 (1.99e-04)	Tok/s 78344 (87503)	Loss/tok 2.6046 (3.3571)	LR 5.000e-04
0: TRAIN [1][3490/3880]	Time 0.124 (0.164)	Data 1.08e-04 (1.99e-04)	Tok/s 83306 (87502)	Loss/tok 3.1000 (3.3570)	LR 5.000e-04
0: TRAIN [1][3500/3880]	Time 0.182 (0.164)	Data 1.06e-04 (1.99e-04)	Tok/s 91901 (87497)	Loss/tok 3.2205 (3.3565)	LR 5.000e-04
0: TRAIN [1][3510/3880]	Time 0.124 (0.164)	Data 1.13e-04 (1.98e-04)	Tok/s 83978 (87490)	Loss/tok 3.0558 (3.3559)	LR 5.000e-04
0: TRAIN [1][3520/3880]	Time 0.124 (0.164)	Data 1.15e-04 (1.98e-04)	Tok/s 82245 (87488)	Loss/tok 3.2192 (3.3557)	LR 5.000e-04
0: TRAIN [1][3530/3880]	Time 0.123 (0.164)	Data 1.05e-04 (1.98e-04)	Tok/s 83355 (87481)	Loss/tok 3.0621 (3.3551)	LR 5.000e-04
0: TRAIN [1][3540/3880]	Time 0.126 (0.164)	Data 1.07e-04 (1.98e-04)	Tok/s 81767 (87478)	Loss/tok 3.0757 (3.3548)	LR 5.000e-04
0: TRAIN [1][3550/3880]	Time 0.125 (0.163)	Data 1.05e-04 (1.98e-04)	Tok/s 83605 (87471)	Loss/tok 3.0167 (3.3543)	LR 5.000e-04
0: TRAIN [1][3560/3880]	Time 0.315 (0.164)	Data 1.02e-04 (1.97e-04)	Tok/s 95428 (87481)	Loss/tok 3.4943 (3.3542)	LR 5.000e-04
0: TRAIN [1][3570/3880]	Time 0.318 (0.164)	Data 1.26e-04 (1.97e-04)	Tok/s 93953 (87482)	Loss/tok 3.5478 (3.3541)	LR 5.000e-04
0: TRAIN [1][3580/3880]	Time 0.066 (0.164)	Data 1.01e-04 (1.97e-04)	Tok/s 77570 (87480)	Loss/tok 2.7033 (3.3537)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3590/3880]	Time 0.124 (0.164)	Data 1.07e-04 (1.97e-04)	Tok/s 81746 (87473)	Loss/tok 3.0811 (3.3533)	LR 5.000e-04
0: TRAIN [1][3600/3880]	Time 0.125 (0.163)	Data 1.02e-04 (1.96e-04)	Tok/s 83405 (87469)	Loss/tok 3.0905 (3.3530)	LR 5.000e-04
0: TRAIN [1][3610/3880]	Time 0.317 (0.164)	Data 1.03e-04 (1.96e-04)	Tok/s 93110 (87474)	Loss/tok 3.6355 (3.3528)	LR 5.000e-04
0: TRAIN [1][3620/3880]	Time 0.184 (0.164)	Data 1.05e-04 (1.96e-04)	Tok/s 90423 (87475)	Loss/tok 3.3192 (3.3527)	LR 5.000e-04
0: TRAIN [1][3630/3880]	Time 0.181 (0.164)	Data 1.03e-04 (1.96e-04)	Tok/s 93770 (87472)	Loss/tok 3.2909 (3.3524)	LR 5.000e-04
0: TRAIN [1][3640/3880]	Time 0.243 (0.164)	Data 1.02e-04 (1.95e-04)	Tok/s 95725 (87468)	Loss/tok 3.4028 (3.3520)	LR 5.000e-04
0: TRAIN [1][3650/3880]	Time 0.245 (0.163)	Data 1.07e-04 (1.95e-04)	Tok/s 95415 (87464)	Loss/tok 3.3612 (3.3516)	LR 5.000e-04
0: TRAIN [1][3660/3880]	Time 0.068 (0.163)	Data 1.02e-04 (1.95e-04)	Tok/s 77570 (87456)	Loss/tok 2.6717 (3.3510)	LR 5.000e-04
0: TRAIN [1][3670/3880]	Time 0.124 (0.163)	Data 1.03e-04 (1.95e-04)	Tok/s 83539 (87461)	Loss/tok 2.9477 (3.3510)	LR 5.000e-04
0: TRAIN [1][3680/3880]	Time 0.124 (0.164)	Data 1.08e-04 (1.94e-04)	Tok/s 82137 (87472)	Loss/tok 3.0566 (3.3508)	LR 5.000e-04
0: TRAIN [1][3690/3880]	Time 0.183 (0.164)	Data 1.12e-04 (1.94e-04)	Tok/s 91552 (87471)	Loss/tok 3.3141 (3.3506)	LR 5.000e-04
0: TRAIN [1][3700/3880]	Time 0.243 (0.163)	Data 1.07e-04 (1.94e-04)	Tok/s 95476 (87465)	Loss/tok 3.3703 (3.3502)	LR 5.000e-04
0: TRAIN [1][3710/3880]	Time 0.183 (0.164)	Data 1.41e-04 (1.94e-04)	Tok/s 92697 (87467)	Loss/tok 3.1785 (3.3499)	LR 5.000e-04
0: TRAIN [1][3720/3880]	Time 0.242 (0.164)	Data 1.34e-04 (1.94e-04)	Tok/s 95713 (87475)	Loss/tok 3.4422 (3.3497)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3730/3880]	Time 0.123 (0.164)	Data 1.17e-04 (1.93e-04)	Tok/s 83303 (87473)	Loss/tok 3.0637 (3.3495)	LR 5.000e-04
0: TRAIN [1][3740/3880]	Time 0.126 (0.164)	Data 1.26e-04 (1.93e-04)	Tok/s 81314 (87466)	Loss/tok 3.0359 (3.3491)	LR 5.000e-04
0: TRAIN [1][3750/3880]	Time 0.185 (0.164)	Data 1.41e-04 (1.93e-04)	Tok/s 88731 (87466)	Loss/tok 3.4340 (3.3488)	LR 5.000e-04
0: TRAIN [1][3760/3880]	Time 0.243 (0.164)	Data 1.37e-04 (1.93e-04)	Tok/s 97160 (87468)	Loss/tok 3.3567 (3.3486)	LR 5.000e-04
0: TRAIN [1][3770/3880]	Time 0.246 (0.164)	Data 1.39e-04 (1.93e-04)	Tok/s 93575 (87468)	Loss/tok 3.5368 (3.3485)	LR 5.000e-04
0: TRAIN [1][3780/3880]	Time 0.124 (0.164)	Data 1.37e-04 (1.93e-04)	Tok/s 84384 (87476)	Loss/tok 2.9957 (3.3482)	LR 5.000e-04
0: TRAIN [1][3790/3880]	Time 0.125 (0.164)	Data 1.05e-04 (1.92e-04)	Tok/s 84363 (87475)	Loss/tok 3.1335 (3.3480)	LR 5.000e-04
0: TRAIN [1][3800/3880]	Time 0.123 (0.164)	Data 1.31e-04 (1.92e-04)	Tok/s 82423 (87472)	Loss/tok 3.1150 (3.3476)	LR 5.000e-04
0: TRAIN [1][3810/3880]	Time 0.183 (0.164)	Data 1.37e-04 (1.92e-04)	Tok/s 91942 (87475)	Loss/tok 3.2400 (3.3473)	LR 5.000e-04
0: TRAIN [1][3820/3880]	Time 0.126 (0.164)	Data 1.12e-04 (1.92e-04)	Tok/s 82104 (87477)	Loss/tok 2.9674 (3.3469)	LR 5.000e-04
0: TRAIN [1][3830/3880]	Time 0.067 (0.164)	Data 1.77e-04 (1.92e-04)	Tok/s 78970 (87479)	Loss/tok 2.7455 (3.3468)	LR 5.000e-04
0: TRAIN [1][3840/3880]	Time 0.183 (0.164)	Data 1.39e-04 (1.92e-04)	Tok/s 91163 (87484)	Loss/tok 3.3158 (3.3466)	LR 5.000e-04
0: TRAIN [1][3850/3880]	Time 0.124 (0.164)	Data 1.26e-04 (1.91e-04)	Tok/s 83423 (87491)	Loss/tok 3.0583 (3.3465)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][3860/3880]	Time 0.123 (0.164)	Data 1.43e-04 (1.91e-04)	Tok/s 85840 (87488)	Loss/tok 3.1013 (3.3465)	LR 5.000e-04
0: TRAIN [1][3870/3880]	Time 0.127 (0.164)	Data 1.09e-04 (1.91e-04)	Tok/s 82415 (87481)	Loss/tok 2.9711 (3.3460)	LR 5.000e-04
:::MLL 1586324579.660 epoch_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 524}}
:::MLL 1586324579.660 eval_start: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [1][0/6]	Time 0.705 (0.705)	Decoder iters 136.0 (136.0)	Tok/s 23375 (23375)
0: Running moses detokenizer
0: BLEU(score=23.06648441814334, counts=[36737, 18070, 10098, 5890], totals=[65707, 62704, 59701, 56702], precisions=[55.910329188670914, 28.817938249553457, 16.914289542888728, 10.387640647596205], bp=1.0, sys_len=65707, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586324582.620 eval_accuracy: {"value": 23.07, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 535}}
:::MLL 1586324582.620 eval_stop: {"value": null, "metadata": {"epoch_num": 2, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 1	Training Loss: 3.3463	Test BLEU: 23.07
0: Performance: Epoch: 1	Training: 349871 Tok/s
0: Finished epoch 1
:::MLL 1586324582.620 block_stop: {"value": null, "metadata": {"first_epoch_num": 2, "file": "train.py", "lineno": 557}}
:::MLL 1586324582.621 block_start: {"value": null, "metadata": {"first_epoch_num": 3, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1586324582.621 epoch_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 514}}
0: Starting epoch 2
0: Executing preallocation
0: Sampler for epoch 2 uses seed 1883509580
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [2][0/3880]	Time 0.798 (0.798)	Data 2.45e-01 (2.45e-01)	Tok/s 21236 (21236)	Loss/tok 3.1223 (3.1223)	LR 5.000e-04
0: TRAIN [2][10/3880]	Time 0.242 (0.221)	Data 1.13e-04 (2.24e-02)	Tok/s 97346 (84065)	Loss/tok 3.3045 (3.1561)	LR 5.000e-04
0: TRAIN [2][20/3880]	Time 0.181 (0.183)	Data 1.15e-04 (1.18e-02)	Tok/s 91629 (85392)	Loss/tok 3.1614 (3.1119)	LR 5.000e-04
0: TRAIN [2][30/3880]	Time 0.124 (0.170)	Data 1.12e-04 (8.02e-03)	Tok/s 82398 (85756)	Loss/tok 2.9964 (3.1059)	LR 5.000e-04
0: TRAIN [2][40/3880]	Time 0.126 (0.164)	Data 1.25e-04 (6.09e-03)	Tok/s 82306 (85823)	Loss/tok 2.9059 (3.1017)	LR 5.000e-04
0: TRAIN [2][50/3880]	Time 0.123 (0.162)	Data 1.13e-04 (4.92e-03)	Tok/s 85485 (86197)	Loss/tok 2.8995 (3.1019)	LR 5.000e-04
0: TRAIN [2][60/3880]	Time 0.183 (0.162)	Data 1.24e-04 (4.13e-03)	Tok/s 92143 (86372)	Loss/tok 3.2217 (3.1131)	LR 5.000e-04
0: TRAIN [2][70/3880]	Time 0.244 (0.165)	Data 1.13e-04 (3.56e-03)	Tok/s 96055 (87019)	Loss/tok 3.3241 (3.1283)	LR 5.000e-04
0: TRAIN [2][80/3880]	Time 0.317 (0.166)	Data 1.25e-04 (3.14e-03)	Tok/s 95808 (87175)	Loss/tok 3.4292 (3.1310)	LR 5.000e-04
0: TRAIN [2][90/3880]	Time 0.243 (0.165)	Data 9.87e-05 (2.81e-03)	Tok/s 95661 (87213)	Loss/tok 3.2995 (3.1383)	LR 5.000e-04
0: TRAIN [2][100/3880]	Time 0.068 (0.166)	Data 1.00e-04 (2.54e-03)	Tok/s 77396 (87284)	Loss/tok 2.5603 (3.1440)	LR 5.000e-04
0: TRAIN [2][110/3880]	Time 0.126 (0.166)	Data 1.09e-04 (2.32e-03)	Tok/s 82038 (87358)	Loss/tok 3.0224 (3.1439)	LR 5.000e-04
0: TRAIN [2][120/3880]	Time 0.123 (0.166)	Data 9.75e-05 (2.14e-03)	Tok/s 82176 (87412)	Loss/tok 2.9833 (3.1508)	LR 5.000e-04
0: TRAIN [2][130/3880]	Time 0.125 (0.166)	Data 9.75e-05 (1.98e-03)	Tok/s 83883 (87461)	Loss/tok 2.9547 (3.1515)	LR 5.000e-04
0: TRAIN [2][140/3880]	Time 0.126 (0.165)	Data 1.14e-04 (1.85e-03)	Tok/s 81305 (87399)	Loss/tok 3.0410 (3.1474)	LR 5.000e-04
0: TRAIN [2][150/3880]	Time 0.183 (0.167)	Data 1.15e-04 (1.73e-03)	Tok/s 93131 (87604)	Loss/tok 3.1716 (3.1551)	LR 5.000e-04
0: TRAIN [2][160/3880]	Time 0.124 (0.164)	Data 1.28e-04 (1.63e-03)	Tok/s 84176 (87360)	Loss/tok 2.9219 (3.1482)	LR 5.000e-04
0: TRAIN [2][170/3880]	Time 0.182 (0.165)	Data 1.11e-04 (1.54e-03)	Tok/s 92594 (87414)	Loss/tok 3.2578 (3.1511)	LR 5.000e-04
0: TRAIN [2][180/3880]	Time 0.125 (0.165)	Data 1.19e-04 (1.47e-03)	Tok/s 84127 (87404)	Loss/tok 3.0300 (3.1526)	LR 5.000e-04
0: TRAIN [2][190/3880]	Time 0.128 (0.165)	Data 1.35e-04 (1.40e-03)	Tok/s 81815 (87433)	Loss/tok 3.0211 (3.1523)	LR 5.000e-04
0: TRAIN [2][200/3880]	Time 0.244 (0.164)	Data 1.19e-04 (1.33e-03)	Tok/s 96408 (87348)	Loss/tok 3.3853 (3.1512)	LR 5.000e-04
0: TRAIN [2][210/3880]	Time 0.125 (0.163)	Data 1.25e-04 (1.28e-03)	Tok/s 82858 (87182)	Loss/tok 2.9897 (3.1458)	LR 5.000e-04
0: TRAIN [2][220/3880]	Time 0.314 (0.164)	Data 1.33e-04 (1.22e-03)	Tok/s 95044 (87276)	Loss/tok 3.4643 (3.1500)	LR 5.000e-04
0: TRAIN [2][230/3880]	Time 0.242 (0.165)	Data 1.42e-04 (1.18e-03)	Tok/s 97813 (87323)	Loss/tok 3.2886 (3.1561)	LR 5.000e-04
0: TRAIN [2][240/3880]	Time 0.125 (0.165)	Data 1.54e-04 (1.13e-03)	Tok/s 83832 (87289)	Loss/tok 3.0729 (3.1622)	LR 5.000e-04
0: TRAIN [2][250/3880]	Time 0.183 (0.166)	Data 1.48e-04 (1.09e-03)	Tok/s 92204 (87332)	Loss/tok 3.1618 (3.1654)	LR 5.000e-04
0: TRAIN [2][260/3880]	Time 0.125 (0.165)	Data 1.43e-04 (1.06e-03)	Tok/s 81871 (87300)	Loss/tok 2.9488 (3.1620)	LR 5.000e-04
0: TRAIN [2][270/3880]	Time 0.248 (0.165)	Data 1.22e-04 (1.02e-03)	Tok/s 94052 (87300)	Loss/tok 3.3796 (3.1641)	LR 5.000e-04
0: TRAIN [2][280/3880]	Time 0.127 (0.166)	Data 1.36e-04 (9.92e-04)	Tok/s 81037 (87319)	Loss/tok 2.9738 (3.1684)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][290/3880]	Time 0.318 (0.167)	Data 1.34e-04 (9.62e-04)	Tok/s 93863 (87357)	Loss/tok 3.5624 (3.1687)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][300/3880]	Time 0.124 (0.167)	Data 1.29e-04 (9.35e-04)	Tok/s 83970 (87378)	Loss/tok 3.0400 (3.1709)	LR 5.000e-04
0: TRAIN [2][310/3880]	Time 0.124 (0.166)	Data 1.11e-04 (9.08e-04)	Tok/s 82092 (87331)	Loss/tok 2.9315 (3.1680)	LR 5.000e-04
0: TRAIN [2][320/3880]	Time 0.125 (0.166)	Data 1.21e-04 (8.84e-04)	Tok/s 80756 (87293)	Loss/tok 2.9268 (3.1665)	LR 5.000e-04
0: TRAIN [2][330/3880]	Time 0.126 (0.166)	Data 1.34e-04 (8.61e-04)	Tok/s 81527 (87310)	Loss/tok 2.9148 (3.1660)	LR 5.000e-04
0: TRAIN [2][340/3880]	Time 0.128 (0.166)	Data 1.19e-04 (8.39e-04)	Tok/s 81072 (87285)	Loss/tok 2.9295 (3.1654)	LR 5.000e-04
0: TRAIN [2][350/3880]	Time 0.066 (0.165)	Data 1.17e-04 (8.19e-04)	Tok/s 79258 (87186)	Loss/tok 2.5536 (3.1622)	LR 5.000e-04
0: TRAIN [2][360/3880]	Time 0.184 (0.165)	Data 1.31e-04 (7.99e-04)	Tok/s 90553 (87294)	Loss/tok 3.1805 (3.1636)	LR 2.500e-04
0: TRAIN [2][370/3880]	Time 0.246 (0.166)	Data 1.39e-04 (7.81e-04)	Tok/s 95479 (87334)	Loss/tok 3.2746 (3.1648)	LR 2.500e-04
0: TRAIN [2][380/3880]	Time 0.124 (0.166)	Data 1.25e-04 (7.64e-04)	Tok/s 84603 (87362)	Loss/tok 2.9157 (3.1631)	LR 2.500e-04
0: TRAIN [2][390/3880]	Time 0.068 (0.166)	Data 1.38e-04 (7.48e-04)	Tok/s 75272 (87398)	Loss/tok 2.4934 (3.1617)	LR 2.500e-04
0: TRAIN [2][400/3880]	Time 0.123 (0.165)	Data 1.15e-04 (7.32e-04)	Tok/s 84225 (87360)	Loss/tok 3.0041 (3.1591)	LR 2.500e-04
0: TRAIN [2][410/3880]	Time 0.244 (0.165)	Data 1.30e-04 (7.17e-04)	Tok/s 95617 (87309)	Loss/tok 3.3241 (3.1570)	LR 2.500e-04
0: TRAIN [2][420/3880]	Time 0.124 (0.165)	Data 1.33e-04 (7.03e-04)	Tok/s 81395 (87321)	Loss/tok 3.1159 (3.1574)	LR 2.500e-04
0: TRAIN [2][430/3880]	Time 0.185 (0.165)	Data 1.14e-04 (6.90e-04)	Tok/s 90641 (87366)	Loss/tok 3.1685 (3.1579)	LR 2.500e-04
0: TRAIN [2][440/3880]	Time 0.315 (0.165)	Data 1.11e-04 (6.77e-04)	Tok/s 95663 (87342)	Loss/tok 3.3089 (3.1568)	LR 2.500e-04
0: TRAIN [2][450/3880]	Time 0.182 (0.164)	Data 1.17e-04 (6.65e-04)	Tok/s 91980 (87327)	Loss/tok 3.2061 (3.1551)	LR 2.500e-04
0: TRAIN [2][460/3880]	Time 0.183 (0.164)	Data 1.47e-04 (6.53e-04)	Tok/s 92845 (87342)	Loss/tok 3.1303 (3.1547)	LR 2.500e-04
0: TRAIN [2][470/3880]	Time 0.313 (0.164)	Data 1.15e-04 (6.42e-04)	Tok/s 94691 (87341)	Loss/tok 3.5735 (3.1547)	LR 2.500e-04
0: TRAIN [2][480/3880]	Time 0.187 (0.163)	Data 1.40e-04 (6.31e-04)	Tok/s 88478 (87297)	Loss/tok 3.1387 (3.1522)	LR 2.500e-04
0: TRAIN [2][490/3880]	Time 0.185 (0.163)	Data 1.30e-04 (6.21e-04)	Tok/s 91149 (87253)	Loss/tok 3.1547 (3.1511)	LR 2.500e-04
0: TRAIN [2][500/3880]	Time 0.314 (0.164)	Data 1.50e-04 (6.11e-04)	Tok/s 94104 (87267)	Loss/tok 3.5336 (3.1540)	LR 2.500e-04
0: TRAIN [2][510/3880]	Time 0.123 (0.163)	Data 1.32e-04 (6.01e-04)	Tok/s 82022 (87255)	Loss/tok 2.9646 (3.1528)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][520/3880]	Time 0.123 (0.164)	Data 1.25e-04 (5.92e-04)	Tok/s 84647 (87288)	Loss/tok 3.0175 (3.1541)	LR 2.500e-04
0: TRAIN [2][530/3880]	Time 0.126 (0.164)	Data 1.23e-04 (5.84e-04)	Tok/s 81059 (87311)	Loss/tok 2.9860 (3.1549)	LR 2.500e-04
0: TRAIN [2][540/3880]	Time 0.242 (0.164)	Data 1.24e-04 (5.75e-04)	Tok/s 95690 (87305)	Loss/tok 3.3366 (3.1549)	LR 2.500e-04
0: TRAIN [2][550/3880]	Time 0.185 (0.164)	Data 1.31e-04 (5.67e-04)	Tok/s 92180 (87355)	Loss/tok 3.0682 (3.1568)	LR 2.500e-04
0: TRAIN [2][560/3880]	Time 0.125 (0.165)	Data 1.16e-04 (5.59e-04)	Tok/s 85510 (87382)	Loss/tok 2.9997 (3.1575)	LR 2.500e-04
0: TRAIN [2][570/3880]	Time 0.243 (0.165)	Data 1.23e-04 (5.52e-04)	Tok/s 95405 (87379)	Loss/tok 3.3927 (3.1584)	LR 2.500e-04
0: TRAIN [2][580/3880]	Time 0.122 (0.165)	Data 1.32e-04 (5.44e-04)	Tok/s 83966 (87389)	Loss/tok 2.9322 (3.1575)	LR 2.500e-04
0: TRAIN [2][590/3880]	Time 0.125 (0.165)	Data 1.29e-04 (5.37e-04)	Tok/s 83073 (87393)	Loss/tok 2.9946 (3.1572)	LR 2.500e-04
0: TRAIN [2][600/3880]	Time 0.123 (0.165)	Data 1.24e-04 (5.31e-04)	Tok/s 84633 (87404)	Loss/tok 3.0191 (3.1572)	LR 2.500e-04
0: TRAIN [2][610/3880]	Time 0.124 (0.165)	Data 1.27e-04 (5.24e-04)	Tok/s 80956 (87449)	Loss/tok 2.9425 (3.1588)	LR 2.500e-04
0: TRAIN [2][620/3880]	Time 0.128 (0.165)	Data 1.16e-04 (5.18e-04)	Tok/s 80418 (87401)	Loss/tok 2.9267 (3.1586)	LR 2.500e-04
0: TRAIN [2][630/3880]	Time 0.317 (0.165)	Data 1.45e-04 (5.12e-04)	Tok/s 94558 (87388)	Loss/tok 3.4557 (3.1599)	LR 2.500e-04
0: TRAIN [2][640/3880]	Time 0.246 (0.165)	Data 1.32e-04 (5.06e-04)	Tok/s 94332 (87388)	Loss/tok 3.3831 (3.1589)	LR 2.500e-04
0: TRAIN [2][650/3880]	Time 0.183 (0.165)	Data 1.19e-04 (5.00e-04)	Tok/s 91969 (87384)	Loss/tok 3.1839 (3.1579)	LR 2.500e-04
0: TRAIN [2][660/3880]	Time 0.125 (0.165)	Data 1.34e-04 (4.94e-04)	Tok/s 83187 (87368)	Loss/tok 2.9737 (3.1587)	LR 2.500e-04
0: TRAIN [2][670/3880]	Time 0.125 (0.165)	Data 1.27e-04 (4.89e-04)	Tok/s 82068 (87352)	Loss/tok 2.9873 (3.1570)	LR 2.500e-04
0: TRAIN [2][680/3880]	Time 0.068 (0.165)	Data 1.13e-04 (4.83e-04)	Tok/s 80779 (87364)	Loss/tok 2.6036 (3.1572)	LR 2.500e-04
0: TRAIN [2][690/3880]	Time 0.068 (0.165)	Data 1.18e-04 (4.78e-04)	Tok/s 77727 (87380)	Loss/tok 2.6650 (3.1583)	LR 2.500e-04
0: TRAIN [2][700/3880]	Time 0.243 (0.165)	Data 1.30e-04 (4.73e-04)	Tok/s 96489 (87423)	Loss/tok 3.2389 (3.1585)	LR 2.500e-04
0: TRAIN [2][710/3880]	Time 0.245 (0.166)	Data 1.32e-04 (4.68e-04)	Tok/s 94625 (87496)	Loss/tok 3.3284 (3.1619)	LR 2.500e-04
0: TRAIN [2][720/3880]	Time 0.181 (0.166)	Data 1.15e-04 (4.64e-04)	Tok/s 92089 (87475)	Loss/tok 3.1815 (3.1610)	LR 2.500e-04
0: TRAIN [2][730/3880]	Time 0.183 (0.166)	Data 1.26e-04 (4.59e-04)	Tok/s 91420 (87481)	Loss/tok 3.1726 (3.1604)	LR 2.500e-04
0: TRAIN [2][740/3880]	Time 0.067 (0.166)	Data 1.10e-04 (4.54e-04)	Tok/s 79100 (87482)	Loss/tok 2.6578 (3.1596)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][750/3880]	Time 0.124 (0.166)	Data 1.46e-04 (4.50e-04)	Tok/s 82063 (87486)	Loss/tok 2.9492 (3.1610)	LR 2.500e-04
0: TRAIN [2][760/3880]	Time 0.245 (0.166)	Data 1.19e-04 (4.46e-04)	Tok/s 95375 (87510)	Loss/tok 3.3062 (3.1616)	LR 2.500e-04
0: TRAIN [2][770/3880]	Time 0.242 (0.166)	Data 1.09e-04 (4.41e-04)	Tok/s 96090 (87513)	Loss/tok 3.3472 (3.1611)	LR 2.500e-04
0: TRAIN [2][780/3880]	Time 0.246 (0.166)	Data 1.18e-04 (4.37e-04)	Tok/s 95440 (87497)	Loss/tok 3.3821 (3.1603)	LR 2.500e-04
0: TRAIN [2][790/3880]	Time 0.180 (0.166)	Data 1.15e-04 (4.33e-04)	Tok/s 92480 (87525)	Loss/tok 3.1913 (3.1622)	LR 2.500e-04
0: TRAIN [2][800/3880]	Time 0.315 (0.166)	Data 1.09e-04 (4.29e-04)	Tok/s 93940 (87544)	Loss/tok 3.5622 (3.1633)	LR 2.500e-04
0: TRAIN [2][810/3880]	Time 0.066 (0.166)	Data 1.12e-04 (4.26e-04)	Tok/s 80877 (87575)	Loss/tok 2.5431 (3.1632)	LR 2.500e-04
0: TRAIN [2][820/3880]	Time 0.242 (0.167)	Data 1.14e-04 (4.22e-04)	Tok/s 95103 (87615)	Loss/tok 3.4890 (3.1643)	LR 2.500e-04
0: TRAIN [2][830/3880]	Time 0.066 (0.166)	Data 1.05e-04 (4.18e-04)	Tok/s 78697 (87575)	Loss/tok 2.4911 (3.1643)	LR 2.500e-04
0: TRAIN [2][840/3880]	Time 0.183 (0.167)	Data 1.18e-04 (4.15e-04)	Tok/s 93980 (87589)	Loss/tok 3.1039 (3.1652)	LR 2.500e-04
0: TRAIN [2][850/3880]	Time 0.123 (0.167)	Data 1.37e-04 (4.11e-04)	Tok/s 82925 (87587)	Loss/tok 3.1302 (3.1652)	LR 2.500e-04
0: TRAIN [2][860/3880]	Time 0.183 (0.167)	Data 1.15e-04 (4.08e-04)	Tok/s 92010 (87581)	Loss/tok 3.1462 (3.1639)	LR 2.500e-04
0: TRAIN [2][870/3880]	Time 0.124 (0.166)	Data 1.48e-04 (4.05e-04)	Tok/s 83848 (87550)	Loss/tok 2.9024 (3.1627)	LR 2.500e-04
0: TRAIN [2][880/3880]	Time 0.125 (0.166)	Data 1.45e-04 (4.02e-04)	Tok/s 82988 (87513)	Loss/tok 2.8774 (3.1615)	LR 2.500e-04
0: TRAIN [2][890/3880]	Time 0.183 (0.166)	Data 1.30e-04 (3.99e-04)	Tok/s 91011 (87518)	Loss/tok 3.1250 (3.1615)	LR 2.500e-04
0: TRAIN [2][900/3880]	Time 0.314 (0.166)	Data 1.34e-04 (3.96e-04)	Tok/s 94220 (87499)	Loss/tok 3.6137 (3.1619)	LR 2.500e-04
0: TRAIN [2][910/3880]	Time 0.124 (0.166)	Data 1.54e-04 (3.93e-04)	Tok/s 84121 (87486)	Loss/tok 2.8334 (3.1612)	LR 2.500e-04
0: TRAIN [2][920/3880]	Time 0.315 (0.166)	Data 1.33e-04 (3.90e-04)	Tok/s 94965 (87473)	Loss/tok 3.4481 (3.1610)	LR 2.500e-04
0: TRAIN [2][930/3880]	Time 0.125 (0.166)	Data 1.31e-04 (3.87e-04)	Tok/s 82993 (87469)	Loss/tok 2.9110 (3.1604)	LR 2.500e-04
0: TRAIN [2][940/3880]	Time 0.125 (0.166)	Data 1.28e-04 (3.85e-04)	Tok/s 82561 (87478)	Loss/tok 2.9461 (3.1605)	LR 2.500e-04
0: TRAIN [2][950/3880]	Time 0.067 (0.166)	Data 1.37e-04 (3.82e-04)	Tok/s 79241 (87463)	Loss/tok 2.6331 (3.1599)	LR 2.500e-04
0: TRAIN [2][960/3880]	Time 0.125 (0.165)	Data 1.76e-04 (3.79e-04)	Tok/s 84290 (87431)	Loss/tok 2.8793 (3.1586)	LR 2.500e-04
0: TRAIN [2][970/3880]	Time 0.125 (0.165)	Data 1.29e-04 (3.77e-04)	Tok/s 83150 (87438)	Loss/tok 2.9124 (3.1583)	LR 2.500e-04
0: TRAIN [2][980/3880]	Time 0.066 (0.165)	Data 1.34e-04 (3.74e-04)	Tok/s 79938 (87410)	Loss/tok 2.6350 (3.1572)	LR 2.500e-04
0: TRAIN [2][990/3880]	Time 0.127 (0.165)	Data 1.09e-04 (3.72e-04)	Tok/s 81132 (87427)	Loss/tok 2.9684 (3.1568)	LR 2.500e-04
0: TRAIN [2][1000/3880]	Time 0.125 (0.165)	Data 1.30e-04 (3.69e-04)	Tok/s 81757 (87421)	Loss/tok 2.9451 (3.1578)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1010/3880]	Time 0.245 (0.165)	Data 1.11e-04 (3.67e-04)	Tok/s 94896 (87414)	Loss/tok 3.2903 (3.1579)	LR 2.500e-04
0: TRAIN [2][1020/3880]	Time 0.124 (0.165)	Data 1.17e-04 (3.64e-04)	Tok/s 82545 (87382)	Loss/tok 3.0134 (3.1573)	LR 2.500e-04
0: TRAIN [2][1030/3880]	Time 0.066 (0.165)	Data 1.15e-04 (3.62e-04)	Tok/s 79688 (87372)	Loss/tok 2.6119 (3.1564)	LR 2.500e-04
0: TRAIN [2][1040/3880]	Time 0.125 (0.165)	Data 1.20e-04 (3.60e-04)	Tok/s 83695 (87386)	Loss/tok 2.9250 (3.1563)	LR 2.500e-04
0: TRAIN [2][1050/3880]	Time 0.067 (0.165)	Data 1.26e-04 (3.57e-04)	Tok/s 79436 (87367)	Loss/tok 2.5219 (3.1565)	LR 2.500e-04
0: TRAIN [2][1060/3880]	Time 0.184 (0.165)	Data 1.48e-04 (3.55e-04)	Tok/s 90372 (87365)	Loss/tok 3.1797 (3.1559)	LR 2.500e-04
0: TRAIN [2][1070/3880]	Time 0.183 (0.165)	Data 1.55e-04 (3.53e-04)	Tok/s 91396 (87373)	Loss/tok 3.0919 (3.1560)	LR 2.500e-04
0: TRAIN [2][1080/3880]	Time 0.125 (0.164)	Data 1.29e-04 (3.51e-04)	Tok/s 82327 (87332)	Loss/tok 3.0433 (3.1550)	LR 2.500e-04
0: TRAIN [2][1090/3880]	Time 0.243 (0.164)	Data 1.12e-04 (3.49e-04)	Tok/s 95346 (87336)	Loss/tok 3.3407 (3.1550)	LR 2.500e-04
0: TRAIN [2][1100/3880]	Time 0.124 (0.164)	Data 1.33e-04 (3.47e-04)	Tok/s 81771 (87298)	Loss/tok 2.7797 (3.1537)	LR 2.500e-04
0: TRAIN [2][1110/3880]	Time 0.065 (0.164)	Data 1.27e-04 (3.45e-04)	Tok/s 78640 (87283)	Loss/tok 2.5796 (3.1538)	LR 2.500e-04
0: TRAIN [2][1120/3880]	Time 0.126 (0.164)	Data 1.32e-04 (3.43e-04)	Tok/s 82194 (87283)	Loss/tok 2.9501 (3.1536)	LR 2.500e-04
0: TRAIN [2][1130/3880]	Time 0.124 (0.164)	Data 1.27e-04 (3.41e-04)	Tok/s 82939 (87292)	Loss/tok 2.8836 (3.1541)	LR 2.500e-04
0: TRAIN [2][1140/3880]	Time 0.182 (0.164)	Data 1.16e-04 (3.39e-04)	Tok/s 92865 (87286)	Loss/tok 3.1578 (3.1533)	LR 2.500e-04
0: TRAIN [2][1150/3880]	Time 0.185 (0.164)	Data 1.15e-04 (3.37e-04)	Tok/s 89510 (87298)	Loss/tok 3.1820 (3.1529)	LR 2.500e-04
0: TRAIN [2][1160/3880]	Time 0.183 (0.164)	Data 1.27e-04 (3.36e-04)	Tok/s 92106 (87296)	Loss/tok 3.1225 (3.1523)	LR 2.500e-04
0: TRAIN [2][1170/3880]	Time 0.181 (0.164)	Data 1.24e-04 (3.34e-04)	Tok/s 93390 (87304)	Loss/tok 3.1289 (3.1518)	LR 1.250e-04
0: TRAIN [2][1180/3880]	Time 0.317 (0.164)	Data 1.34e-04 (3.32e-04)	Tok/s 93745 (87326)	Loss/tok 3.4384 (3.1520)	LR 1.250e-04
0: TRAIN [2][1190/3880]	Time 0.244 (0.164)	Data 1.14e-04 (3.30e-04)	Tok/s 96621 (87337)	Loss/tok 3.3637 (3.1524)	LR 1.250e-04
0: TRAIN [2][1200/3880]	Time 0.244 (0.164)	Data 1.24e-04 (3.28e-04)	Tok/s 94423 (87346)	Loss/tok 3.3411 (3.1525)	LR 1.250e-04
0: TRAIN [2][1210/3880]	Time 0.126 (0.164)	Data 1.12e-04 (3.27e-04)	Tok/s 81901 (87355)	Loss/tok 2.9257 (3.1528)	LR 1.250e-04
0: TRAIN [2][1220/3880]	Time 0.067 (0.164)	Data 1.08e-04 (3.25e-04)	Tok/s 78131 (87368)	Loss/tok 2.4349 (3.1524)	LR 1.250e-04
0: TRAIN [2][1230/3880]	Time 0.245 (0.164)	Data 1.13e-04 (3.23e-04)	Tok/s 95373 (87358)	Loss/tok 3.3097 (3.1522)	LR 1.250e-04
0: TRAIN [2][1240/3880]	Time 0.185 (0.164)	Data 1.30e-04 (3.22e-04)	Tok/s 92052 (87366)	Loss/tok 3.1413 (3.1525)	LR 1.250e-04
0: TRAIN [2][1250/3880]	Time 0.244 (0.164)	Data 1.10e-04 (3.20e-04)	Tok/s 94328 (87372)	Loss/tok 3.3337 (3.1532)	LR 1.250e-04
0: TRAIN [2][1260/3880]	Time 0.126 (0.164)	Data 1.08e-04 (3.18e-04)	Tok/s 84310 (87381)	Loss/tok 2.9744 (3.1532)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1270/3880]	Time 0.184 (0.164)	Data 1.17e-04 (3.17e-04)	Tok/s 90657 (87373)	Loss/tok 3.1709 (3.1531)	LR 1.250e-04
0: TRAIN [2][1280/3880]	Time 0.244 (0.164)	Data 1.25e-04 (3.15e-04)	Tok/s 94796 (87360)	Loss/tok 3.3227 (3.1527)	LR 1.250e-04
0: TRAIN [2][1290/3880]	Time 0.183 (0.164)	Data 1.07e-04 (3.14e-04)	Tok/s 90773 (87360)	Loss/tok 3.1401 (3.1528)	LR 1.250e-04
0: TRAIN [2][1300/3880]	Time 0.126 (0.164)	Data 1.13e-04 (3.12e-04)	Tok/s 83245 (87351)	Loss/tok 2.9621 (3.1526)	LR 1.250e-04
0: TRAIN [2][1310/3880]	Time 0.186 (0.164)	Data 1.36e-04 (3.11e-04)	Tok/s 89502 (87365)	Loss/tok 3.2174 (3.1528)	LR 1.250e-04
0: TRAIN [2][1320/3880]	Time 0.123 (0.164)	Data 1.26e-04 (3.09e-04)	Tok/s 84446 (87358)	Loss/tok 2.9655 (3.1520)	LR 1.250e-04
0: TRAIN [2][1330/3880]	Time 0.183 (0.164)	Data 1.18e-04 (3.08e-04)	Tok/s 91252 (87361)	Loss/tok 3.0620 (3.1519)	LR 1.250e-04
0: TRAIN [2][1340/3880]	Time 0.124 (0.164)	Data 1.15e-04 (3.07e-04)	Tok/s 82970 (87350)	Loss/tok 2.8922 (3.1514)	LR 1.250e-04
0: TRAIN [2][1350/3880]	Time 0.124 (0.164)	Data 1.12e-04 (3.05e-04)	Tok/s 84391 (87343)	Loss/tok 2.8767 (3.1512)	LR 1.250e-04
0: TRAIN [2][1360/3880]	Time 0.123 (0.164)	Data 1.51e-04 (3.04e-04)	Tok/s 84803 (87341)	Loss/tok 2.8294 (3.1508)	LR 1.250e-04
0: TRAIN [2][1370/3880]	Time 0.184 (0.164)	Data 1.25e-04 (3.02e-04)	Tok/s 91342 (87351)	Loss/tok 3.1387 (3.1516)	LR 1.250e-04
0: TRAIN [2][1380/3880]	Time 0.124 (0.165)	Data 1.04e-04 (3.01e-04)	Tok/s 82492 (87368)	Loss/tok 3.0147 (3.1521)	LR 1.250e-04
0: TRAIN [2][1390/3880]	Time 0.185 (0.164)	Data 1.18e-04 (3.00e-04)	Tok/s 90211 (87358)	Loss/tok 3.1387 (3.1518)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][1400/3880]	Time 0.246 (0.164)	Data 1.08e-04 (2.98e-04)	Tok/s 93361 (87358)	Loss/tok 3.4051 (3.1514)	LR 1.250e-04
0: TRAIN [2][1410/3880]	Time 0.244 (0.164)	Data 1.16e-04 (2.97e-04)	Tok/s 95185 (87364)	Loss/tok 3.3841 (3.1515)	LR 1.250e-04
0: TRAIN [2][1420/3880]	Time 0.123 (0.164)	Data 1.31e-04 (2.96e-04)	Tok/s 83985 (87350)	Loss/tok 2.8373 (3.1509)	LR 1.250e-04
0: TRAIN [2][1430/3880]	Time 0.183 (0.164)	Data 1.26e-04 (2.95e-04)	Tok/s 90203 (87334)	Loss/tok 3.2086 (3.1501)	LR 1.250e-04
0: TRAIN [2][1440/3880]	Time 0.067 (0.164)	Data 1.06e-04 (2.94e-04)	Tok/s 78013 (87341)	Loss/tok 2.5659 (3.1502)	LR 1.250e-04
0: TRAIN [2][1450/3880]	Time 0.067 (0.164)	Data 1.21e-04 (2.92e-04)	Tok/s 77037 (87327)	Loss/tok 2.4685 (3.1502)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1460/3880]	Time 0.066 (0.164)	Data 1.04e-04 (2.91e-04)	Tok/s 80165 (87309)	Loss/tok 2.5575 (3.1499)	LR 1.250e-04
0: TRAIN [2][1470/3880]	Time 0.182 (0.164)	Data 1.15e-04 (2.90e-04)	Tok/s 92763 (87306)	Loss/tok 3.1822 (3.1499)	LR 1.250e-04
0: TRAIN [2][1480/3880]	Time 0.182 (0.164)	Data 1.25e-04 (2.89e-04)	Tok/s 92322 (87309)	Loss/tok 3.1077 (3.1498)	LR 1.250e-04
0: TRAIN [2][1490/3880]	Time 0.183 (0.164)	Data 1.18e-04 (2.87e-04)	Tok/s 91561 (87290)	Loss/tok 3.1685 (3.1491)	LR 1.250e-04
0: TRAIN [2][1500/3880]	Time 0.124 (0.164)	Data 1.38e-04 (2.86e-04)	Tok/s 84070 (87294)	Loss/tok 2.9498 (3.1490)	LR 1.250e-04
0: TRAIN [2][1510/3880]	Time 0.124 (0.164)	Data 1.14e-04 (2.85e-04)	Tok/s 81845 (87298)	Loss/tok 2.9473 (3.1498)	LR 1.250e-04
0: TRAIN [2][1520/3880]	Time 0.126 (0.164)	Data 1.10e-04 (2.84e-04)	Tok/s 80803 (87301)	Loss/tok 2.8771 (3.1497)	LR 1.250e-04
0: TRAIN [2][1530/3880]	Time 0.124 (0.164)	Data 1.07e-04 (2.83e-04)	Tok/s 82969 (87307)	Loss/tok 2.8693 (3.1497)	LR 1.250e-04
0: TRAIN [2][1540/3880]	Time 0.068 (0.164)	Data 1.15e-04 (2.82e-04)	Tok/s 79910 (87291)	Loss/tok 2.5501 (3.1490)	LR 1.250e-04
0: TRAIN [2][1550/3880]	Time 0.067 (0.164)	Data 1.28e-04 (2.81e-04)	Tok/s 78471 (87314)	Loss/tok 2.4969 (3.1495)	LR 1.250e-04
0: TRAIN [2][1560/3880]	Time 0.182 (0.164)	Data 1.08e-04 (2.80e-04)	Tok/s 92478 (87307)	Loss/tok 3.1502 (3.1488)	LR 1.250e-04
0: TRAIN [2][1570/3880]	Time 0.123 (0.164)	Data 1.10e-04 (2.79e-04)	Tok/s 83487 (87314)	Loss/tok 2.8568 (3.1489)	LR 1.250e-04
0: TRAIN [2][1580/3880]	Time 0.242 (0.164)	Data 1.18e-04 (2.78e-04)	Tok/s 97170 (87312)	Loss/tok 3.3228 (3.1485)	LR 1.250e-04
0: TRAIN [2][1590/3880]	Time 0.124 (0.164)	Data 1.07e-04 (2.77e-04)	Tok/s 84623 (87300)	Loss/tok 2.9332 (3.1479)	LR 1.250e-04
0: TRAIN [2][1600/3880]	Time 0.181 (0.164)	Data 1.29e-04 (2.76e-04)	Tok/s 93086 (87314)	Loss/tok 3.0590 (3.1480)	LR 1.250e-04
0: TRAIN [2][1610/3880]	Time 0.181 (0.164)	Data 1.60e-04 (2.75e-04)	Tok/s 91405 (87303)	Loss/tok 3.1638 (3.1477)	LR 1.250e-04
0: TRAIN [2][1620/3880]	Time 0.125 (0.164)	Data 1.23e-04 (2.74e-04)	Tok/s 80013 (87286)	Loss/tok 2.9120 (3.1472)	LR 1.250e-04
0: TRAIN [2][1630/3880]	Time 0.245 (0.164)	Data 1.33e-04 (2.73e-04)	Tok/s 95165 (87296)	Loss/tok 3.2161 (3.1479)	LR 1.250e-04
0: TRAIN [2][1640/3880]	Time 0.070 (0.164)	Data 1.17e-04 (2.72e-04)	Tok/s 77169 (87302)	Loss/tok 2.5688 (3.1479)	LR 1.250e-04
0: TRAIN [2][1650/3880]	Time 0.184 (0.164)	Data 1.13e-04 (2.71e-04)	Tok/s 90729 (87301)	Loss/tok 3.1991 (3.1482)	LR 1.250e-04
0: TRAIN [2][1660/3880]	Time 0.318 (0.164)	Data 1.05e-04 (2.70e-04)	Tok/s 94062 (87325)	Loss/tok 3.4223 (3.1498)	LR 1.250e-04
0: TRAIN [2][1670/3880]	Time 0.067 (0.164)	Data 1.21e-04 (2.69e-04)	Tok/s 80130 (87329)	Loss/tok 2.4877 (3.1495)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1680/3880]	Time 0.184 (0.164)	Data 1.28e-04 (2.68e-04)	Tok/s 91022 (87328)	Loss/tok 3.1432 (3.1495)	LR 1.250e-04
0: TRAIN [2][1690/3880]	Time 0.067 (0.164)	Data 1.31e-04 (2.67e-04)	Tok/s 77890 (87332)	Loss/tok 2.4987 (3.1495)	LR 1.250e-04
0: TRAIN [2][1700/3880]	Time 0.128 (0.164)	Data 1.32e-04 (2.67e-04)	Tok/s 81649 (87337)	Loss/tok 2.9253 (3.1493)	LR 1.250e-04
0: TRAIN [2][1710/3880]	Time 0.243 (0.164)	Data 1.33e-04 (2.66e-04)	Tok/s 95470 (87334)	Loss/tok 3.3908 (3.1493)	LR 1.250e-04
0: TRAIN [2][1720/3880]	Time 0.183 (0.164)	Data 1.27e-04 (2.65e-04)	Tok/s 90608 (87322)	Loss/tok 3.2034 (3.1486)	LR 1.250e-04
0: TRAIN [2][1730/3880]	Time 0.181 (0.164)	Data 1.35e-04 (2.64e-04)	Tok/s 92660 (87334)	Loss/tok 3.2560 (3.1489)	LR 1.250e-04
0: TRAIN [2][1740/3880]	Time 0.066 (0.164)	Data 1.32e-04 (2.64e-04)	Tok/s 79995 (87329)	Loss/tok 2.5611 (3.1489)	LR 1.250e-04
0: TRAIN [2][1750/3880]	Time 0.126 (0.164)	Data 1.39e-04 (2.63e-04)	Tok/s 81317 (87339)	Loss/tok 2.9497 (3.1489)	LR 1.250e-04
0: TRAIN [2][1760/3880]	Time 0.125 (0.164)	Data 1.51e-04 (2.62e-04)	Tok/s 82536 (87326)	Loss/tok 3.0938 (3.1487)	LR 1.250e-04
0: TRAIN [2][1770/3880]	Time 0.125 (0.164)	Data 1.26e-04 (2.61e-04)	Tok/s 81511 (87314)	Loss/tok 3.0443 (3.1481)	LR 1.250e-04
0: TRAIN [2][1780/3880]	Time 0.125 (0.164)	Data 1.43e-04 (2.61e-04)	Tok/s 82023 (87317)	Loss/tok 2.9644 (3.1485)	LR 1.250e-04
0: TRAIN [2][1790/3880]	Time 0.183 (0.164)	Data 1.36e-04 (2.60e-04)	Tok/s 90965 (87335)	Loss/tok 3.1933 (3.1491)	LR 1.250e-04
0: TRAIN [2][1800/3880]	Time 0.243 (0.164)	Data 1.11e-04 (2.59e-04)	Tok/s 95893 (87345)	Loss/tok 3.3063 (3.1492)	LR 1.250e-04
0: TRAIN [2][1810/3880]	Time 0.187 (0.164)	Data 1.19e-04 (2.58e-04)	Tok/s 89072 (87348)	Loss/tok 3.1957 (3.1490)	LR 1.250e-04
0: TRAIN [2][1820/3880]	Time 0.247 (0.164)	Data 1.18e-04 (2.58e-04)	Tok/s 95142 (87357)	Loss/tok 3.2557 (3.1489)	LR 1.250e-04
0: TRAIN [2][1830/3880]	Time 0.124 (0.164)	Data 1.29e-04 (2.57e-04)	Tok/s 82375 (87351)	Loss/tok 2.9651 (3.1490)	LR 1.250e-04
0: TRAIN [2][1840/3880]	Time 0.125 (0.164)	Data 1.42e-04 (2.56e-04)	Tok/s 81529 (87343)	Loss/tok 2.9391 (3.1484)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][1850/3880]	Time 0.126 (0.164)	Data 1.23e-04 (2.56e-04)	Tok/s 79970 (87340)	Loss/tok 2.9916 (3.1484)	LR 1.250e-04
0: TRAIN [2][1860/3880]	Time 0.124 (0.164)	Data 1.29e-04 (2.55e-04)	Tok/s 84699 (87334)	Loss/tok 3.0147 (3.1481)	LR 1.250e-04
0: TRAIN [2][1870/3880]	Time 0.067 (0.164)	Data 1.16e-04 (2.54e-04)	Tok/s 80590 (87334)	Loss/tok 2.5205 (3.1477)	LR 1.250e-04
0: TRAIN [2][1880/3880]	Time 0.247 (0.164)	Data 1.50e-04 (2.54e-04)	Tok/s 95827 (87355)	Loss/tok 3.3544 (3.1478)	LR 1.250e-04
0: TRAIN [2][1890/3880]	Time 0.124 (0.164)	Data 1.01e-04 (2.53e-04)	Tok/s 82528 (87355)	Loss/tok 2.9247 (3.1476)	LR 1.250e-04
0: TRAIN [2][1900/3880]	Time 0.126 (0.164)	Data 1.29e-04 (2.52e-04)	Tok/s 82116 (87357)	Loss/tok 3.0143 (3.1475)	LR 1.250e-04
0: TRAIN [2][1910/3880]	Time 0.247 (0.164)	Data 1.08e-04 (2.51e-04)	Tok/s 94111 (87366)	Loss/tok 3.2203 (3.1479)	LR 1.250e-04
0: TRAIN [2][1920/3880]	Time 0.244 (0.164)	Data 1.08e-04 (2.51e-04)	Tok/s 95417 (87360)	Loss/tok 3.2556 (3.1474)	LR 1.250e-04
0: TRAIN [2][1930/3880]	Time 0.124 (0.164)	Data 1.03e-04 (2.50e-04)	Tok/s 83325 (87358)	Loss/tok 3.0668 (3.1472)	LR 1.250e-04
0: TRAIN [2][1940/3880]	Time 0.184 (0.164)	Data 1.13e-04 (2.49e-04)	Tok/s 90031 (87382)	Loss/tok 3.1003 (3.1476)	LR 1.250e-04
0: TRAIN [2][1950/3880]	Time 0.183 (0.164)	Data 1.18e-04 (2.48e-04)	Tok/s 91382 (87375)	Loss/tok 3.1185 (3.1470)	LR 1.250e-04
0: TRAIN [2][1960/3880]	Time 0.184 (0.164)	Data 1.08e-04 (2.48e-04)	Tok/s 91615 (87373)	Loss/tok 3.0976 (3.1466)	LR 1.250e-04
0: TRAIN [2][1970/3880]	Time 0.125 (0.164)	Data 1.29e-04 (2.47e-04)	Tok/s 83026 (87395)	Loss/tok 2.9360 (3.1475)	LR 1.250e-04
0: TRAIN [2][1980/3880]	Time 0.317 (0.164)	Data 1.02e-04 (2.46e-04)	Tok/s 94221 (87396)	Loss/tok 3.5605 (3.1475)	LR 1.250e-04
0: TRAIN [2][1990/3880]	Time 0.183 (0.164)	Data 1.03e-04 (2.46e-04)	Tok/s 91811 (87391)	Loss/tok 3.0642 (3.1470)	LR 1.250e-04
0: TRAIN [2][2000/3880]	Time 0.246 (0.164)	Data 1.06e-04 (2.45e-04)	Tok/s 94406 (87389)	Loss/tok 3.3995 (3.1467)	LR 1.250e-04
0: TRAIN [2][2010/3880]	Time 0.123 (0.164)	Data 1.03e-04 (2.44e-04)	Tok/s 83464 (87384)	Loss/tok 3.0323 (3.1468)	LR 1.250e-04
0: TRAIN [2][2020/3880]	Time 0.186 (0.164)	Data 1.18e-04 (2.43e-04)	Tok/s 90631 (87395)	Loss/tok 3.0764 (3.1464)	LR 1.250e-04
0: TRAIN [2][2030/3880]	Time 0.184 (0.164)	Data 1.21e-04 (2.43e-04)	Tok/s 91320 (87397)	Loss/tok 3.0620 (3.1461)	LR 1.250e-04
0: TRAIN [2][2040/3880]	Time 0.125 (0.164)	Data 1.03e-04 (2.42e-04)	Tok/s 80327 (87396)	Loss/tok 2.9073 (3.1463)	LR 1.250e-04
0: TRAIN [2][2050/3880]	Time 0.245 (0.164)	Data 1.01e-04 (2.42e-04)	Tok/s 95797 (87404)	Loss/tok 3.3051 (3.1463)	LR 1.250e-04
0: TRAIN [2][2060/3880]	Time 0.124 (0.164)	Data 1.01e-04 (2.41e-04)	Tok/s 82410 (87409)	Loss/tok 2.9333 (3.1465)	LR 1.250e-04
0: TRAIN [2][2070/3880]	Time 0.067 (0.164)	Data 1.05e-04 (2.40e-04)	Tok/s 79256 (87390)	Loss/tok 2.5296 (3.1459)	LR 1.250e-04
0: TRAIN [2][2080/3880]	Time 0.124 (0.164)	Data 1.09e-04 (2.40e-04)	Tok/s 84328 (87398)	Loss/tok 2.8902 (3.1466)	LR 1.250e-04
0: TRAIN [2][2090/3880]	Time 0.247 (0.164)	Data 1.14e-04 (2.39e-04)	Tok/s 94858 (87406)	Loss/tok 3.3253 (3.1470)	LR 1.250e-04
0: TRAIN [2][2100/3880]	Time 0.124 (0.164)	Data 1.07e-04 (2.38e-04)	Tok/s 85538 (87406)	Loss/tok 2.9604 (3.1469)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2110/3880]	Time 0.124 (0.164)	Data 1.18e-04 (2.38e-04)	Tok/s 83115 (87407)	Loss/tok 3.0350 (3.1469)	LR 1.250e-04
0: TRAIN [2][2120/3880]	Time 0.241 (0.164)	Data 1.26e-04 (2.37e-04)	Tok/s 95914 (87414)	Loss/tok 3.2797 (3.1473)	LR 1.250e-04
0: TRAIN [2][2130/3880]	Time 0.247 (0.164)	Data 1.23e-04 (2.37e-04)	Tok/s 94386 (87425)	Loss/tok 3.3661 (3.1476)	LR 1.250e-04
0: TRAIN [2][2140/3880]	Time 0.068 (0.164)	Data 1.22e-04 (2.36e-04)	Tok/s 78626 (87422)	Loss/tok 2.4596 (3.1474)	LR 1.250e-04
0: TRAIN [2][2150/3880]	Time 0.123 (0.164)	Data 1.12e-04 (2.35e-04)	Tok/s 83711 (87422)	Loss/tok 2.9349 (3.1473)	LR 1.250e-04
0: TRAIN [2][2160/3880]	Time 0.124 (0.164)	Data 1.25e-04 (2.35e-04)	Tok/s 81928 (87429)	Loss/tok 2.8952 (3.1473)	LR 1.250e-04
0: TRAIN [2][2170/3880]	Time 0.315 (0.165)	Data 1.10e-04 (2.34e-04)	Tok/s 94807 (87437)	Loss/tok 3.5035 (3.1481)	LR 1.250e-04
0: TRAIN [2][2180/3880]	Time 0.242 (0.165)	Data 1.28e-04 (2.34e-04)	Tok/s 96182 (87448)	Loss/tok 3.2906 (3.1482)	LR 1.250e-04
0: TRAIN [2][2190/3880]	Time 0.181 (0.165)	Data 1.97e-04 (2.33e-04)	Tok/s 93545 (87462)	Loss/tok 3.1061 (3.1488)	LR 1.250e-04
0: TRAIN [2][2200/3880]	Time 0.182 (0.165)	Data 1.32e-04 (2.33e-04)	Tok/s 92826 (87469)	Loss/tok 3.1410 (3.1488)	LR 1.250e-04
0: TRAIN [2][2210/3880]	Time 0.127 (0.165)	Data 1.07e-04 (2.32e-04)	Tok/s 81973 (87463)	Loss/tok 2.8752 (3.1486)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2220/3880]	Time 0.068 (0.165)	Data 1.09e-04 (2.32e-04)	Tok/s 77802 (87464)	Loss/tok 2.6135 (3.1484)	LR 1.250e-04
0: TRAIN [2][2230/3880]	Time 0.185 (0.165)	Data 1.04e-04 (2.31e-04)	Tok/s 90396 (87464)	Loss/tok 3.1577 (3.1482)	LR 1.250e-04
0: TRAIN [2][2240/3880]	Time 0.126 (0.165)	Data 1.06e-04 (2.31e-04)	Tok/s 81983 (87457)	Loss/tok 2.9589 (3.1479)	LR 1.250e-04
0: TRAIN [2][2250/3880]	Time 0.183 (0.165)	Data 1.06e-04 (2.30e-04)	Tok/s 93304 (87457)	Loss/tok 3.3229 (3.1477)	LR 1.250e-04
0: TRAIN [2][2260/3880]	Time 0.126 (0.165)	Data 1.21e-04 (2.30e-04)	Tok/s 81630 (87444)	Loss/tok 2.9429 (3.1478)	LR 1.250e-04
0: TRAIN [2][2270/3880]	Time 0.185 (0.165)	Data 1.12e-04 (2.29e-04)	Tok/s 90130 (87445)	Loss/tok 3.1621 (3.1477)	LR 1.250e-04
0: TRAIN [2][2280/3880]	Time 0.315 (0.165)	Data 1.06e-04 (2.29e-04)	Tok/s 95038 (87448)	Loss/tok 3.4395 (3.1480)	LR 1.250e-04
0: TRAIN [2][2290/3880]	Time 0.124 (0.165)	Data 1.03e-04 (2.28e-04)	Tok/s 84744 (87437)	Loss/tok 2.9789 (3.1476)	LR 1.250e-04
0: TRAIN [2][2300/3880]	Time 0.184 (0.165)	Data 1.03e-04 (2.27e-04)	Tok/s 91729 (87441)	Loss/tok 3.1596 (3.1475)	LR 1.250e-04
0: TRAIN [2][2310/3880]	Time 0.125 (0.165)	Data 1.03e-04 (2.27e-04)	Tok/s 83634 (87443)	Loss/tok 2.9411 (3.1478)	LR 1.250e-04
0: TRAIN [2][2320/3880]	Time 0.068 (0.165)	Data 1.03e-04 (2.26e-04)	Tok/s 78365 (87438)	Loss/tok 2.5335 (3.1475)	LR 1.250e-04
0: TRAIN [2][2330/3880]	Time 0.124 (0.165)	Data 1.26e-04 (2.26e-04)	Tok/s 83026 (87440)	Loss/tok 3.0263 (3.1477)	LR 1.250e-04
0: TRAIN [2][2340/3880]	Time 0.126 (0.165)	Data 1.18e-04 (2.26e-04)	Tok/s 84279 (87441)	Loss/tok 2.9814 (3.1478)	LR 1.250e-04
0: TRAIN [2][2350/3880]	Time 0.182 (0.165)	Data 1.31e-04 (2.25e-04)	Tok/s 91746 (87434)	Loss/tok 3.0870 (3.1475)	LR 1.250e-04
0: TRAIN [2][2360/3880]	Time 0.068 (0.164)	Data 1.29e-04 (2.25e-04)	Tok/s 79872 (87423)	Loss/tok 2.6086 (3.1470)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2370/3880]	Time 0.124 (0.164)	Data 1.11e-04 (2.24e-04)	Tok/s 82582 (87424)	Loss/tok 2.9462 (3.1470)	LR 1.250e-04
0: TRAIN [2][2380/3880]	Time 0.124 (0.164)	Data 1.20e-04 (2.24e-04)	Tok/s 83970 (87417)	Loss/tok 2.8918 (3.1466)	LR 1.250e-04
0: TRAIN [2][2390/3880]	Time 0.182 (0.165)	Data 1.14e-04 (2.23e-04)	Tok/s 92037 (87432)	Loss/tok 3.1838 (3.1477)	LR 1.250e-04
0: TRAIN [2][2400/3880]	Time 0.126 (0.165)	Data 1.03e-04 (2.23e-04)	Tok/s 83533 (87435)	Loss/tok 2.9364 (3.1475)	LR 1.250e-04
0: TRAIN [2][2410/3880]	Time 0.123 (0.164)	Data 1.08e-04 (2.23e-04)	Tok/s 83951 (87423)	Loss/tok 2.8893 (3.1471)	LR 1.250e-04
0: TRAIN [2][2420/3880]	Time 0.184 (0.164)	Data 1.00e-04 (2.22e-04)	Tok/s 92730 (87413)	Loss/tok 3.0535 (3.1466)	LR 1.250e-04
0: TRAIN [2][2430/3880]	Time 0.243 (0.164)	Data 1.09e-04 (2.22e-04)	Tok/s 94631 (87418)	Loss/tok 3.3148 (3.1465)	LR 1.250e-04
0: TRAIN [2][2440/3880]	Time 0.067 (0.164)	Data 1.01e-04 (2.21e-04)	Tok/s 78959 (87410)	Loss/tok 2.6182 (3.1462)	LR 1.250e-04
0: TRAIN [2][2450/3880]	Time 0.123 (0.164)	Data 1.01e-04 (2.21e-04)	Tok/s 82065 (87408)	Loss/tok 2.9881 (3.1462)	LR 1.250e-04
0: TRAIN [2][2460/3880]	Time 0.127 (0.164)	Data 1.21e-04 (2.20e-04)	Tok/s 80774 (87410)	Loss/tok 2.9602 (3.1463)	LR 1.250e-04
0: TRAIN [2][2470/3880]	Time 0.187 (0.164)	Data 1.03e-04 (2.20e-04)	Tok/s 90846 (87416)	Loss/tok 3.1345 (3.1465)	LR 1.250e-04
0: TRAIN [2][2480/3880]	Time 0.124 (0.164)	Data 1.10e-04 (2.19e-04)	Tok/s 81799 (87419)	Loss/tok 2.9956 (3.1464)	LR 1.250e-04
0: TRAIN [2][2490/3880]	Time 0.123 (0.164)	Data 1.07e-04 (2.19e-04)	Tok/s 84619 (87416)	Loss/tok 2.9248 (3.1460)	LR 1.250e-04
0: TRAIN [2][2500/3880]	Time 0.183 (0.164)	Data 1.19e-04 (2.19e-04)	Tok/s 93448 (87421)	Loss/tok 3.1734 (3.1458)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2510/3880]	Time 0.181 (0.164)	Data 1.28e-04 (2.18e-04)	Tok/s 91543 (87429)	Loss/tok 3.1539 (3.1457)	LR 1.250e-04
0: TRAIN [2][2520/3880]	Time 0.123 (0.164)	Data 1.07e-04 (2.18e-04)	Tok/s 85611 (87429)	Loss/tok 2.9819 (3.1456)	LR 1.250e-04
0: TRAIN [2][2530/3880]	Time 0.126 (0.164)	Data 1.21e-04 (2.17e-04)	Tok/s 81769 (87427)	Loss/tok 2.9810 (3.1456)	LR 1.250e-04
0: TRAIN [2][2540/3880]	Time 0.182 (0.164)	Data 9.99e-05 (2.17e-04)	Tok/s 92531 (87426)	Loss/tok 3.1362 (3.1456)	LR 1.250e-04
0: TRAIN [2][2550/3880]	Time 0.184 (0.164)	Data 1.08e-04 (2.16e-04)	Tok/s 91169 (87429)	Loss/tok 3.1998 (3.1455)	LR 1.250e-04
0: TRAIN [2][2560/3880]	Time 0.126 (0.164)	Data 1.01e-04 (2.16e-04)	Tok/s 82720 (87428)	Loss/tok 2.9409 (3.1453)	LR 1.250e-04
0: TRAIN [2][2570/3880]	Time 0.126 (0.164)	Data 1.03e-04 (2.16e-04)	Tok/s 82610 (87426)	Loss/tok 3.0188 (3.1450)	LR 1.250e-04
0: TRAIN [2][2580/3880]	Time 0.181 (0.164)	Data 1.17e-04 (2.15e-04)	Tok/s 93095 (87425)	Loss/tok 3.0669 (3.1447)	LR 1.250e-04
0: TRAIN [2][2590/3880]	Time 0.182 (0.164)	Data 1.08e-04 (2.15e-04)	Tok/s 93261 (87422)	Loss/tok 3.0781 (3.1446)	LR 1.250e-04
0: TRAIN [2][2600/3880]	Time 0.183 (0.164)	Data 1.07e-04 (2.14e-04)	Tok/s 90797 (87426)	Loss/tok 3.0418 (3.1446)	LR 1.250e-04
0: TRAIN [2][2610/3880]	Time 0.123 (0.164)	Data 1.28e-04 (2.14e-04)	Tok/s 83920 (87421)	Loss/tok 2.9027 (3.1446)	LR 1.250e-04
0: TRAIN [2][2620/3880]	Time 0.313 (0.164)	Data 1.04e-04 (2.14e-04)	Tok/s 94379 (87424)	Loss/tok 3.5536 (3.1450)	LR 1.250e-04
0: TRAIN [2][2630/3880]	Time 0.181 (0.164)	Data 1.06e-04 (2.13e-04)	Tok/s 94106 (87417)	Loss/tok 3.2149 (3.1450)	LR 1.250e-04
0: TRAIN [2][2640/3880]	Time 0.123 (0.164)	Data 1.13e-04 (2.13e-04)	Tok/s 82475 (87414)	Loss/tok 2.8669 (3.1448)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][2650/3880]	Time 0.248 (0.164)	Data 1.06e-04 (2.12e-04)	Tok/s 94290 (87415)	Loss/tok 3.2373 (3.1450)	LR 1.250e-04
0: TRAIN [2][2660/3880]	Time 0.182 (0.164)	Data 1.08e-04 (2.12e-04)	Tok/s 92684 (87416)	Loss/tok 3.0100 (3.1448)	LR 1.250e-04
0: TRAIN [2][2670/3880]	Time 0.124 (0.164)	Data 9.94e-05 (2.12e-04)	Tok/s 82226 (87409)	Loss/tok 2.9609 (3.1445)	LR 1.250e-04
0: TRAIN [2][2680/3880]	Time 0.182 (0.164)	Data 1.04e-04 (2.11e-04)	Tok/s 91569 (87418)	Loss/tok 3.2174 (3.1446)	LR 1.250e-04
0: TRAIN [2][2690/3880]	Time 0.182 (0.164)	Data 1.04e-04 (2.11e-04)	Tok/s 91216 (87408)	Loss/tok 3.0570 (3.1441)	LR 1.250e-04
0: TRAIN [2][2700/3880]	Time 0.183 (0.164)	Data 9.82e-05 (2.11e-04)	Tok/s 93565 (87407)	Loss/tok 3.0509 (3.1441)	LR 1.250e-04
0: TRAIN [2][2710/3880]	Time 0.183 (0.164)	Data 1.31e-04 (2.10e-04)	Tok/s 91518 (87396)	Loss/tok 3.1586 (3.1437)	LR 1.250e-04
0: TRAIN [2][2720/3880]	Time 0.125 (0.164)	Data 1.10e-04 (2.10e-04)	Tok/s 83969 (87398)	Loss/tok 2.9541 (3.1436)	LR 1.250e-04
0: TRAIN [2][2730/3880]	Time 0.317 (0.164)	Data 1.15e-04 (2.09e-04)	Tok/s 93332 (87407)	Loss/tok 3.5420 (3.1440)	LR 1.250e-04
0: TRAIN [2][2740/3880]	Time 0.181 (0.164)	Data 1.04e-04 (2.09e-04)	Tok/s 92907 (87408)	Loss/tok 3.1335 (3.1439)	LR 1.250e-04
0: TRAIN [2][2750/3880]	Time 0.123 (0.164)	Data 1.01e-04 (2.09e-04)	Tok/s 83193 (87397)	Loss/tok 2.9132 (3.1434)	LR 1.250e-04
0: TRAIN [2][2760/3880]	Time 0.067 (0.164)	Data 1.03e-04 (2.08e-04)	Tok/s 78675 (87383)	Loss/tok 2.5588 (3.1429)	LR 1.250e-04
0: TRAIN [2][2770/3880]	Time 0.316 (0.164)	Data 1.22e-04 (2.08e-04)	Tok/s 93557 (87385)	Loss/tok 3.3367 (3.1429)	LR 1.250e-04
0: TRAIN [2][2780/3880]	Time 0.182 (0.164)	Data 1.09e-04 (2.08e-04)	Tok/s 92211 (87392)	Loss/tok 3.2242 (3.1429)	LR 1.250e-04
0: TRAIN [2][2790/3880]	Time 0.183 (0.164)	Data 1.02e-04 (2.07e-04)	Tok/s 92013 (87403)	Loss/tok 3.0346 (3.1430)	LR 1.250e-04
0: TRAIN [2][2800/3880]	Time 0.125 (0.164)	Data 1.37e-04 (2.07e-04)	Tok/s 83616 (87404)	Loss/tok 2.7906 (3.1429)	LR 1.250e-04
0: TRAIN [2][2810/3880]	Time 0.126 (0.164)	Data 1.13e-04 (2.07e-04)	Tok/s 81642 (87397)	Loss/tok 2.9845 (3.1425)	LR 1.250e-04
0: TRAIN [2][2820/3880]	Time 0.125 (0.164)	Data 1.39e-04 (2.06e-04)	Tok/s 82157 (87387)	Loss/tok 2.9045 (3.1420)	LR 1.250e-04
0: TRAIN [2][2830/3880]	Time 0.126 (0.164)	Data 1.42e-04 (2.06e-04)	Tok/s 81240 (87396)	Loss/tok 3.0067 (3.1420)	LR 1.250e-04
0: TRAIN [2][2840/3880]	Time 0.244 (0.164)	Data 1.28e-04 (2.06e-04)	Tok/s 95068 (87399)	Loss/tok 3.3331 (3.1421)	LR 1.250e-04
0: TRAIN [2][2850/3880]	Time 0.125 (0.164)	Data 1.34e-04 (2.06e-04)	Tok/s 83631 (87391)	Loss/tok 2.8607 (3.1418)	LR 1.250e-04
0: TRAIN [2][2860/3880]	Time 0.183 (0.164)	Data 1.10e-04 (2.05e-04)	Tok/s 92612 (87391)	Loss/tok 3.0959 (3.1417)	LR 1.250e-04
0: TRAIN [2][2870/3880]	Time 0.124 (0.164)	Data 1.10e-04 (2.05e-04)	Tok/s 84073 (87382)	Loss/tok 2.9733 (3.1417)	LR 1.250e-04
0: TRAIN [2][2880/3880]	Time 0.126 (0.164)	Data 1.36e-04 (2.05e-04)	Tok/s 80911 (87383)	Loss/tok 3.0213 (3.1418)	LR 1.250e-04
0: TRAIN [2][2890/3880]	Time 0.124 (0.164)	Data 1.09e-04 (2.04e-04)	Tok/s 81467 (87390)	Loss/tok 2.9336 (3.1421)	LR 1.250e-04
0: TRAIN [2][2900/3880]	Time 0.124 (0.164)	Data 1.12e-04 (2.04e-04)	Tok/s 85714 (87383)	Loss/tok 2.8997 (3.1419)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][2910/3880]	Time 0.067 (0.164)	Data 1.54e-04 (2.04e-04)	Tok/s 77916 (87390)	Loss/tok 2.5303 (3.1418)	LR 1.250e-04
0: TRAIN [2][2920/3880]	Time 0.124 (0.164)	Data 1.16e-04 (2.04e-04)	Tok/s 84135 (87393)	Loss/tok 2.9124 (3.1418)	LR 1.250e-04
0: TRAIN [2][2930/3880]	Time 0.244 (0.164)	Data 1.08e-04 (2.03e-04)	Tok/s 94733 (87394)	Loss/tok 3.2739 (3.1418)	LR 1.250e-04
0: TRAIN [2][2940/3880]	Time 0.126 (0.164)	Data 1.34e-04 (2.03e-04)	Tok/s 81474 (87398)	Loss/tok 2.9313 (3.1418)	LR 1.250e-04
0: TRAIN [2][2950/3880]	Time 0.124 (0.164)	Data 1.27e-04 (2.03e-04)	Tok/s 83856 (87403)	Loss/tok 2.9477 (3.1420)	LR 1.250e-04
0: TRAIN [2][2960/3880]	Time 0.126 (0.164)	Data 1.57e-04 (2.02e-04)	Tok/s 81439 (87398)	Loss/tok 2.9285 (3.1421)	LR 1.250e-04
0: TRAIN [2][2970/3880]	Time 0.126 (0.164)	Data 1.17e-04 (2.02e-04)	Tok/s 82836 (87402)	Loss/tok 3.0184 (3.1423)	LR 1.250e-04
0: TRAIN [2][2980/3880]	Time 0.183 (0.164)	Data 1.03e-04 (2.02e-04)	Tok/s 91554 (87394)	Loss/tok 3.1316 (3.1422)	LR 1.250e-04
0: TRAIN [2][2990/3880]	Time 0.126 (0.164)	Data 1.09e-04 (2.02e-04)	Tok/s 83424 (87385)	Loss/tok 2.8836 (3.1419)	LR 1.250e-04
0: TRAIN [2][3000/3880]	Time 0.124 (0.164)	Data 1.09e-04 (2.01e-04)	Tok/s 84807 (87386)	Loss/tok 2.9615 (3.1417)	LR 1.250e-04
0: TRAIN [2][3010/3880]	Time 0.185 (0.164)	Data 1.12e-04 (2.01e-04)	Tok/s 90690 (87384)	Loss/tok 3.1604 (3.1415)	LR 1.250e-04
0: TRAIN [2][3020/3880]	Time 0.246 (0.163)	Data 1.17e-04 (2.01e-04)	Tok/s 93452 (87383)	Loss/tok 3.3968 (3.1413)	LR 1.250e-04
0: TRAIN [2][3030/3880]	Time 0.125 (0.163)	Data 1.12e-04 (2.00e-04)	Tok/s 82378 (87374)	Loss/tok 2.9017 (3.1411)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3040/3880]	Time 0.182 (0.163)	Data 1.23e-04 (2.00e-04)	Tok/s 91758 (87378)	Loss/tok 3.1014 (3.1408)	LR 1.250e-04
0: TRAIN [2][3050/3880]	Time 0.244 (0.163)	Data 1.12e-04 (2.00e-04)	Tok/s 95029 (87382)	Loss/tok 3.2385 (3.1407)	LR 1.250e-04
0: TRAIN [2][3060/3880]	Time 0.125 (0.163)	Data 1.03e-04 (2.00e-04)	Tok/s 82006 (87376)	Loss/tok 2.9852 (3.1408)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3070/3880]	Time 0.316 (0.163)	Data 1.12e-04 (1.99e-04)	Tok/s 93295 (87361)	Loss/tok 3.3971 (3.1405)	LR 1.250e-04
0: TRAIN [2][3080/3880]	Time 0.184 (0.163)	Data 1.11e-04 (1.99e-04)	Tok/s 91230 (87358)	Loss/tok 3.2287 (3.1402)	LR 1.250e-04
0: TRAIN [2][3090/3880]	Time 0.246 (0.163)	Data 1.06e-04 (1.99e-04)	Tok/s 93959 (87365)	Loss/tok 3.3370 (3.1402)	LR 1.250e-04
0: TRAIN [2][3100/3880]	Time 0.124 (0.163)	Data 1.06e-04 (1.98e-04)	Tok/s 83241 (87358)	Loss/tok 2.9465 (3.1399)	LR 1.250e-04
0: TRAIN [2][3110/3880]	Time 0.183 (0.163)	Data 1.12e-04 (1.98e-04)	Tok/s 91479 (87360)	Loss/tok 3.2666 (3.1400)	LR 1.250e-04
0: TRAIN [2][3120/3880]	Time 0.245 (0.163)	Data 1.21e-04 (1.98e-04)	Tok/s 94516 (87355)	Loss/tok 3.3589 (3.1399)	LR 1.250e-04
0: TRAIN [2][3130/3880]	Time 0.123 (0.163)	Data 1.21e-04 (1.98e-04)	Tok/s 84883 (87351)	Loss/tok 2.9366 (3.1399)	LR 1.250e-04
0: TRAIN [2][3140/3880]	Time 0.123 (0.163)	Data 1.28e-04 (1.97e-04)	Tok/s 83677 (87356)	Loss/tok 2.9244 (3.1399)	LR 1.250e-04
0: TRAIN [2][3150/3880]	Time 0.183 (0.163)	Data 1.18e-04 (1.97e-04)	Tok/s 91129 (87358)	Loss/tok 3.1759 (3.1398)	LR 1.250e-04
0: TRAIN [2][3160/3880]	Time 0.316 (0.163)	Data 1.20e-04 (1.97e-04)	Tok/s 94402 (87366)	Loss/tok 3.4046 (3.1402)	LR 1.250e-04
0: TRAIN [2][3170/3880]	Time 0.244 (0.163)	Data 1.25e-04 (1.97e-04)	Tok/s 95655 (87372)	Loss/tok 3.2935 (3.1404)	LR 1.250e-04
0: TRAIN [2][3180/3880]	Time 0.245 (0.163)	Data 1.15e-04 (1.96e-04)	Tok/s 95969 (87374)	Loss/tok 3.2972 (3.1402)	LR 1.250e-04
0: TRAIN [2][3190/3880]	Time 0.184 (0.164)	Data 1.42e-04 (1.96e-04)	Tok/s 91272 (87394)	Loss/tok 3.0351 (3.1408)	LR 1.250e-04
0: TRAIN [2][3200/3880]	Time 0.246 (0.163)	Data 1.07e-04 (1.96e-04)	Tok/s 94591 (87389)	Loss/tok 3.2897 (3.1407)	LR 1.250e-04
0: TRAIN [2][3210/3880]	Time 0.125 (0.163)	Data 1.39e-04 (1.96e-04)	Tok/s 81919 (87385)	Loss/tok 2.9422 (3.1404)	LR 1.250e-04
0: TRAIN [2][3220/3880]	Time 0.183 (0.163)	Data 1.23e-04 (1.96e-04)	Tok/s 91584 (87403)	Loss/tok 3.1593 (3.1407)	LR 1.250e-04
0: TRAIN [2][3230/3880]	Time 0.068 (0.164)	Data 1.47e-04 (1.95e-04)	Tok/s 80060 (87411)	Loss/tok 2.6047 (3.1411)	LR 1.250e-04
0: TRAIN [2][3240/3880]	Time 0.124 (0.164)	Data 1.12e-04 (1.95e-04)	Tok/s 85130 (87413)	Loss/tok 2.9867 (3.1410)	LR 1.250e-04
0: TRAIN [2][3250/3880]	Time 0.183 (0.164)	Data 1.08e-04 (1.95e-04)	Tok/s 91923 (87419)	Loss/tok 3.1598 (3.1412)	LR 1.250e-04
0: TRAIN [2][3260/3880]	Time 0.185 (0.164)	Data 1.34e-04 (1.95e-04)	Tok/s 91164 (87426)	Loss/tok 3.1618 (3.1416)	LR 1.250e-04
0: TRAIN [2][3270/3880]	Time 0.183 (0.164)	Data 1.09e-04 (1.94e-04)	Tok/s 92809 (87430)	Loss/tok 3.1411 (3.1417)	LR 1.250e-04
0: TRAIN [2][3280/3880]	Time 0.181 (0.164)	Data 1.06e-04 (1.94e-04)	Tok/s 92375 (87430)	Loss/tok 3.2276 (3.1419)	LR 1.250e-04
0: TRAIN [2][3290/3880]	Time 0.124 (0.164)	Data 1.29e-04 (1.94e-04)	Tok/s 82060 (87435)	Loss/tok 2.9971 (3.1418)	LR 1.250e-04
0: TRAIN [2][3300/3880]	Time 0.243 (0.164)	Data 1.15e-04 (1.94e-04)	Tok/s 97133 (87431)	Loss/tok 3.2243 (3.1416)	LR 1.250e-04
0: TRAIN [2][3310/3880]	Time 0.123 (0.164)	Data 1.05e-04 (1.93e-04)	Tok/s 83151 (87432)	Loss/tok 2.8721 (3.1416)	LR 1.250e-04
0: TRAIN [2][3320/3880]	Time 0.126 (0.164)	Data 1.15e-04 (1.93e-04)	Tok/s 81369 (87428)	Loss/tok 2.9471 (3.1416)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [2][3330/3880]	Time 0.240 (0.164)	Data 1.09e-04 (1.93e-04)	Tok/s 96870 (87433)	Loss/tok 3.2663 (3.1417)	LR 1.250e-04
0: TRAIN [2][3340/3880]	Time 0.243 (0.164)	Data 1.18e-04 (1.93e-04)	Tok/s 95557 (87435)	Loss/tok 3.2635 (3.1417)	LR 1.250e-04
0: TRAIN [2][3350/3880]	Time 0.124 (0.164)	Data 1.35e-04 (1.93e-04)	Tok/s 83092 (87437)	Loss/tok 2.8833 (3.1417)	LR 1.250e-04
0: TRAIN [2][3360/3880]	Time 0.184 (0.164)	Data 1.03e-04 (1.92e-04)	Tok/s 90755 (87440)	Loss/tok 3.1188 (3.1418)	LR 1.250e-04
0: TRAIN [2][3370/3880]	Time 0.179 (0.164)	Data 1.05e-04 (1.92e-04)	Tok/s 92664 (87437)	Loss/tok 3.1476 (3.1417)	LR 1.250e-04
0: TRAIN [2][3380/3880]	Time 0.125 (0.164)	Data 1.42e-04 (1.92e-04)	Tok/s 84498 (87438)	Loss/tok 2.9024 (3.1417)	LR 1.250e-04
0: TRAIN [2][3390/3880]	Time 0.124 (0.164)	Data 1.23e-04 (1.92e-04)	Tok/s 83200 (87437)	Loss/tok 2.9524 (3.1415)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3400/3880]	Time 0.316 (0.164)	Data 1.29e-04 (1.91e-04)	Tok/s 93698 (87439)	Loss/tok 3.4467 (3.1418)	LR 1.250e-04
0: TRAIN [2][3410/3880]	Time 0.067 (0.164)	Data 1.34e-04 (1.91e-04)	Tok/s 77595 (87436)	Loss/tok 2.6074 (3.1417)	LR 1.250e-04
0: TRAIN [2][3420/3880]	Time 0.125 (0.164)	Data 1.02e-04 (1.91e-04)	Tok/s 83052 (87438)	Loss/tok 2.9259 (3.1418)	LR 1.250e-04
0: TRAIN [2][3430/3880]	Time 0.125 (0.164)	Data 1.10e-04 (1.91e-04)	Tok/s 82603 (87427)	Loss/tok 2.8216 (3.1414)	LR 1.250e-04
0: TRAIN [2][3440/3880]	Time 0.182 (0.164)	Data 1.45e-04 (1.90e-04)	Tok/s 92349 (87432)	Loss/tok 3.2212 (3.1416)	LR 1.250e-04
0: TRAIN [2][3450/3880]	Time 0.184 (0.164)	Data 1.10e-04 (1.90e-04)	Tok/s 89753 (87429)	Loss/tok 3.1206 (3.1414)	LR 1.250e-04
0: TRAIN [2][3460/3880]	Time 0.124 (0.164)	Data 1.04e-04 (1.90e-04)	Tok/s 83489 (87430)	Loss/tok 3.1089 (3.1412)	LR 1.250e-04
0: TRAIN [2][3470/3880]	Time 0.185 (0.164)	Data 1.03e-04 (1.90e-04)	Tok/s 91244 (87429)	Loss/tok 3.0877 (3.1411)	LR 1.250e-04
0: TRAIN [2][3480/3880]	Time 0.126 (0.164)	Data 1.14e-04 (1.90e-04)	Tok/s 82641 (87434)	Loss/tok 3.0450 (3.1410)	LR 1.250e-04
0: TRAIN [2][3490/3880]	Time 0.125 (0.164)	Data 1.17e-04 (1.89e-04)	Tok/s 83148 (87431)	Loss/tok 2.8391 (3.1409)	LR 1.250e-04
0: TRAIN [2][3500/3880]	Time 0.125 (0.164)	Data 1.04e-04 (1.89e-04)	Tok/s 81699 (87424)	Loss/tok 2.9934 (3.1408)	LR 1.250e-04
0: TRAIN [2][3510/3880]	Time 0.126 (0.164)	Data 1.29e-04 (1.89e-04)	Tok/s 81689 (87425)	Loss/tok 2.9199 (3.1408)	LR 1.250e-04
0: TRAIN [2][3520/3880]	Time 0.125 (0.164)	Data 1.06e-04 (1.89e-04)	Tok/s 81657 (87427)	Loss/tok 2.8771 (3.1408)	LR 1.250e-04
0: TRAIN [2][3530/3880]	Time 0.127 (0.164)	Data 1.24e-04 (1.89e-04)	Tok/s 82088 (87423)	Loss/tok 2.9795 (3.1406)	LR 1.250e-04
0: TRAIN [2][3540/3880]	Time 0.317 (0.164)	Data 1.08e-04 (1.88e-04)	Tok/s 94015 (87419)	Loss/tok 3.5331 (3.1407)	LR 1.250e-04
0: TRAIN [2][3550/3880]	Time 0.182 (0.164)	Data 1.30e-04 (1.88e-04)	Tok/s 93069 (87421)	Loss/tok 3.0941 (3.1411)	LR 1.250e-04
0: TRAIN [2][3560/3880]	Time 0.125 (0.164)	Data 1.36e-04 (1.88e-04)	Tok/s 83843 (87417)	Loss/tok 2.8445 (3.1409)	LR 1.250e-04
0: TRAIN [2][3570/3880]	Time 0.124 (0.164)	Data 1.11e-04 (1.88e-04)	Tok/s 82177 (87409)	Loss/tok 2.9271 (3.1407)	LR 1.250e-04
0: TRAIN [2][3580/3880]	Time 0.182 (0.164)	Data 1.08e-04 (1.88e-04)	Tok/s 90235 (87412)	Loss/tok 3.1477 (3.1405)	LR 1.250e-04
0: TRAIN [2][3590/3880]	Time 0.182 (0.164)	Data 1.13e-04 (1.87e-04)	Tok/s 91275 (87423)	Loss/tok 3.1227 (3.1405)	LR 1.250e-04
0: TRAIN [2][3600/3880]	Time 0.247 (0.164)	Data 1.09e-04 (1.87e-04)	Tok/s 93626 (87429)	Loss/tok 3.2792 (3.1405)	LR 1.250e-04
0: TRAIN [2][3610/3880]	Time 0.185 (0.164)	Data 1.03e-04 (1.87e-04)	Tok/s 89620 (87423)	Loss/tok 3.2506 (3.1402)	LR 1.250e-04
0: TRAIN [2][3620/3880]	Time 0.245 (0.164)	Data 1.18e-04 (1.87e-04)	Tok/s 95323 (87434)	Loss/tok 3.3697 (3.1404)	LR 1.250e-04
0: TRAIN [2][3630/3880]	Time 0.123 (0.164)	Data 1.06e-04 (1.87e-04)	Tok/s 84639 (87431)	Loss/tok 2.9789 (3.1404)	LR 1.250e-04
0: TRAIN [2][3640/3880]	Time 0.126 (0.164)	Data 1.09e-04 (1.87e-04)	Tok/s 81743 (87433)	Loss/tok 2.9376 (3.1403)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [2][3650/3880]	Time 0.314 (0.164)	Data 1.15e-04 (1.86e-04)	Tok/s 94854 (87431)	Loss/tok 3.4458 (3.1406)	LR 1.250e-04
0: TRAIN [2][3660/3880]	Time 0.180 (0.164)	Data 1.23e-04 (1.86e-04)	Tok/s 93757 (87432)	Loss/tok 3.2136 (3.1404)	LR 1.250e-04
0: TRAIN [2][3670/3880]	Time 0.243 (0.164)	Data 1.26e-04 (1.86e-04)	Tok/s 96069 (87435)	Loss/tok 3.2960 (3.1406)	LR 1.250e-04
0: TRAIN [2][3680/3880]	Time 0.181 (0.164)	Data 1.39e-04 (1.86e-04)	Tok/s 93285 (87430)	Loss/tok 3.1235 (3.1403)	LR 1.250e-04
0: TRAIN [2][3690/3880]	Time 0.127 (0.164)	Data 1.12e-04 (1.86e-04)	Tok/s 80292 (87436)	Loss/tok 3.0164 (3.1406)	LR 1.250e-04
0: TRAIN [2][3700/3880]	Time 0.183 (0.164)	Data 1.05e-04 (1.85e-04)	Tok/s 92853 (87435)	Loss/tok 3.0149 (3.1404)	LR 1.250e-04
0: TRAIN [2][3710/3880]	Time 0.124 (0.164)	Data 1.34e-04 (1.85e-04)	Tok/s 84540 (87431)	Loss/tok 2.9236 (3.1402)	LR 1.250e-04
0: TRAIN [2][3720/3880]	Time 0.126 (0.164)	Data 1.11e-04 (1.85e-04)	Tok/s 81538 (87436)	Loss/tok 2.9767 (3.1403)	LR 1.250e-04
0: TRAIN [2][3730/3880]	Time 0.125 (0.164)	Data 1.30e-04 (1.85e-04)	Tok/s 82071 (87438)	Loss/tok 2.8805 (3.1403)	LR 1.250e-04
0: TRAIN [2][3740/3880]	Time 0.183 (0.164)	Data 1.14e-04 (1.85e-04)	Tok/s 91894 (87440)	Loss/tok 3.2115 (3.1405)	LR 1.250e-04
0: TRAIN [2][3750/3880]	Time 0.126 (0.164)	Data 1.18e-04 (1.85e-04)	Tok/s 81451 (87440)	Loss/tok 2.9197 (3.1406)	LR 1.250e-04
0: TRAIN [2][3760/3880]	Time 0.125 (0.164)	Data 1.08e-04 (1.84e-04)	Tok/s 83162 (87442)	Loss/tok 2.9344 (3.1405)	LR 1.250e-04
0: TRAIN [2][3770/3880]	Time 0.125 (0.164)	Data 1.11e-04 (1.84e-04)	Tok/s 82863 (87442)	Loss/tok 2.9785 (3.1403)	LR 1.250e-04
0: TRAIN [2][3780/3880]	Time 0.124 (0.164)	Data 1.18e-04 (1.84e-04)	Tok/s 83765 (87436)	Loss/tok 2.8394 (3.1400)	LR 1.250e-04
0: TRAIN [2][3790/3880]	Time 0.066 (0.164)	Data 1.07e-04 (1.84e-04)	Tok/s 80483 (87426)	Loss/tok 2.6518 (3.1396)	LR 1.250e-04
0: TRAIN [2][3800/3880]	Time 0.067 (0.163)	Data 1.09e-04 (1.84e-04)	Tok/s 78262 (87419)	Loss/tok 2.5606 (3.1393)	LR 1.250e-04
0: TRAIN [2][3810/3880]	Time 0.183 (0.163)	Data 1.33e-04 (1.83e-04)	Tok/s 91413 (87415)	Loss/tok 3.0471 (3.1392)	LR 1.250e-04
0: TRAIN [2][3820/3880]	Time 0.123 (0.163)	Data 1.19e-04 (1.83e-04)	Tok/s 82044 (87416)	Loss/tok 3.0750 (3.1394)	LR 1.250e-04
0: TRAIN [2][3830/3880]	Time 0.067 (0.163)	Data 1.12e-04 (1.83e-04)	Tok/s 78012 (87415)	Loss/tok 2.6027 (3.1393)	LR 1.250e-04
0: TRAIN [2][3840/3880]	Time 0.186 (0.163)	Data 1.30e-04 (1.83e-04)	Tok/s 90158 (87422)	Loss/tok 3.1531 (3.1395)	LR 1.250e-04
0: TRAIN [2][3850/3880]	Time 0.317 (0.163)	Data 1.47e-04 (1.83e-04)	Tok/s 93663 (87425)	Loss/tok 3.4850 (3.1395)	LR 1.250e-04
0: TRAIN [2][3860/3880]	Time 0.245 (0.164)	Data 1.29e-04 (1.83e-04)	Tok/s 96735 (87427)	Loss/tok 3.2679 (3.1396)	LR 1.250e-04
0: TRAIN [2][3870/3880]	Time 0.184 (0.164)	Data 1.09e-04 (1.83e-04)	Tok/s 92261 (87427)	Loss/tok 3.0686 (3.1396)	LR 1.250e-04
:::MLL 1586325217.875 epoch_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 524}}
:::MLL 1586325217.876 eval_start: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [2][0/6]	Time 0.733 (0.733)	Decoder iters 149.0 (149.0)	Tok/s 22528 (22528)
0: Running moses detokenizer
0: BLEU(score=23.619787065314583, counts=[37018, 18374, 10425, 6183], totals=[65858, 62855, 59852, 56854], precisions=[56.208812900482855, 29.232360194097527, 17.417964311969524, 10.87522425862736], bp=1.0, sys_len=65858, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586325220.889 eval_accuracy: {"value": 23.62, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 535}}
:::MLL 1586325220.890 eval_stop: {"value": null, "metadata": {"epoch_num": 3, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 2	Training Loss: 3.1398	Test BLEU: 23.62
0: Performance: Epoch: 2	Training: 349657 Tok/s
0: Finished epoch 2
:::MLL 1586325220.890 block_stop: {"value": null, "metadata": {"first_epoch_num": 3, "file": "train.py", "lineno": 557}}
:::MLL 1586325220.890 block_start: {"value": null, "metadata": {"first_epoch_num": 4, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1586325220.891 epoch_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 514}}
0: Starting epoch 3
0: Executing preallocation
0: Sampler for epoch 3 uses seed 3600465734
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][0/3880]	Time 0.626 (0.626)	Data 4.46e-01 (4.46e-01)	Tok/s 27054 (27054)	Loss/tok 3.0322 (3.0322)	LR 1.250e-04
0: TRAIN [3][10/3880]	Time 0.125 (0.213)	Data 1.27e-04 (4.07e-02)	Tok/s 82765 (82629)	Loss/tok 2.9296 (3.1142)	LR 1.250e-04
0: TRAIN [3][20/3880]	Time 0.244 (0.206)	Data 1.14e-04 (2.14e-02)	Tok/s 96371 (86008)	Loss/tok 3.2361 (3.1621)	LR 1.250e-04
0: TRAIN [3][30/3880]	Time 0.128 (0.188)	Data 1.16e-04 (1.45e-02)	Tok/s 80559 (86080)	Loss/tok 2.9540 (3.1186)	LR 1.250e-04
0: TRAIN [3][40/3880]	Time 0.187 (0.180)	Data 1.08e-04 (1.10e-02)	Tok/s 89126 (86170)	Loss/tok 3.0635 (3.0981)	LR 1.250e-04
0: TRAIN [3][50/3880]	Time 0.126 (0.173)	Data 1.19e-04 (8.87e-03)	Tok/s 82085 (85938)	Loss/tok 2.9562 (3.0830)	LR 1.250e-04
0: TRAIN [3][60/3880]	Time 0.245 (0.174)	Data 1.36e-04 (7.43e-03)	Tok/s 94144 (86176)	Loss/tok 3.3889 (3.1057)	LR 1.250e-04
0: TRAIN [3][70/3880]	Time 0.126 (0.174)	Data 1.09e-04 (6.40e-03)	Tok/s 81841 (86327)	Loss/tok 3.0170 (3.1150)	LR 1.250e-04
0: TRAIN [3][80/3880]	Time 0.125 (0.170)	Data 1.09e-04 (5.63e-03)	Tok/s 82299 (86160)	Loss/tok 2.9074 (3.1031)	LR 1.250e-04
0: TRAIN [3][90/3880]	Time 0.068 (0.168)	Data 1.62e-04 (5.02e-03)	Tok/s 76654 (86419)	Loss/tok 2.6981 (3.0985)	LR 1.250e-04
0: TRAIN [3][100/3880]	Time 0.124 (0.168)	Data 1.33e-04 (4.54e-03)	Tok/s 82859 (86384)	Loss/tok 2.9525 (3.1052)	LR 1.250e-04
0: TRAIN [3][110/3880]	Time 0.124 (0.168)	Data 1.13e-04 (4.14e-03)	Tok/s 82620 (86455)	Loss/tok 2.9224 (3.1083)	LR 1.250e-04
0: TRAIN [3][120/3880]	Time 0.245 (0.168)	Data 1.10e-04 (3.81e-03)	Tok/s 94373 (86533)	Loss/tok 3.3084 (3.1074)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][130/3880]	Time 0.320 (0.168)	Data 1.13e-04 (3.53e-03)	Tok/s 94301 (86628)	Loss/tok 3.3188 (3.1057)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][140/3880]	Time 0.242 (0.167)	Data 1.20e-04 (3.29e-03)	Tok/s 96007 (86706)	Loss/tok 3.3259 (3.1053)	LR 1.250e-04
0: TRAIN [3][150/3880]	Time 0.243 (0.168)	Data 1.26e-04 (3.08e-03)	Tok/s 96350 (86926)	Loss/tok 3.1506 (3.1069)	LR 1.250e-04
0: TRAIN [3][160/3880]	Time 0.124 (0.168)	Data 1.36e-04 (2.89e-03)	Tok/s 82645 (87003)	Loss/tok 3.0634 (3.1080)	LR 1.250e-04
0: TRAIN [3][170/3880]	Time 0.184 (0.168)	Data 1.31e-04 (2.73e-03)	Tok/s 91500 (87057)	Loss/tok 3.0670 (3.1082)	LR 1.250e-04
0: TRAIN [3][180/3880]	Time 0.124 (0.167)	Data 1.38e-04 (2.59e-03)	Tok/s 81870 (87046)	Loss/tok 2.9361 (3.1048)	LR 1.250e-04
0: TRAIN [3][190/3880]	Time 0.067 (0.166)	Data 9.94e-05 (2.46e-03)	Tok/s 76796 (87089)	Loss/tok 2.5553 (3.1001)	LR 1.250e-04
0: TRAIN [3][200/3880]	Time 0.183 (0.166)	Data 9.73e-05 (2.34e-03)	Tok/s 92773 (87133)	Loss/tok 3.0527 (3.1006)	LR 1.250e-04
0: TRAIN [3][210/3880]	Time 0.069 (0.165)	Data 9.42e-05 (2.24e-03)	Tok/s 78505 (87034)	Loss/tok 2.5215 (3.0987)	LR 1.250e-04
0: TRAIN [3][220/3880]	Time 0.184 (0.166)	Data 1.22e-04 (2.14e-03)	Tok/s 91359 (87156)	Loss/tok 3.0326 (3.1003)	LR 1.250e-04
0: TRAIN [3][230/3880]	Time 0.124 (0.165)	Data 1.16e-04 (2.05e-03)	Tok/s 84370 (87139)	Loss/tok 2.8975 (3.0996)	LR 1.250e-04
0: TRAIN [3][240/3880]	Time 0.183 (0.166)	Data 9.47e-05 (1.97e-03)	Tok/s 91973 (87281)	Loss/tok 3.0472 (3.1032)	LR 1.250e-04
0: TRAIN [3][250/3880]	Time 0.124 (0.166)	Data 1.13e-04 (1.90e-03)	Tok/s 82347 (87240)	Loss/tok 2.9691 (3.1038)	LR 1.250e-04
0: TRAIN [3][260/3880]	Time 0.125 (0.166)	Data 1.08e-04 (1.83e-03)	Tok/s 82801 (87288)	Loss/tok 2.8003 (3.1032)	LR 1.250e-04
0: TRAIN [3][270/3880]	Time 0.068 (0.165)	Data 9.73e-05 (1.76e-03)	Tok/s 76935 (87247)	Loss/tok 2.4881 (3.1021)	LR 1.250e-04
0: TRAIN [3][280/3880]	Time 0.124 (0.164)	Data 1.06e-04 (1.71e-03)	Tok/s 83041 (87135)	Loss/tok 2.8037 (3.0975)	LR 1.250e-04
0: TRAIN [3][290/3880]	Time 0.182 (0.164)	Data 1.08e-04 (1.65e-03)	Tok/s 92752 (87084)	Loss/tok 3.1070 (3.0962)	LR 1.250e-04
0: TRAIN [3][300/3880]	Time 0.318 (0.163)	Data 1.24e-04 (1.60e-03)	Tok/s 91713 (87060)	Loss/tok 3.4988 (3.0977)	LR 1.250e-04
0: TRAIN [3][310/3880]	Time 0.126 (0.164)	Data 1.22e-04 (1.55e-03)	Tok/s 82082 (87064)	Loss/tok 2.8265 (3.0982)	LR 1.250e-04
0: TRAIN [3][320/3880]	Time 0.123 (0.163)	Data 1.40e-04 (1.51e-03)	Tok/s 83401 (87033)	Loss/tok 2.9604 (3.0990)	LR 1.250e-04
0: TRAIN [3][330/3880]	Time 0.181 (0.163)	Data 1.12e-04 (1.47e-03)	Tok/s 92598 (87043)	Loss/tok 3.0793 (3.0975)	LR 1.250e-04
0: TRAIN [3][340/3880]	Time 0.185 (0.163)	Data 1.44e-04 (1.43e-03)	Tok/s 91335 (87122)	Loss/tok 3.2222 (3.0995)	LR 1.250e-04
0: TRAIN [3][350/3880]	Time 0.244 (0.164)	Data 1.24e-04 (1.39e-03)	Tok/s 95128 (87188)	Loss/tok 3.3556 (3.1056)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][360/3880]	Time 0.124 (0.164)	Data 1.15e-04 (1.35e-03)	Tok/s 84432 (87227)	Loss/tok 2.9036 (3.1053)	LR 1.250e-04
0: TRAIN [3][370/3880]	Time 0.182 (0.164)	Data 1.10e-04 (1.32e-03)	Tok/s 92366 (87199)	Loss/tok 3.1099 (3.1031)	LR 1.250e-04
0: TRAIN [3][380/3880]	Time 0.124 (0.164)	Data 1.22e-04 (1.29e-03)	Tok/s 81409 (87180)	Loss/tok 2.9252 (3.1033)	LR 1.250e-04
0: TRAIN [3][390/3880]	Time 0.126 (0.163)	Data 1.14e-04 (1.26e-03)	Tok/s 82205 (87156)	Loss/tok 2.9458 (3.1021)	LR 1.250e-04
0: TRAIN [3][400/3880]	Time 0.246 (0.164)	Data 1.11e-04 (1.23e-03)	Tok/s 95702 (87173)	Loss/tok 3.2124 (3.1010)	LR 1.250e-04
0: TRAIN [3][410/3880]	Time 0.126 (0.164)	Data 1.27e-04 (1.20e-03)	Tok/s 81447 (87224)	Loss/tok 3.0095 (3.1024)	LR 1.250e-04
0: TRAIN [3][420/3880]	Time 0.184 (0.164)	Data 1.21e-04 (1.18e-03)	Tok/s 91758 (87194)	Loss/tok 3.1488 (3.1020)	LR 1.250e-04
0: TRAIN [3][430/3880]	Time 0.317 (0.164)	Data 1.13e-04 (1.15e-03)	Tok/s 94189 (87203)	Loss/tok 3.3716 (3.1037)	LR 1.250e-04
0: TRAIN [3][440/3880]	Time 0.126 (0.164)	Data 1.17e-04 (1.13e-03)	Tok/s 81570 (87191)	Loss/tok 2.8721 (3.1021)	LR 1.250e-04
0: TRAIN [3][450/3880]	Time 0.182 (0.165)	Data 1.25e-04 (1.11e-03)	Tok/s 92220 (87279)	Loss/tok 3.0154 (3.1039)	LR 1.250e-04
0: TRAIN [3][460/3880]	Time 0.185 (0.164)	Data 1.29e-04 (1.09e-03)	Tok/s 90016 (87252)	Loss/tok 3.2578 (3.1037)	LR 1.250e-04
0: TRAIN [3][470/3880]	Time 0.124 (0.164)	Data 1.27e-04 (1.07e-03)	Tok/s 82480 (87271)	Loss/tok 2.7679 (3.1031)	LR 1.250e-04
0: TRAIN [3][480/3880]	Time 0.124 (0.164)	Data 1.07e-04 (1.05e-03)	Tok/s 84301 (87217)	Loss/tok 2.8097 (3.1027)	LR 1.250e-04
0: TRAIN [3][490/3880]	Time 0.184 (0.164)	Data 1.44e-04 (1.03e-03)	Tok/s 90760 (87206)	Loss/tok 3.1806 (3.1021)	LR 1.250e-04
0: TRAIN [3][500/3880]	Time 0.245 (0.164)	Data 1.31e-04 (1.01e-03)	Tok/s 94731 (87216)	Loss/tok 3.4242 (3.1038)	LR 1.250e-04
0: TRAIN [3][510/3880]	Time 0.184 (0.164)	Data 1.23e-04 (9.91e-04)	Tok/s 90432 (87199)	Loss/tok 3.1070 (3.1035)	LR 1.250e-04
0: TRAIN [3][520/3880]	Time 0.125 (0.163)	Data 1.12e-04 (9.75e-04)	Tok/s 83995 (87157)	Loss/tok 2.9364 (3.1014)	LR 1.250e-04
0: TRAIN [3][530/3880]	Time 0.124 (0.163)	Data 1.16e-04 (9.58e-04)	Tok/s 84946 (87132)	Loss/tok 2.8894 (3.1000)	LR 1.250e-04
0: TRAIN [3][540/3880]	Time 0.124 (0.163)	Data 1.31e-04 (9.43e-04)	Tok/s 83512 (87153)	Loss/tok 2.9045 (3.0993)	LR 1.250e-04
0: TRAIN [3][550/3880]	Time 0.124 (0.163)	Data 1.26e-04 (9.28e-04)	Tok/s 85892 (87202)	Loss/tok 2.9213 (3.1009)	LR 1.250e-04
0: TRAIN [3][560/3880]	Time 0.183 (0.163)	Data 1.15e-04 (9.14e-04)	Tok/s 92301 (87224)	Loss/tok 3.1714 (3.1011)	LR 1.250e-04
0: TRAIN [3][570/3880]	Time 0.182 (0.164)	Data 1.15e-04 (9.00e-04)	Tok/s 92461 (87300)	Loss/tok 3.0918 (3.1024)	LR 1.250e-04
0: TRAIN [3][580/3880]	Time 0.245 (0.164)	Data 1.10e-04 (8.86e-04)	Tok/s 95528 (87327)	Loss/tok 3.1877 (3.1027)	LR 1.250e-04
0: TRAIN [3][590/3880]	Time 0.127 (0.164)	Data 1.18e-04 (8.73e-04)	Tok/s 82136 (87299)	Loss/tok 2.9092 (3.1017)	LR 1.250e-04
0: TRAIN [3][600/3880]	Time 0.126 (0.164)	Data 1.26e-04 (8.61e-04)	Tok/s 81515 (87326)	Loss/tok 2.8574 (3.1029)	LR 1.250e-04
0: TRAIN [3][610/3880]	Time 0.315 (0.164)	Data 1.24e-04 (8.48e-04)	Tok/s 95442 (87335)	Loss/tok 3.3269 (3.1028)	LR 1.250e-04
0: TRAIN [3][620/3880]	Time 0.125 (0.164)	Data 1.05e-04 (8.37e-04)	Tok/s 83572 (87351)	Loss/tok 2.8436 (3.1015)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][630/3880]	Time 0.067 (0.164)	Data 1.08e-04 (8.25e-04)	Tok/s 78144 (87334)	Loss/tok 2.5015 (3.1003)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][640/3880]	Time 0.126 (0.164)	Data 1.28e-04 (8.14e-04)	Tok/s 81716 (87342)	Loss/tok 2.8918 (3.1009)	LR 1.250e-04
0: TRAIN [3][650/3880]	Time 0.183 (0.164)	Data 1.27e-04 (8.03e-04)	Tok/s 91223 (87353)	Loss/tok 3.1326 (3.1002)	LR 1.250e-04
0: TRAIN [3][660/3880]	Time 0.128 (0.164)	Data 1.26e-04 (7.93e-04)	Tok/s 78572 (87354)	Loss/tok 2.8640 (3.1007)	LR 1.250e-04
0: TRAIN [3][670/3880]	Time 0.186 (0.164)	Data 1.28e-04 (7.83e-04)	Tok/s 91236 (87343)	Loss/tok 2.9982 (3.1001)	LR 1.250e-04
0: TRAIN [3][680/3880]	Time 0.183 (0.164)	Data 1.12e-04 (7.73e-04)	Tok/s 91410 (87336)	Loss/tok 3.0945 (3.0988)	LR 1.250e-04
0: TRAIN [3][690/3880]	Time 0.126 (0.164)	Data 1.30e-04 (7.64e-04)	Tok/s 84509 (87408)	Loss/tok 2.8121 (3.1016)	LR 1.250e-04
0: TRAIN [3][700/3880]	Time 0.126 (0.165)	Data 1.41e-04 (7.55e-04)	Tok/s 82977 (87427)	Loss/tok 2.9514 (3.1036)	LR 1.250e-04
0: TRAIN [3][710/3880]	Time 0.187 (0.165)	Data 1.13e-04 (7.46e-04)	Tok/s 88527 (87422)	Loss/tok 3.2377 (3.1028)	LR 1.250e-04
0: TRAIN [3][720/3880]	Time 0.127 (0.165)	Data 1.26e-04 (7.37e-04)	Tok/s 82410 (87431)	Loss/tok 2.9286 (3.1040)	LR 1.250e-04
0: TRAIN [3][730/3880]	Time 0.128 (0.165)	Data 1.17e-04 (7.29e-04)	Tok/s 80231 (87433)	Loss/tok 2.9674 (3.1039)	LR 1.250e-04
0: TRAIN [3][740/3880]	Time 0.067 (0.165)	Data 1.30e-04 (7.20e-04)	Tok/s 79079 (87427)	Loss/tok 2.5513 (3.1039)	LR 1.250e-04
0: TRAIN [3][750/3880]	Time 0.244 (0.165)	Data 1.10e-04 (7.12e-04)	Tok/s 96509 (87449)	Loss/tok 3.1867 (3.1040)	LR 1.250e-04
0: TRAIN [3][760/3880]	Time 0.124 (0.166)	Data 1.32e-04 (7.05e-04)	Tok/s 84164 (87474)	Loss/tok 2.9417 (3.1052)	LR 1.250e-04
0: TRAIN [3][770/3880]	Time 0.242 (0.166)	Data 1.23e-04 (6.97e-04)	Tok/s 96312 (87476)	Loss/tok 3.4451 (3.1054)	LR 1.250e-04
0: TRAIN [3][780/3880]	Time 0.243 (0.166)	Data 1.13e-04 (6.89e-04)	Tok/s 96937 (87501)	Loss/tok 3.2424 (3.1073)	LR 1.250e-04
0: TRAIN [3][790/3880]	Time 0.127 (0.166)	Data 1.25e-04 (6.82e-04)	Tok/s 81235 (87476)	Loss/tok 3.0017 (3.1062)	LR 1.250e-04
0: TRAIN [3][800/3880]	Time 0.182 (0.165)	Data 1.13e-04 (6.75e-04)	Tok/s 92477 (87437)	Loss/tok 3.0773 (3.1050)	LR 1.250e-04
0: TRAIN [3][810/3880]	Time 0.068 (0.165)	Data 1.38e-04 (6.69e-04)	Tok/s 76710 (87413)	Loss/tok 2.4289 (3.1043)	LR 1.250e-04
0: TRAIN [3][820/3880]	Time 0.183 (0.165)	Data 1.34e-04 (6.62e-04)	Tok/s 91918 (87462)	Loss/tok 3.0867 (3.1050)	LR 1.250e-04
0: TRAIN [3][830/3880]	Time 0.123 (0.165)	Data 1.12e-04 (6.56e-04)	Tok/s 84608 (87444)	Loss/tok 2.8011 (3.1039)	LR 1.250e-04
0: TRAIN [3][840/3880]	Time 0.123 (0.165)	Data 1.12e-04 (6.49e-04)	Tok/s 83732 (87420)	Loss/tok 2.8642 (3.1033)	LR 1.250e-04
0: TRAIN [3][850/3880]	Time 0.124 (0.165)	Data 1.11e-04 (6.43e-04)	Tok/s 82663 (87411)	Loss/tok 2.9047 (3.1026)	LR 1.250e-04
0: TRAIN [3][860/3880]	Time 0.128 (0.165)	Data 1.42e-04 (6.37e-04)	Tok/s 81611 (87421)	Loss/tok 2.7968 (3.1026)	LR 1.250e-04
0: TRAIN [3][870/3880]	Time 0.184 (0.164)	Data 1.17e-04 (6.31e-04)	Tok/s 90480 (87413)	Loss/tok 3.1716 (3.1018)	LR 1.250e-04
0: TRAIN [3][880/3880]	Time 0.183 (0.164)	Data 1.09e-04 (6.25e-04)	Tok/s 91479 (87409)	Loss/tok 3.1344 (3.1016)	LR 1.250e-04
0: TRAIN [3][890/3880]	Time 0.125 (0.164)	Data 1.26e-04 (6.20e-04)	Tok/s 82314 (87392)	Loss/tok 2.8712 (3.1004)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][900/3880]	Time 0.125 (0.164)	Data 1.08e-04 (6.14e-04)	Tok/s 84217 (87362)	Loss/tok 2.9131 (3.0996)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][910/3880]	Time 0.184 (0.164)	Data 1.12e-04 (6.08e-04)	Tok/s 90106 (87352)	Loss/tok 3.0376 (3.0993)	LR 1.250e-04
0: TRAIN [3][920/3880]	Time 0.184 (0.164)	Data 1.13e-04 (6.03e-04)	Tok/s 90482 (87355)	Loss/tok 3.1825 (3.0995)	LR 1.250e-04
0: TRAIN [3][930/3880]	Time 0.183 (0.163)	Data 1.03e-04 (5.98e-04)	Tok/s 92278 (87348)	Loss/tok 3.1284 (3.0987)	LR 1.250e-04
0: TRAIN [3][940/3880]	Time 0.184 (0.163)	Data 1.08e-04 (5.93e-04)	Tok/s 91015 (87325)	Loss/tok 3.1449 (3.0978)	LR 1.250e-04
0: TRAIN [3][950/3880]	Time 0.184 (0.163)	Data 1.19e-04 (5.88e-04)	Tok/s 91902 (87358)	Loss/tok 3.1025 (3.0982)	LR 1.250e-04
0: TRAIN [3][960/3880]	Time 0.125 (0.163)	Data 1.12e-04 (5.83e-04)	Tok/s 81471 (87348)	Loss/tok 2.9091 (3.0978)	LR 1.250e-04
0: TRAIN [3][970/3880]	Time 0.246 (0.163)	Data 1.26e-04 (5.78e-04)	Tok/s 95840 (87352)	Loss/tok 3.2913 (3.0981)	LR 1.250e-04
0: TRAIN [3][980/3880]	Time 0.124 (0.163)	Data 1.37e-04 (5.73e-04)	Tok/s 82597 (87314)	Loss/tok 2.8413 (3.0978)	LR 1.250e-04
0: TRAIN [3][990/3880]	Time 0.317 (0.163)	Data 1.27e-04 (5.69e-04)	Tok/s 93544 (87326)	Loss/tok 3.3805 (3.0979)	LR 1.250e-04
0: TRAIN [3][1000/3880]	Time 0.183 (0.163)	Data 1.22e-04 (5.64e-04)	Tok/s 90822 (87309)	Loss/tok 3.1271 (3.0978)	LR 1.250e-04
0: TRAIN [3][1010/3880]	Time 0.184 (0.163)	Data 1.27e-04 (5.60e-04)	Tok/s 90661 (87313)	Loss/tok 2.9730 (3.0973)	LR 1.250e-04
0: TRAIN [3][1020/3880]	Time 0.126 (0.163)	Data 1.17e-04 (5.56e-04)	Tok/s 81327 (87302)	Loss/tok 3.0325 (3.0966)	LR 1.250e-04
0: TRAIN [3][1030/3880]	Time 0.124 (0.163)	Data 1.22e-04 (5.51e-04)	Tok/s 85169 (87282)	Loss/tok 2.8788 (3.0959)	LR 1.250e-04
0: TRAIN [3][1040/3880]	Time 0.127 (0.163)	Data 1.27e-04 (5.47e-04)	Tok/s 80191 (87277)	Loss/tok 3.0125 (3.0956)	LR 1.250e-04
0: TRAIN [3][1050/3880]	Time 0.184 (0.163)	Data 1.37e-04 (5.43e-04)	Tok/s 91306 (87272)	Loss/tok 3.1086 (3.0954)	LR 1.250e-04
0: TRAIN [3][1060/3880]	Time 0.125 (0.163)	Data 1.23e-04 (5.39e-04)	Tok/s 82177 (87281)	Loss/tok 2.9180 (3.0955)	LR 1.250e-04
0: TRAIN [3][1070/3880]	Time 0.068 (0.162)	Data 1.26e-04 (5.35e-04)	Tok/s 79253 (87267)	Loss/tok 2.4588 (3.0955)	LR 1.250e-04
0: TRAIN [3][1080/3880]	Time 0.181 (0.163)	Data 1.24e-04 (5.32e-04)	Tok/s 91850 (87272)	Loss/tok 3.1372 (3.0958)	LR 1.250e-04
0: TRAIN [3][1090/3880]	Time 0.183 (0.163)	Data 1.29e-04 (5.28e-04)	Tok/s 91680 (87268)	Loss/tok 3.0310 (3.0961)	LR 1.250e-04
0: TRAIN [3][1100/3880]	Time 0.123 (0.162)	Data 1.28e-04 (5.24e-04)	Tok/s 82868 (87259)	Loss/tok 2.8857 (3.0954)	LR 1.250e-04
0: TRAIN [3][1110/3880]	Time 0.242 (0.163)	Data 1.29e-04 (5.21e-04)	Tok/s 97086 (87263)	Loss/tok 3.2231 (3.0961)	LR 1.250e-04
0: TRAIN [3][1120/3880]	Time 0.181 (0.163)	Data 1.28e-04 (5.17e-04)	Tok/s 92395 (87283)	Loss/tok 3.1397 (3.0962)	LR 1.250e-04
0: TRAIN [3][1130/3880]	Time 0.184 (0.163)	Data 1.13e-04 (5.14e-04)	Tok/s 90669 (87278)	Loss/tok 3.0777 (3.0960)	LR 1.250e-04
0: TRAIN [3][1140/3880]	Time 0.182 (0.163)	Data 1.17e-04 (5.10e-04)	Tok/s 91975 (87280)	Loss/tok 3.1381 (3.0958)	LR 1.250e-04
0: TRAIN [3][1150/3880]	Time 0.180 (0.163)	Data 1.35e-04 (5.07e-04)	Tok/s 94018 (87281)	Loss/tok 3.1356 (3.0957)	LR 1.250e-04
0: TRAIN [3][1160/3880]	Time 0.123 (0.162)	Data 1.14e-04 (5.03e-04)	Tok/s 83602 (87275)	Loss/tok 2.9602 (3.0954)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1170/3880]	Time 0.066 (0.162)	Data 1.22e-04 (5.00e-04)	Tok/s 77793 (87272)	Loss/tok 2.4983 (3.0955)	LR 1.250e-04
0: TRAIN [3][1180/3880]	Time 0.123 (0.162)	Data 1.26e-04 (4.97e-04)	Tok/s 83504 (87259)	Loss/tok 2.9437 (3.0948)	LR 1.250e-04
0: TRAIN [3][1190/3880]	Time 0.247 (0.162)	Data 1.21e-04 (4.94e-04)	Tok/s 93611 (87268)	Loss/tok 3.3702 (3.0946)	LR 1.250e-04
0: TRAIN [3][1200/3880]	Time 0.123 (0.162)	Data 1.30e-04 (4.91e-04)	Tok/s 82251 (87281)	Loss/tok 2.7995 (3.0944)	LR 1.250e-04
0: TRAIN [3][1210/3880]	Time 0.246 (0.162)	Data 1.29e-04 (4.88e-04)	Tok/s 94838 (87274)	Loss/tok 3.3037 (3.0943)	LR 1.250e-04
0: TRAIN [3][1220/3880]	Time 0.317 (0.162)	Data 1.14e-04 (4.85e-04)	Tok/s 95186 (87259)	Loss/tok 3.4643 (3.0941)	LR 1.250e-04
0: TRAIN [3][1230/3880]	Time 0.125 (0.162)	Data 1.31e-04 (4.82e-04)	Tok/s 82979 (87270)	Loss/tok 2.8905 (3.0955)	LR 1.250e-04
0: TRAIN [3][1240/3880]	Time 0.182 (0.162)	Data 1.18e-04 (4.79e-04)	Tok/s 93675 (87286)	Loss/tok 3.0947 (3.0957)	LR 1.250e-04
0: TRAIN [3][1250/3880]	Time 0.184 (0.162)	Data 1.17e-04 (4.76e-04)	Tok/s 92074 (87289)	Loss/tok 3.0435 (3.0955)	LR 1.250e-04
0: TRAIN [3][1260/3880]	Time 0.125 (0.162)	Data 1.27e-04 (4.73e-04)	Tok/s 80478 (87299)	Loss/tok 2.8658 (3.0965)	LR 1.250e-04
0: TRAIN [3][1270/3880]	Time 0.182 (0.162)	Data 1.32e-04 (4.71e-04)	Tok/s 93031 (87304)	Loss/tok 3.0097 (3.0966)	LR 1.250e-04
0: TRAIN [3][1280/3880]	Time 0.317 (0.163)	Data 1.19e-04 (4.68e-04)	Tok/s 94940 (87330)	Loss/tok 3.3774 (3.0975)	LR 1.250e-04
0: TRAIN [3][1290/3880]	Time 0.124 (0.163)	Data 1.08e-04 (4.65e-04)	Tok/s 85591 (87321)	Loss/tok 2.8734 (3.0969)	LR 1.250e-04
0: TRAIN [3][1300/3880]	Time 0.181 (0.162)	Data 1.14e-04 (4.63e-04)	Tok/s 93480 (87302)	Loss/tok 3.1099 (3.0963)	LR 1.250e-04
0: TRAIN [3][1310/3880]	Time 0.124 (0.162)	Data 1.14e-04 (4.60e-04)	Tok/s 83143 (87282)	Loss/tok 2.9796 (3.0955)	LR 1.250e-04
0: TRAIN [3][1320/3880]	Time 0.124 (0.162)	Data 1.39e-04 (4.57e-04)	Tok/s 83064 (87279)	Loss/tok 2.8625 (3.0948)	LR 1.250e-04
0: TRAIN [3][1330/3880]	Time 0.123 (0.162)	Data 1.17e-04 (4.55e-04)	Tok/s 83698 (87283)	Loss/tok 2.9262 (3.0945)	LR 1.250e-04
0: TRAIN [3][1340/3880]	Time 0.125 (0.162)	Data 1.08e-04 (4.52e-04)	Tok/s 81915 (87260)	Loss/tok 3.0030 (3.0942)	LR 1.250e-04
0: TRAIN [3][1350/3880]	Time 0.123 (0.162)	Data 1.08e-04 (4.50e-04)	Tok/s 83483 (87250)	Loss/tok 2.9283 (3.0938)	LR 1.250e-04
0: TRAIN [3][1360/3880]	Time 0.127 (0.161)	Data 1.29e-04 (4.48e-04)	Tok/s 80356 (87234)	Loss/tok 3.0322 (3.0933)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1370/3880]	Time 0.316 (0.162)	Data 1.22e-04 (4.45e-04)	Tok/s 94833 (87234)	Loss/tok 3.4747 (3.0936)	LR 1.250e-04
0: TRAIN [3][1380/3880]	Time 0.183 (0.162)	Data 1.31e-04 (4.43e-04)	Tok/s 91067 (87259)	Loss/tok 3.2119 (3.0941)	LR 1.250e-04
0: TRAIN [3][1390/3880]	Time 0.184 (0.162)	Data 1.34e-04 (4.41e-04)	Tok/s 90788 (87256)	Loss/tok 3.1381 (3.0947)	LR 1.250e-04
0: TRAIN [3][1400/3880]	Time 0.068 (0.162)	Data 1.15e-04 (4.38e-04)	Tok/s 77835 (87250)	Loss/tok 2.5204 (3.0949)	LR 1.250e-04
0: TRAIN [3][1410/3880]	Time 0.125 (0.162)	Data 1.27e-04 (4.36e-04)	Tok/s 83110 (87234)	Loss/tok 2.9577 (3.0950)	LR 1.250e-04
0: TRAIN [3][1420/3880]	Time 0.245 (0.162)	Data 1.08e-04 (4.34e-04)	Tok/s 95226 (87249)	Loss/tok 3.2782 (3.0951)	LR 1.250e-04
0: TRAIN [3][1430/3880]	Time 0.126 (0.162)	Data 1.31e-04 (4.32e-04)	Tok/s 80978 (87244)	Loss/tok 3.0210 (3.0951)	LR 1.250e-04
0: TRAIN [3][1440/3880]	Time 0.127 (0.162)	Data 1.21e-04 (4.30e-04)	Tok/s 83148 (87213)	Loss/tok 2.9019 (3.0947)	LR 1.250e-04
0: TRAIN [3][1450/3880]	Time 0.184 (0.162)	Data 1.21e-04 (4.28e-04)	Tok/s 92289 (87231)	Loss/tok 3.0028 (3.0953)	LR 1.250e-04
0: TRAIN [3][1460/3880]	Time 0.126 (0.162)	Data 1.29e-04 (4.26e-04)	Tok/s 80890 (87240)	Loss/tok 2.8698 (3.0955)	LR 1.250e-04
0: TRAIN [3][1470/3880]	Time 0.128 (0.162)	Data 1.23e-04 (4.23e-04)	Tok/s 80032 (87238)	Loss/tok 2.9921 (3.0952)	LR 1.250e-04
0: TRAIN [3][1480/3880]	Time 0.245 (0.162)	Data 1.35e-04 (4.21e-04)	Tok/s 95494 (87233)	Loss/tok 3.2742 (3.0950)	LR 1.250e-04
0: TRAIN [3][1490/3880]	Time 0.124 (0.162)	Data 1.26e-04 (4.20e-04)	Tok/s 84557 (87247)	Loss/tok 2.7444 (3.0953)	LR 1.250e-04
0: TRAIN [3][1500/3880]	Time 0.126 (0.162)	Data 1.35e-04 (4.18e-04)	Tok/s 82437 (87261)	Loss/tok 2.9038 (3.0963)	LR 1.250e-04
0: TRAIN [3][1510/3880]	Time 0.181 (0.162)	Data 1.39e-04 (4.16e-04)	Tok/s 93175 (87297)	Loss/tok 3.0742 (3.0972)	LR 1.250e-04
0: TRAIN [3][1520/3880]	Time 0.127 (0.162)	Data 1.18e-04 (4.14e-04)	Tok/s 81514 (87289)	Loss/tok 2.8475 (3.0970)	LR 1.250e-04
0: TRAIN [3][1530/3880]	Time 0.122 (0.162)	Data 1.14e-04 (4.12e-04)	Tok/s 84582 (87287)	Loss/tok 2.9518 (3.0973)	LR 1.250e-04
0: TRAIN [3][1540/3880]	Time 0.245 (0.162)	Data 1.31e-04 (4.10e-04)	Tok/s 95412 (87275)	Loss/tok 3.3797 (3.0971)	LR 1.250e-04
0: TRAIN [3][1550/3880]	Time 0.182 (0.162)	Data 1.23e-04 (4.08e-04)	Tok/s 91132 (87264)	Loss/tok 3.2065 (3.0966)	LR 1.250e-04
0: TRAIN [3][1560/3880]	Time 0.182 (0.162)	Data 1.23e-04 (4.06e-04)	Tok/s 92069 (87263)	Loss/tok 3.1276 (3.0959)	LR 1.250e-04
0: TRAIN [3][1570/3880]	Time 0.183 (0.162)	Data 1.17e-04 (4.05e-04)	Tok/s 91667 (87272)	Loss/tok 3.1331 (3.0963)	LR 1.250e-04
0: TRAIN [3][1580/3880]	Time 0.314 (0.162)	Data 1.46e-04 (4.03e-04)	Tok/s 94987 (87273)	Loss/tok 3.3972 (3.0966)	LR 1.250e-04
0: TRAIN [3][1590/3880]	Time 0.182 (0.162)	Data 1.27e-04 (4.01e-04)	Tok/s 92496 (87278)	Loss/tok 3.0885 (3.0967)	LR 1.250e-04
0: TRAIN [3][1600/3880]	Time 0.184 (0.162)	Data 1.31e-04 (3.99e-04)	Tok/s 91858 (87270)	Loss/tok 3.0230 (3.0962)	LR 1.250e-04
0: TRAIN [3][1610/3880]	Time 0.125 (0.162)	Data 1.41e-04 (3.98e-04)	Tok/s 82748 (87279)	Loss/tok 3.0001 (3.0963)	LR 1.250e-04
0: TRAIN [3][1620/3880]	Time 0.182 (0.162)	Data 1.36e-04 (3.96e-04)	Tok/s 91588 (87291)	Loss/tok 3.0012 (3.0965)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][1630/3880]	Time 0.184 (0.162)	Data 1.15e-04 (3.94e-04)	Tok/s 91604 (87288)	Loss/tok 3.1689 (3.0966)	LR 1.250e-04
0: TRAIN [3][1640/3880]	Time 0.126 (0.162)	Data 1.44e-04 (3.93e-04)	Tok/s 82885 (87296)	Loss/tok 2.8654 (3.0967)	LR 1.250e-04
0: TRAIN [3][1650/3880]	Time 0.319 (0.162)	Data 1.34e-04 (3.91e-04)	Tok/s 93392 (87307)	Loss/tok 3.4096 (3.0971)	LR 1.250e-04
0: TRAIN [3][1660/3880]	Time 0.122 (0.162)	Data 1.38e-04 (3.90e-04)	Tok/s 85172 (87297)	Loss/tok 2.8602 (3.0966)	LR 1.250e-04
0: TRAIN [3][1670/3880]	Time 0.123 (0.162)	Data 1.13e-04 (3.88e-04)	Tok/s 82563 (87278)	Loss/tok 2.8941 (3.0960)	LR 1.250e-04
0: TRAIN [3][1680/3880]	Time 0.182 (0.162)	Data 1.14e-04 (3.87e-04)	Tok/s 92920 (87267)	Loss/tok 3.2241 (3.0957)	LR 1.250e-04
0: TRAIN [3][1690/3880]	Time 0.067 (0.162)	Data 1.33e-04 (3.85e-04)	Tok/s 77806 (87271)	Loss/tok 2.5166 (3.0959)	LR 1.250e-04
0: TRAIN [3][1700/3880]	Time 0.185 (0.162)	Data 1.31e-04 (3.83e-04)	Tok/s 91213 (87265)	Loss/tok 3.0409 (3.0955)	LR 1.250e-04
0: TRAIN [3][1710/3880]	Time 0.123 (0.162)	Data 1.33e-04 (3.82e-04)	Tok/s 82793 (87253)	Loss/tok 2.8666 (3.0950)	LR 1.250e-04
0: TRAIN [3][1720/3880]	Time 0.181 (0.162)	Data 1.24e-04 (3.81e-04)	Tok/s 93517 (87259)	Loss/tok 3.0395 (3.0946)	LR 1.250e-04
0: TRAIN [3][1730/3880]	Time 0.183 (0.162)	Data 1.10e-04 (3.79e-04)	Tok/s 92539 (87276)	Loss/tok 3.2120 (3.0953)	LR 1.250e-04
0: TRAIN [3][1740/3880]	Time 0.183 (0.162)	Data 1.29e-04 (3.78e-04)	Tok/s 92211 (87264)	Loss/tok 3.0984 (3.0951)	LR 1.250e-04
0: TRAIN [3][1750/3880]	Time 0.125 (0.162)	Data 1.11e-04 (3.76e-04)	Tok/s 81540 (87251)	Loss/tok 2.9510 (3.0946)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][1760/3880]	Time 0.241 (0.162)	Data 1.13e-04 (3.75e-04)	Tok/s 97213 (87271)	Loss/tok 3.2961 (3.0955)	LR 1.250e-04
0: TRAIN [3][1770/3880]	Time 0.123 (0.162)	Data 1.23e-04 (3.73e-04)	Tok/s 84488 (87277)	Loss/tok 2.9265 (3.0953)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1780/3880]	Time 0.124 (0.162)	Data 1.32e-04 (3.72e-04)	Tok/s 82981 (87296)	Loss/tok 2.8760 (3.0961)	LR 1.250e-04
0: TRAIN [3][1790/3880]	Time 0.317 (0.162)	Data 1.22e-04 (3.70e-04)	Tok/s 94318 (87292)	Loss/tok 3.4213 (3.0964)	LR 1.250e-04
0: TRAIN [3][1800/3880]	Time 0.184 (0.162)	Data 1.42e-04 (3.69e-04)	Tok/s 92309 (87300)	Loss/tok 3.1090 (3.0966)	LR 1.250e-04
0: TRAIN [3][1810/3880]	Time 0.316 (0.162)	Data 1.46e-04 (3.68e-04)	Tok/s 93087 (87301)	Loss/tok 3.4677 (3.0970)	LR 1.250e-04
0: TRAIN [3][1820/3880]	Time 0.242 (0.162)	Data 1.34e-04 (3.66e-04)	Tok/s 97511 (87319)	Loss/tok 3.2039 (3.0977)	LR 1.250e-04
0: TRAIN [3][1830/3880]	Time 0.067 (0.162)	Data 1.15e-04 (3.65e-04)	Tok/s 77971 (87304)	Loss/tok 2.6550 (3.0974)	LR 1.250e-04
0: TRAIN [3][1840/3880]	Time 0.067 (0.162)	Data 1.13e-04 (3.64e-04)	Tok/s 78074 (87294)	Loss/tok 2.5267 (3.0974)	LR 1.250e-04
0: TRAIN [3][1850/3880]	Time 0.181 (0.162)	Data 1.25e-04 (3.63e-04)	Tok/s 92280 (87308)	Loss/tok 3.1099 (3.0974)	LR 1.250e-04
0: TRAIN [3][1860/3880]	Time 0.244 (0.162)	Data 1.28e-04 (3.61e-04)	Tok/s 95697 (87312)	Loss/tok 3.2501 (3.0972)	LR 1.250e-04
0: TRAIN [3][1870/3880]	Time 0.244 (0.162)	Data 1.32e-04 (3.60e-04)	Tok/s 94896 (87309)	Loss/tok 3.2752 (3.0972)	LR 1.250e-04
0: TRAIN [3][1880/3880]	Time 0.244 (0.162)	Data 1.20e-04 (3.59e-04)	Tok/s 96429 (87306)	Loss/tok 3.2815 (3.0973)	LR 1.250e-04
0: TRAIN [3][1890/3880]	Time 0.068 (0.162)	Data 1.38e-04 (3.57e-04)	Tok/s 76896 (87296)	Loss/tok 2.4846 (3.0970)	LR 1.250e-04
0: TRAIN [3][1900/3880]	Time 0.245 (0.162)	Data 1.36e-04 (3.56e-04)	Tok/s 95005 (87287)	Loss/tok 3.2589 (3.0967)	LR 1.250e-04
0: TRAIN [3][1910/3880]	Time 0.126 (0.162)	Data 1.14e-04 (3.55e-04)	Tok/s 81822 (87277)	Loss/tok 2.9431 (3.0964)	LR 1.250e-04
0: TRAIN [3][1920/3880]	Time 0.243 (0.162)	Data 1.30e-04 (3.54e-04)	Tok/s 96111 (87278)	Loss/tok 3.1508 (3.0962)	LR 1.250e-04
0: TRAIN [3][1930/3880]	Time 0.185 (0.162)	Data 1.27e-04 (3.53e-04)	Tok/s 91569 (87289)	Loss/tok 3.0661 (3.0967)	LR 1.250e-04
0: TRAIN [3][1940/3880]	Time 0.125 (0.162)	Data 1.03e-04 (3.51e-04)	Tok/s 83943 (87290)	Loss/tok 2.8887 (3.0970)	LR 1.250e-04
0: TRAIN [3][1950/3880]	Time 0.243 (0.162)	Data 1.23e-04 (3.50e-04)	Tok/s 95545 (87307)	Loss/tok 3.2789 (3.0973)	LR 1.250e-04
0: TRAIN [3][1960/3880]	Time 0.244 (0.162)	Data 1.18e-04 (3.49e-04)	Tok/s 96093 (87323)	Loss/tok 3.2613 (3.0980)	LR 1.250e-04
0: TRAIN [3][1970/3880]	Time 0.247 (0.162)	Data 1.22e-04 (3.48e-04)	Tok/s 93964 (87326)	Loss/tok 3.2796 (3.0983)	LR 1.250e-04
0: TRAIN [3][1980/3880]	Time 0.125 (0.162)	Data 1.08e-04 (3.47e-04)	Tok/s 82547 (87318)	Loss/tok 2.9108 (3.0980)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][1990/3880]	Time 0.182 (0.162)	Data 1.16e-04 (3.46e-04)	Tok/s 93009 (87316)	Loss/tok 3.0742 (3.0980)	LR 1.250e-04
0: TRAIN [3][2000/3880]	Time 0.124 (0.162)	Data 1.07e-04 (3.45e-04)	Tok/s 82886 (87315)	Loss/tok 2.9540 (3.0983)	LR 1.250e-04
0: TRAIN [3][2010/3880]	Time 0.067 (0.162)	Data 1.15e-04 (3.43e-04)	Tok/s 80917 (87318)	Loss/tok 2.5277 (3.0982)	LR 1.250e-04
0: TRAIN [3][2020/3880]	Time 0.124 (0.162)	Data 1.11e-04 (3.42e-04)	Tok/s 83511 (87323)	Loss/tok 2.9719 (3.0981)	LR 1.250e-04
0: TRAIN [3][2030/3880]	Time 0.127 (0.162)	Data 1.04e-04 (3.41e-04)	Tok/s 80413 (87318)	Loss/tok 2.8184 (3.0977)	LR 1.250e-04
0: TRAIN [3][2040/3880]	Time 0.316 (0.162)	Data 1.07e-04 (3.40e-04)	Tok/s 93951 (87317)	Loss/tok 3.4244 (3.0977)	LR 1.250e-04
0: TRAIN [3][2050/3880]	Time 0.124 (0.162)	Data 1.04e-04 (3.39e-04)	Tok/s 84859 (87324)	Loss/tok 2.8320 (3.0977)	LR 1.250e-04
0: TRAIN [3][2060/3880]	Time 0.314 (0.162)	Data 1.08e-04 (3.38e-04)	Tok/s 94444 (87333)	Loss/tok 3.5123 (3.0980)	LR 1.250e-04
0: TRAIN [3][2070/3880]	Time 0.126 (0.162)	Data 1.11e-04 (3.37e-04)	Tok/s 81215 (87324)	Loss/tok 2.9043 (3.0978)	LR 1.250e-04
0: TRAIN [3][2080/3880]	Time 0.125 (0.162)	Data 1.21e-04 (3.36e-04)	Tok/s 81431 (87310)	Loss/tok 3.0648 (3.0972)	LR 1.250e-04
0: TRAIN [3][2090/3880]	Time 0.124 (0.162)	Data 1.07e-04 (3.34e-04)	Tok/s 84437 (87303)	Loss/tok 2.9472 (3.0975)	LR 1.250e-04
0: TRAIN [3][2100/3880]	Time 0.127 (0.162)	Data 1.08e-04 (3.33e-04)	Tok/s 80558 (87302)	Loss/tok 2.8631 (3.0973)	LR 1.250e-04
0: TRAIN [3][2110/3880]	Time 0.241 (0.162)	Data 1.28e-04 (3.32e-04)	Tok/s 97360 (87298)	Loss/tok 3.3410 (3.0974)	LR 1.250e-04
0: TRAIN [3][2120/3880]	Time 0.246 (0.162)	Data 1.04e-04 (3.31e-04)	Tok/s 94466 (87294)	Loss/tok 3.2975 (3.0972)	LR 1.250e-04
0: TRAIN [3][2130/3880]	Time 0.124 (0.162)	Data 1.08e-04 (3.30e-04)	Tok/s 84074 (87285)	Loss/tok 2.9940 (3.0966)	LR 1.250e-04
0: TRAIN [3][2140/3880]	Time 0.124 (0.162)	Data 1.09e-04 (3.29e-04)	Tok/s 85311 (87281)	Loss/tok 2.9314 (3.0963)	LR 1.250e-04
0: TRAIN [3][2150/3880]	Time 0.182 (0.162)	Data 1.31e-04 (3.28e-04)	Tok/s 90559 (87279)	Loss/tok 2.9652 (3.0965)	LR 1.250e-04
0: TRAIN [3][2160/3880]	Time 0.183 (0.162)	Data 1.05e-04 (3.27e-04)	Tok/s 91680 (87271)	Loss/tok 3.0705 (3.0961)	LR 1.250e-04
0: TRAIN [3][2170/3880]	Time 0.182 (0.162)	Data 1.34e-04 (3.26e-04)	Tok/s 92307 (87283)	Loss/tok 3.1615 (3.0965)	LR 1.250e-04
0: TRAIN [3][2180/3880]	Time 0.067 (0.162)	Data 1.18e-04 (3.25e-04)	Tok/s 77588 (87278)	Loss/tok 2.4743 (3.0965)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2190/3880]	Time 0.124 (0.162)	Data 1.26e-04 (3.24e-04)	Tok/s 83529 (87284)	Loss/tok 2.9282 (3.0965)	LR 1.250e-04
0: TRAIN [3][2200/3880]	Time 0.182 (0.162)	Data 1.26e-04 (3.24e-04)	Tok/s 93102 (87285)	Loss/tok 3.1118 (3.0965)	LR 1.250e-04
0: TRAIN [3][2210/3880]	Time 0.124 (0.162)	Data 1.39e-04 (3.23e-04)	Tok/s 84403 (87288)	Loss/tok 2.8060 (3.0964)	LR 1.250e-04
0: TRAIN [3][2220/3880]	Time 0.126 (0.162)	Data 1.14e-04 (3.22e-04)	Tok/s 81447 (87286)	Loss/tok 2.8811 (3.0967)	LR 1.250e-04
0: TRAIN [3][2230/3880]	Time 0.317 (0.162)	Data 1.22e-04 (3.21e-04)	Tok/s 92913 (87294)	Loss/tok 3.4058 (3.0969)	LR 1.250e-04
0: TRAIN [3][2240/3880]	Time 0.183 (0.162)	Data 1.31e-04 (3.20e-04)	Tok/s 90649 (87298)	Loss/tok 3.0730 (3.0969)	LR 1.250e-04
0: TRAIN [3][2250/3880]	Time 0.185 (0.162)	Data 1.08e-04 (3.19e-04)	Tok/s 91540 (87304)	Loss/tok 3.1799 (3.0970)	LR 1.250e-04
0: TRAIN [3][2260/3880]	Time 0.125 (0.162)	Data 1.18e-04 (3.18e-04)	Tok/s 82938 (87300)	Loss/tok 2.9869 (3.0970)	LR 1.250e-04
0: TRAIN [3][2270/3880]	Time 0.183 (0.162)	Data 1.14e-04 (3.17e-04)	Tok/s 91645 (87302)	Loss/tok 3.1095 (3.0967)	LR 1.250e-04
0: TRAIN [3][2280/3880]	Time 0.185 (0.162)	Data 1.17e-04 (3.16e-04)	Tok/s 92679 (87308)	Loss/tok 3.0672 (3.0971)	LR 1.250e-04
0: TRAIN [3][2290/3880]	Time 0.124 (0.162)	Data 1.24e-04 (3.16e-04)	Tok/s 83357 (87305)	Loss/tok 2.8550 (3.0970)	LR 1.250e-04
0: TRAIN [3][2300/3880]	Time 0.313 (0.162)	Data 1.42e-04 (3.15e-04)	Tok/s 93860 (87325)	Loss/tok 3.4407 (3.0976)	LR 1.250e-04
0: TRAIN [3][2310/3880]	Time 0.126 (0.162)	Data 1.10e-04 (3.14e-04)	Tok/s 81294 (87319)	Loss/tok 2.8244 (3.0972)	LR 1.250e-04
0: TRAIN [3][2320/3880]	Time 0.184 (0.162)	Data 1.30e-04 (3.13e-04)	Tok/s 92571 (87319)	Loss/tok 3.0580 (3.0972)	LR 1.250e-04
0: TRAIN [3][2330/3880]	Time 0.124 (0.162)	Data 1.13e-04 (3.12e-04)	Tok/s 80729 (87302)	Loss/tok 2.9289 (3.0968)	LR 1.250e-04
0: TRAIN [3][2340/3880]	Time 0.246 (0.162)	Data 1.34e-04 (3.11e-04)	Tok/s 95279 (87307)	Loss/tok 3.2507 (3.0970)	LR 1.250e-04
0: TRAIN [3][2350/3880]	Time 0.125 (0.162)	Data 1.41e-04 (3.11e-04)	Tok/s 82070 (87310)	Loss/tok 2.9308 (3.0969)	LR 1.250e-04
0: TRAIN [3][2360/3880]	Time 0.124 (0.162)	Data 1.26e-04 (3.10e-04)	Tok/s 84925 (87312)	Loss/tok 2.9338 (3.0966)	LR 1.250e-04
0: TRAIN [3][2370/3880]	Time 0.184 (0.162)	Data 1.46e-04 (3.09e-04)	Tok/s 90344 (87322)	Loss/tok 3.1261 (3.0968)	LR 1.250e-04
0: TRAIN [3][2380/3880]	Time 0.124 (0.162)	Data 1.11e-04 (3.08e-04)	Tok/s 82637 (87320)	Loss/tok 2.9025 (3.0968)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2390/3880]	Time 0.126 (0.162)	Data 1.37e-04 (3.08e-04)	Tok/s 81345 (87331)	Loss/tok 2.9277 (3.0975)	LR 1.250e-04
0: TRAIN [3][2400/3880]	Time 0.124 (0.162)	Data 1.22e-04 (3.07e-04)	Tok/s 84561 (87324)	Loss/tok 2.9791 (3.0976)	LR 1.250e-04
0: TRAIN [3][2410/3880]	Time 0.185 (0.162)	Data 1.23e-04 (3.06e-04)	Tok/s 91340 (87331)	Loss/tok 3.1837 (3.0979)	LR 1.250e-04
0: TRAIN [3][2420/3880]	Time 0.126 (0.162)	Data 1.35e-04 (3.05e-04)	Tok/s 82873 (87327)	Loss/tok 2.8120 (3.0977)	LR 1.250e-04
0: TRAIN [3][2430/3880]	Time 0.126 (0.162)	Data 1.41e-04 (3.05e-04)	Tok/s 82156 (87328)	Loss/tok 2.9160 (3.0985)	LR 1.250e-04
0: TRAIN [3][2440/3880]	Time 0.183 (0.162)	Data 1.32e-04 (3.04e-04)	Tok/s 91181 (87328)	Loss/tok 3.0487 (3.0986)	LR 1.250e-04
0: TRAIN [3][2450/3880]	Time 0.185 (0.162)	Data 1.30e-04 (3.03e-04)	Tok/s 89076 (87330)	Loss/tok 3.1214 (3.0984)	LR 1.250e-04
0: TRAIN [3][2460/3880]	Time 0.126 (0.162)	Data 1.39e-04 (3.02e-04)	Tok/s 83136 (87330)	Loss/tok 2.8165 (3.0985)	LR 1.250e-04
0: TRAIN [3][2470/3880]	Time 0.128 (0.162)	Data 1.40e-04 (3.02e-04)	Tok/s 81533 (87327)	Loss/tok 2.9228 (3.0989)	LR 1.250e-04
0: TRAIN [3][2480/3880]	Time 0.184 (0.162)	Data 1.39e-04 (3.01e-04)	Tok/s 92372 (87316)	Loss/tok 3.0949 (3.0986)	LR 1.250e-04
0: TRAIN [3][2490/3880]	Time 0.245 (0.162)	Data 1.36e-04 (3.00e-04)	Tok/s 95477 (87315)	Loss/tok 3.2747 (3.0984)	LR 1.250e-04
0: TRAIN [3][2500/3880]	Time 0.125 (0.162)	Data 1.27e-04 (3.00e-04)	Tok/s 84557 (87310)	Loss/tok 2.8211 (3.0983)	LR 1.250e-04
0: TRAIN [3][2510/3880]	Time 0.186 (0.162)	Data 1.22e-04 (2.99e-04)	Tok/s 90897 (87315)	Loss/tok 3.0332 (3.0985)	LR 1.250e-04
0: TRAIN [3][2520/3880]	Time 0.244 (0.162)	Data 1.34e-04 (2.98e-04)	Tok/s 96080 (87323)	Loss/tok 3.2618 (3.0985)	LR 1.250e-04
0: TRAIN [3][2530/3880]	Time 0.315 (0.162)	Data 1.18e-04 (2.98e-04)	Tok/s 93401 (87315)	Loss/tok 3.4746 (3.0986)	LR 1.250e-04
0: TRAIN [3][2540/3880]	Time 0.125 (0.163)	Data 1.20e-04 (2.97e-04)	Tok/s 81995 (87317)	Loss/tok 3.0260 (3.0990)	LR 1.250e-04
0: TRAIN [3][2550/3880]	Time 0.248 (0.163)	Data 1.29e-04 (2.96e-04)	Tok/s 93419 (87328)	Loss/tok 3.3732 (3.0995)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2560/3880]	Time 0.122 (0.163)	Data 1.28e-04 (2.96e-04)	Tok/s 85697 (87333)	Loss/tok 2.9293 (3.0996)	LR 1.250e-04
0: TRAIN [3][2570/3880]	Time 0.123 (0.163)	Data 1.14e-04 (2.95e-04)	Tok/s 83464 (87326)	Loss/tok 2.8941 (3.0995)	LR 1.250e-04
0: TRAIN [3][2580/3880]	Time 0.315 (0.163)	Data 1.13e-04 (2.94e-04)	Tok/s 94282 (87340)	Loss/tok 3.3652 (3.1004)	LR 1.250e-04
0: TRAIN [3][2590/3880]	Time 0.182 (0.163)	Data 1.14e-04 (2.94e-04)	Tok/s 91447 (87336)	Loss/tok 3.0421 (3.1002)	LR 1.250e-04
0: TRAIN [3][2600/3880]	Time 0.316 (0.163)	Data 1.32e-04 (2.93e-04)	Tok/s 94279 (87354)	Loss/tok 3.4281 (3.1009)	LR 1.250e-04
0: TRAIN [3][2610/3880]	Time 0.127 (0.163)	Data 1.41e-04 (2.92e-04)	Tok/s 82303 (87351)	Loss/tok 2.9143 (3.1009)	LR 1.250e-04
0: TRAIN [3][2620/3880]	Time 0.246 (0.163)	Data 1.24e-04 (2.92e-04)	Tok/s 95355 (87359)	Loss/tok 3.3029 (3.1009)	LR 1.250e-04
0: TRAIN [3][2630/3880]	Time 0.243 (0.163)	Data 1.49e-04 (2.91e-04)	Tok/s 95475 (87364)	Loss/tok 3.3051 (3.1010)	LR 1.250e-04
0: TRAIN [3][2640/3880]	Time 0.125 (0.163)	Data 1.33e-04 (2.91e-04)	Tok/s 83992 (87364)	Loss/tok 3.0802 (3.1010)	LR 1.250e-04
0: TRAIN [3][2650/3880]	Time 0.245 (0.163)	Data 1.18e-04 (2.90e-04)	Tok/s 95444 (87365)	Loss/tok 3.3300 (3.1012)	LR 1.250e-04
0: TRAIN [3][2660/3880]	Time 0.184 (0.163)	Data 1.17e-04 (2.89e-04)	Tok/s 91139 (87361)	Loss/tok 3.0730 (3.1010)	LR 1.250e-04
0: TRAIN [3][2670/3880]	Time 0.244 (0.163)	Data 1.41e-04 (2.89e-04)	Tok/s 96941 (87370)	Loss/tok 3.1952 (3.1014)	LR 1.250e-04
0: TRAIN [3][2680/3880]	Time 0.124 (0.163)	Data 1.37e-04 (2.88e-04)	Tok/s 81205 (87370)	Loss/tok 3.0111 (3.1015)	LR 1.250e-04
0: TRAIN [3][2690/3880]	Time 0.185 (0.163)	Data 1.17e-04 (2.88e-04)	Tok/s 90007 (87378)	Loss/tok 3.1353 (3.1019)	LR 1.250e-04
0: TRAIN [3][2700/3880]	Time 0.183 (0.163)	Data 1.34e-04 (2.87e-04)	Tok/s 91459 (87375)	Loss/tok 3.1468 (3.1019)	LR 1.250e-04
0: TRAIN [3][2710/3880]	Time 0.126 (0.163)	Data 1.16e-04 (2.86e-04)	Tok/s 83182 (87390)	Loss/tok 3.0113 (3.1023)	LR 1.250e-04
0: TRAIN [3][2720/3880]	Time 0.123 (0.163)	Data 1.20e-04 (2.86e-04)	Tok/s 82734 (87384)	Loss/tok 3.0347 (3.1022)	LR 1.250e-04
0: TRAIN [3][2730/3880]	Time 0.182 (0.163)	Data 1.24e-04 (2.85e-04)	Tok/s 91446 (87397)	Loss/tok 3.1071 (3.1025)	LR 1.250e-04
0: TRAIN [3][2740/3880]	Time 0.124 (0.163)	Data 1.11e-04 (2.84e-04)	Tok/s 84672 (87385)	Loss/tok 2.9075 (3.1023)	LR 1.250e-04
0: TRAIN [3][2750/3880]	Time 0.183 (0.163)	Data 1.08e-04 (2.84e-04)	Tok/s 92121 (87396)	Loss/tok 3.0396 (3.1027)	LR 1.250e-04
0: TRAIN [3][2760/3880]	Time 0.124 (0.163)	Data 1.24e-04 (2.83e-04)	Tok/s 83535 (87391)	Loss/tok 2.8909 (3.1025)	LR 1.250e-04
0: TRAIN [3][2770/3880]	Time 0.246 (0.163)	Data 1.16e-04 (2.83e-04)	Tok/s 95142 (87392)	Loss/tok 3.1936 (3.1024)	LR 1.250e-04
0: TRAIN [3][2780/3880]	Time 0.124 (0.163)	Data 1.14e-04 (2.82e-04)	Tok/s 83726 (87379)	Loss/tok 2.9830 (3.1020)	LR 1.250e-04
0: TRAIN [3][2790/3880]	Time 0.125 (0.163)	Data 1.12e-04 (2.82e-04)	Tok/s 81631 (87383)	Loss/tok 2.7919 (3.1022)	LR 1.250e-04
0: TRAIN [3][2800/3880]	Time 0.182 (0.163)	Data 1.20e-04 (2.81e-04)	Tok/s 91212 (87383)	Loss/tok 3.0480 (3.1022)	LR 1.250e-04
0: TRAIN [3][2810/3880]	Time 0.124 (0.163)	Data 1.24e-04 (2.80e-04)	Tok/s 81711 (87377)	Loss/tok 2.9342 (3.1019)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [3][2820/3880]	Time 0.181 (0.163)	Data 1.18e-04 (2.80e-04)	Tok/s 91632 (87375)	Loss/tok 2.9813 (3.1016)	LR 1.250e-04
0: TRAIN [3][2830/3880]	Time 0.124 (0.163)	Data 1.24e-04 (2.79e-04)	Tok/s 83500 (87368)	Loss/tok 2.8244 (3.1016)	LR 1.250e-04
0: TRAIN [3][2840/3880]	Time 0.181 (0.163)	Data 1.39e-04 (2.79e-04)	Tok/s 93178 (87371)	Loss/tok 3.0685 (3.1015)	LR 1.250e-04
0: TRAIN [3][2850/3880]	Time 0.185 (0.163)	Data 1.33e-04 (2.78e-04)	Tok/s 91270 (87369)	Loss/tok 3.0237 (3.1013)	LR 1.250e-04
0: TRAIN [3][2860/3880]	Time 0.124 (0.163)	Data 1.26e-04 (2.78e-04)	Tok/s 84035 (87371)	Loss/tok 2.8350 (3.1011)	LR 1.250e-04
0: TRAIN [3][2870/3880]	Time 0.123 (0.163)	Data 1.24e-04 (2.77e-04)	Tok/s 83576 (87371)	Loss/tok 2.9594 (3.1010)	LR 1.250e-04
0: TRAIN [3][2880/3880]	Time 0.182 (0.163)	Data 1.37e-04 (2.77e-04)	Tok/s 93589 (87370)	Loss/tok 3.0779 (3.1008)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][2890/3880]	Time 0.125 (0.163)	Data 1.22e-04 (2.76e-04)	Tok/s 81831 (87364)	Loss/tok 3.0340 (3.1008)	LR 1.250e-04
0: TRAIN [3][2900/3880]	Time 0.124 (0.163)	Data 1.18e-04 (2.75e-04)	Tok/s 83952 (87362)	Loss/tok 2.9930 (3.1007)	LR 1.250e-04
0: TRAIN [3][2910/3880]	Time 0.243 (0.163)	Data 1.29e-04 (2.75e-04)	Tok/s 95916 (87370)	Loss/tok 3.2206 (3.1009)	LR 1.250e-04
0: TRAIN [3][2920/3880]	Time 0.314 (0.163)	Data 1.21e-04 (2.74e-04)	Tok/s 94987 (87374)	Loss/tok 3.4624 (3.1014)	LR 1.250e-04
0: TRAIN [3][2930/3880]	Time 0.127 (0.163)	Data 1.27e-04 (2.74e-04)	Tok/s 82955 (87374)	Loss/tok 2.8339 (3.1017)	LR 1.250e-04
0: TRAIN [3][2940/3880]	Time 0.125 (0.163)	Data 1.33e-04 (2.73e-04)	Tok/s 85089 (87367)	Loss/tok 2.9665 (3.1018)	LR 1.250e-04
0: TRAIN [3][2950/3880]	Time 0.123 (0.163)	Data 1.34e-04 (2.73e-04)	Tok/s 85305 (87376)	Loss/tok 2.8737 (3.1019)	LR 1.250e-04
0: TRAIN [3][2960/3880]	Time 0.242 (0.163)	Data 1.18e-04 (2.72e-04)	Tok/s 96423 (87382)	Loss/tok 3.2135 (3.1022)	LR 1.250e-04
0: TRAIN [3][2970/3880]	Time 0.125 (0.163)	Data 1.16e-04 (2.72e-04)	Tok/s 83163 (87378)	Loss/tok 2.9089 (3.1023)	LR 1.250e-04
0: TRAIN [3][2980/3880]	Time 0.126 (0.163)	Data 1.42e-04 (2.71e-04)	Tok/s 81824 (87373)	Loss/tok 2.8369 (3.1019)	LR 1.250e-04
0: TRAIN [3][2990/3880]	Time 0.125 (0.163)	Data 1.18e-04 (2.71e-04)	Tok/s 82818 (87377)	Loss/tok 2.8393 (3.1019)	LR 1.250e-04
0: TRAIN [3][3000/3880]	Time 0.125 (0.163)	Data 1.11e-04 (2.70e-04)	Tok/s 82906 (87371)	Loss/tok 2.9154 (3.1017)	LR 1.250e-04
0: TRAIN [3][3010/3880]	Time 0.185 (0.163)	Data 1.34e-04 (2.70e-04)	Tok/s 89177 (87372)	Loss/tok 3.1159 (3.1017)	LR 1.250e-04
0: TRAIN [3][3020/3880]	Time 0.123 (0.163)	Data 1.25e-04 (2.69e-04)	Tok/s 82856 (87370)	Loss/tok 2.7997 (3.1017)	LR 1.250e-04
0: TRAIN [3][3030/3880]	Time 0.122 (0.163)	Data 1.08e-04 (2.69e-04)	Tok/s 87364 (87373)	Loss/tok 2.9320 (3.1019)	LR 1.250e-04
0: TRAIN [3][3040/3880]	Time 0.124 (0.163)	Data 1.33e-04 (2.69e-04)	Tok/s 83298 (87378)	Loss/tok 2.8371 (3.1022)	LR 1.250e-04
0: TRAIN [3][3050/3880]	Time 0.068 (0.163)	Data 1.16e-04 (2.68e-04)	Tok/s 76749 (87377)	Loss/tok 2.5268 (3.1022)	LR 1.250e-04
0: TRAIN [3][3060/3880]	Time 0.182 (0.163)	Data 1.16e-04 (2.68e-04)	Tok/s 92534 (87386)	Loss/tok 3.0801 (3.1022)	LR 1.250e-04
0: TRAIN [3][3070/3880]	Time 0.183 (0.163)	Data 1.24e-04 (2.67e-04)	Tok/s 92255 (87393)	Loss/tok 3.0376 (3.1024)	LR 1.250e-04
0: TRAIN [3][3080/3880]	Time 0.123 (0.163)	Data 1.26e-04 (2.67e-04)	Tok/s 83543 (87394)	Loss/tok 2.9225 (3.1026)	LR 1.250e-04
0: TRAIN [3][3090/3880]	Time 0.125 (0.163)	Data 1.13e-04 (2.66e-04)	Tok/s 85123 (87388)	Loss/tok 2.8572 (3.1024)	LR 1.250e-04
0: TRAIN [3][3100/3880]	Time 0.181 (0.163)	Data 1.34e-04 (2.66e-04)	Tok/s 94000 (87385)	Loss/tok 3.0685 (3.1021)	LR 1.250e-04
0: TRAIN [3][3110/3880]	Time 0.312 (0.163)	Data 1.24e-04 (2.65e-04)	Tok/s 95901 (87384)	Loss/tok 3.3659 (3.1021)	LR 1.250e-04
0: TRAIN [3][3120/3880]	Time 0.067 (0.163)	Data 1.14e-04 (2.65e-04)	Tok/s 77998 (87382)	Loss/tok 2.5800 (3.1018)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3130/3880]	Time 0.181 (0.163)	Data 1.21e-04 (2.64e-04)	Tok/s 91335 (87380)	Loss/tok 3.1242 (3.1018)	LR 1.250e-04
0: TRAIN [3][3140/3880]	Time 0.245 (0.163)	Data 1.13e-04 (2.64e-04)	Tok/s 94878 (87380)	Loss/tok 3.3986 (3.1020)	LR 1.250e-04
0: TRAIN [3][3150/3880]	Time 0.124 (0.163)	Data 1.24e-04 (2.63e-04)	Tok/s 83424 (87377)	Loss/tok 2.9377 (3.1018)	LR 1.250e-04
0: TRAIN [3][3160/3880]	Time 0.316 (0.163)	Data 1.38e-04 (2.63e-04)	Tok/s 94023 (87375)	Loss/tok 3.3971 (3.1017)	LR 1.250e-04
0: TRAIN [3][3170/3880]	Time 0.123 (0.163)	Data 1.17e-04 (2.63e-04)	Tok/s 82485 (87366)	Loss/tok 2.8714 (3.1015)	LR 1.250e-04
0: TRAIN [3][3180/3880]	Time 0.124 (0.163)	Data 1.37e-04 (2.62e-04)	Tok/s 83347 (87367)	Loss/tok 2.8921 (3.1017)	LR 1.250e-04
0: TRAIN [3][3190/3880]	Time 0.186 (0.163)	Data 1.45e-04 (2.62e-04)	Tok/s 90004 (87376)	Loss/tok 3.1043 (3.1017)	LR 1.250e-04
0: TRAIN [3][3200/3880]	Time 0.242 (0.163)	Data 1.18e-04 (2.61e-04)	Tok/s 97504 (87381)	Loss/tok 3.2451 (3.1020)	LR 1.250e-04
0: TRAIN [3][3210/3880]	Time 0.183 (0.163)	Data 1.30e-04 (2.61e-04)	Tok/s 91475 (87388)	Loss/tok 3.1390 (3.1021)	LR 1.250e-04
0: TRAIN [3][3220/3880]	Time 0.244 (0.163)	Data 1.25e-04 (2.61e-04)	Tok/s 94625 (87393)	Loss/tok 3.2622 (3.1025)	LR 1.250e-04
0: TRAIN [3][3230/3880]	Time 0.125 (0.163)	Data 1.17e-04 (2.60e-04)	Tok/s 83191 (87395)	Loss/tok 2.8605 (3.1025)	LR 1.250e-04
0: TRAIN [3][3240/3880]	Time 0.125 (0.163)	Data 1.43e-04 (2.60e-04)	Tok/s 83230 (87395)	Loss/tok 2.8721 (3.1024)	LR 1.250e-04
0: TRAIN [3][3250/3880]	Time 0.184 (0.163)	Data 1.28e-04 (2.59e-04)	Tok/s 92499 (87398)	Loss/tok 3.1169 (3.1022)	LR 1.250e-04
0: TRAIN [3][3260/3880]	Time 0.124 (0.163)	Data 1.17e-04 (2.59e-04)	Tok/s 81923 (87398)	Loss/tok 2.8651 (3.1022)	LR 1.250e-04
0: TRAIN [3][3270/3880]	Time 0.127 (0.163)	Data 1.09e-04 (2.58e-04)	Tok/s 81957 (87403)	Loss/tok 2.8661 (3.1024)	LR 1.250e-04
0: TRAIN [3][3280/3880]	Time 0.181 (0.163)	Data 1.37e-04 (2.58e-04)	Tok/s 91259 (87404)	Loss/tok 3.0903 (3.1023)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3290/3880]	Time 0.126 (0.163)	Data 1.43e-04 (2.58e-04)	Tok/s 82513 (87402)	Loss/tok 2.8935 (3.1026)	LR 1.250e-04
0: TRAIN [3][3300/3880]	Time 0.067 (0.164)	Data 1.10e-04 (2.57e-04)	Tok/s 81970 (87403)	Loss/tok 2.5260 (3.1029)	LR 1.250e-04
0: TRAIN [3][3310/3880]	Time 0.123 (0.163)	Data 1.36e-04 (2.57e-04)	Tok/s 85812 (87400)	Loss/tok 3.0372 (3.1028)	LR 1.250e-04
0: TRAIN [3][3320/3880]	Time 0.127 (0.164)	Data 1.26e-04 (2.56e-04)	Tok/s 81863 (87404)	Loss/tok 2.7838 (3.1032)	LR 1.250e-04
0: TRAIN [3][3330/3880]	Time 0.124 (0.164)	Data 1.19e-04 (2.56e-04)	Tok/s 83243 (87415)	Loss/tok 2.9033 (3.1035)	LR 1.250e-04
0: TRAIN [3][3340/3880]	Time 0.246 (0.164)	Data 1.34e-04 (2.55e-04)	Tok/s 95258 (87412)	Loss/tok 3.2737 (3.1034)	LR 1.250e-04
0: TRAIN [3][3350/3880]	Time 0.123 (0.164)	Data 1.11e-04 (2.55e-04)	Tok/s 84043 (87422)	Loss/tok 2.9503 (3.1035)	LR 1.250e-04
0: TRAIN [3][3360/3880]	Time 0.183 (0.164)	Data 1.10e-04 (2.55e-04)	Tok/s 91918 (87422)	Loss/tok 3.1496 (3.1035)	LR 1.250e-04
0: TRAIN [3][3370/3880]	Time 0.126 (0.164)	Data 1.26e-04 (2.54e-04)	Tok/s 81506 (87417)	Loss/tok 2.8845 (3.1034)	LR 1.250e-04
0: TRAIN [3][3380/3880]	Time 0.183 (0.164)	Data 1.42e-04 (2.54e-04)	Tok/s 91878 (87421)	Loss/tok 3.1282 (3.1036)	LR 1.250e-04
0: TRAIN [3][3390/3880]	Time 0.244 (0.164)	Data 1.15e-04 (2.53e-04)	Tok/s 96322 (87424)	Loss/tok 3.2894 (3.1040)	LR 1.250e-04
0: TRAIN [3][3400/3880]	Time 0.068 (0.164)	Data 1.28e-04 (2.53e-04)	Tok/s 76483 (87420)	Loss/tok 2.5442 (3.1040)	LR 1.250e-04
0: TRAIN [3][3410/3880]	Time 0.066 (0.164)	Data 1.44e-04 (2.53e-04)	Tok/s 80549 (87425)	Loss/tok 2.4839 (3.1042)	LR 1.250e-04
0: TRAIN [3][3420/3880]	Time 0.124 (0.164)	Data 1.24e-04 (2.52e-04)	Tok/s 83167 (87423)	Loss/tok 2.8165 (3.1042)	LR 1.250e-04
0: TRAIN [3][3430/3880]	Time 0.184 (0.164)	Data 1.42e-04 (2.52e-04)	Tok/s 91758 (87431)	Loss/tok 3.2025 (3.1044)	LR 1.250e-04
0: TRAIN [3][3440/3880]	Time 0.124 (0.164)	Data 1.08e-04 (2.52e-04)	Tok/s 83713 (87424)	Loss/tok 2.8631 (3.1043)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3450/3880]	Time 0.124 (0.164)	Data 1.10e-04 (2.51e-04)	Tok/s 84280 (87417)	Loss/tok 2.9297 (3.1043)	LR 1.250e-04
0: TRAIN [3][3460/3880]	Time 0.123 (0.164)	Data 1.16e-04 (2.51e-04)	Tok/s 84919 (87425)	Loss/tok 2.8732 (3.1045)	LR 1.250e-04
0: TRAIN [3][3470/3880]	Time 0.123 (0.164)	Data 1.31e-04 (2.50e-04)	Tok/s 83801 (87413)	Loss/tok 2.9065 (3.1042)	LR 1.250e-04
0: TRAIN [3][3480/3880]	Time 0.186 (0.164)	Data 1.22e-04 (2.50e-04)	Tok/s 91549 (87406)	Loss/tok 3.0899 (3.1039)	LR 1.250e-04
0: TRAIN [3][3490/3880]	Time 0.127 (0.164)	Data 1.25e-04 (2.50e-04)	Tok/s 81283 (87405)	Loss/tok 2.9174 (3.1038)	LR 1.250e-04
0: TRAIN [3][3500/3880]	Time 0.067 (0.164)	Data 1.27e-04 (2.49e-04)	Tok/s 79433 (87404)	Loss/tok 2.4954 (3.1037)	LR 1.250e-04
0: TRAIN [3][3510/3880]	Time 0.123 (0.164)	Data 1.16e-04 (2.49e-04)	Tok/s 82230 (87401)	Loss/tok 2.9203 (3.1035)	LR 1.250e-04
0: TRAIN [3][3520/3880]	Time 0.318 (0.164)	Data 1.32e-04 (2.49e-04)	Tok/s 92977 (87402)	Loss/tok 3.4424 (3.1037)	LR 1.250e-04
0: TRAIN [3][3530/3880]	Time 0.183 (0.164)	Data 1.33e-04 (2.48e-04)	Tok/s 92085 (87401)	Loss/tok 3.1771 (3.1036)	LR 1.250e-04
0: TRAIN [3][3540/3880]	Time 0.245 (0.163)	Data 1.17e-04 (2.48e-04)	Tok/s 96108 (87395)	Loss/tok 3.2288 (3.1033)	LR 1.250e-04
0: TRAIN [3][3550/3880]	Time 0.126 (0.163)	Data 1.50e-04 (2.48e-04)	Tok/s 80588 (87398)	Loss/tok 2.8960 (3.1033)	LR 1.250e-04
0: TRAIN [3][3560/3880]	Time 0.123 (0.163)	Data 1.22e-04 (2.47e-04)	Tok/s 83018 (87395)	Loss/tok 3.0190 (3.1033)	LR 1.250e-04
0: TRAIN [3][3570/3880]	Time 0.183 (0.163)	Data 1.42e-04 (2.47e-04)	Tok/s 91300 (87392)	Loss/tok 3.1194 (3.1030)	LR 1.250e-04
0: TRAIN [3][3580/3880]	Time 0.185 (0.163)	Data 1.21e-04 (2.47e-04)	Tok/s 90518 (87396)	Loss/tok 3.1827 (3.1031)	LR 1.250e-04
0: TRAIN [3][3590/3880]	Time 0.068 (0.163)	Data 1.38e-04 (2.46e-04)	Tok/s 78156 (87403)	Loss/tok 2.5283 (3.1034)	LR 1.250e-04
0: TRAIN [3][3600/3880]	Time 0.124 (0.164)	Data 1.34e-04 (2.46e-04)	Tok/s 84223 (87408)	Loss/tok 2.8580 (3.1037)	LR 1.250e-04
0: TRAIN [3][3610/3880]	Time 0.247 (0.164)	Data 1.39e-04 (2.46e-04)	Tok/s 94378 (87414)	Loss/tok 3.1809 (3.1038)	LR 1.250e-04
0: TRAIN [3][3620/3880]	Time 0.123 (0.164)	Data 1.24e-04 (2.45e-04)	Tok/s 84190 (87420)	Loss/tok 2.8400 (3.1043)	LR 1.250e-04
0: TRAIN [3][3630/3880]	Time 0.068 (0.164)	Data 1.30e-04 (2.45e-04)	Tok/s 77142 (87408)	Loss/tok 2.4409 (3.1039)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3640/3880]	Time 0.124 (0.164)	Data 1.31e-04 (2.45e-04)	Tok/s 82835 (87412)	Loss/tok 2.8736 (3.1040)	LR 1.250e-04
0: TRAIN [3][3650/3880]	Time 0.181 (0.164)	Data 1.16e-04 (2.44e-04)	Tok/s 92668 (87407)	Loss/tok 3.0543 (3.1037)	LR 1.250e-04
0: TRAIN [3][3660/3880]	Time 0.122 (0.164)	Data 1.13e-04 (2.44e-04)	Tok/s 83308 (87405)	Loss/tok 2.8801 (3.1038)	LR 1.250e-04
0: TRAIN [3][3670/3880]	Time 0.183 (0.164)	Data 1.39e-04 (2.44e-04)	Tok/s 93071 (87402)	Loss/tok 3.0509 (3.1038)	LR 1.250e-04
0: TRAIN [3][3680/3880]	Time 0.245 (0.164)	Data 1.21e-04 (2.43e-04)	Tok/s 93681 (87412)	Loss/tok 3.3316 (3.1040)	LR 1.250e-04
0: TRAIN [3][3690/3880]	Time 0.123 (0.164)	Data 1.15e-04 (2.43e-04)	Tok/s 83723 (87410)	Loss/tok 2.9850 (3.1039)	LR 1.250e-04
0: TRAIN [3][3700/3880]	Time 0.127 (0.164)	Data 1.36e-04 (2.43e-04)	Tok/s 80304 (87410)	Loss/tok 3.0438 (3.1039)	LR 1.250e-04
0: TRAIN [3][3710/3880]	Time 0.125 (0.164)	Data 1.44e-04 (2.43e-04)	Tok/s 81958 (87408)	Loss/tok 2.8106 (3.1038)	LR 1.250e-04
0: TRAIN [3][3720/3880]	Time 0.183 (0.164)	Data 1.32e-04 (2.42e-04)	Tok/s 91879 (87411)	Loss/tok 3.1211 (3.1038)	LR 1.250e-04
0: TRAIN [3][3730/3880]	Time 0.186 (0.164)	Data 1.31e-04 (2.42e-04)	Tok/s 91646 (87421)	Loss/tok 3.1915 (3.1041)	LR 1.250e-04
0: TRAIN [3][3740/3880]	Time 0.124 (0.164)	Data 1.34e-04 (2.42e-04)	Tok/s 83670 (87425)	Loss/tok 2.7815 (3.1041)	LR 1.250e-04
0: TRAIN [3][3750/3880]	Time 0.125 (0.164)	Data 1.14e-04 (2.41e-04)	Tok/s 82635 (87427)	Loss/tok 2.9014 (3.1041)	LR 1.250e-04
0: TRAIN [3][3760/3880]	Time 0.181 (0.164)	Data 1.12e-04 (2.41e-04)	Tok/s 92422 (87426)	Loss/tok 3.1671 (3.1039)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [3][3770/3880]	Time 0.246 (0.164)	Data 1.19e-04 (2.41e-04)	Tok/s 94992 (87430)	Loss/tok 3.2663 (3.1040)	LR 1.250e-04
0: TRAIN [3][3780/3880]	Time 0.123 (0.164)	Data 1.24e-04 (2.40e-04)	Tok/s 82779 (87431)	Loss/tok 2.9418 (3.1040)	LR 1.250e-04
0: TRAIN [3][3790/3880]	Time 0.182 (0.164)	Data 1.12e-04 (2.40e-04)	Tok/s 92251 (87431)	Loss/tok 3.1177 (3.1038)	LR 1.250e-04
0: TRAIN [3][3800/3880]	Time 0.123 (0.164)	Data 1.37e-04 (2.40e-04)	Tok/s 83371 (87435)	Loss/tok 2.8901 (3.1038)	LR 1.250e-04
0: TRAIN [3][3810/3880]	Time 0.124 (0.164)	Data 1.13e-04 (2.39e-04)	Tok/s 81990 (87428)	Loss/tok 2.9155 (3.1036)	LR 1.250e-04
0: TRAIN [3][3820/3880]	Time 0.315 (0.164)	Data 1.55e-04 (2.39e-04)	Tok/s 95169 (87433)	Loss/tok 3.4733 (3.1038)	LR 1.250e-04
0: TRAIN [3][3830/3880]	Time 0.318 (0.164)	Data 1.38e-04 (2.39e-04)	Tok/s 95411 (87431)	Loss/tok 3.2455 (3.1036)	LR 1.250e-04
0: TRAIN [3][3840/3880]	Time 0.068 (0.164)	Data 1.41e-04 (2.39e-04)	Tok/s 76790 (87423)	Loss/tok 2.5114 (3.1033)	LR 1.250e-04
0: TRAIN [3][3850/3880]	Time 0.123 (0.164)	Data 1.27e-04 (2.38e-04)	Tok/s 84923 (87429)	Loss/tok 2.9738 (3.1034)	LR 1.250e-04
0: TRAIN [3][3860/3880]	Time 0.067 (0.163)	Data 1.12e-04 (2.38e-04)	Tok/s 79007 (87418)	Loss/tok 2.5307 (3.1030)	LR 1.250e-04
0: TRAIN [3][3870/3880]	Time 0.184 (0.163)	Data 1.34e-04 (2.38e-04)	Tok/s 90914 (87420)	Loss/tok 3.0977 (3.1031)	LR 1.250e-04
:::MLL 1586325855.889 epoch_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 524}}
:::MLL 1586325855.889 eval_start: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [3][0/6]	Time 0.734 (0.734)	Decoder iters 149.0 (149.0)	Tok/s 22395 (22395)
0: Running moses detokenizer
0: BLEU(score=23.59206270360128, counts=[36992, 18411, 10362, 6068], totals=[65571, 62568, 59565, 56566], precisions=[56.41518354150463, 29.425584963559647, 17.39612188365651, 10.727292012869922], bp=1.0, sys_len=65571, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586325858.919 eval_accuracy: {"value": 23.59, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 535}}
:::MLL 1586325858.919 eval_stop: {"value": null, "metadata": {"epoch_num": 4, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 3	Training Loss: 3.1038	Test BLEU: 23.59
0: Performance: Epoch: 3	Training: 349689 Tok/s
0: Finished epoch 3
:::MLL 1586325858.920 block_stop: {"value": null, "metadata": {"first_epoch_num": 4, "file": "train.py", "lineno": 557}}
:::MLL 1586325858.920 block_start: {"value": null, "metadata": {"first_epoch_num": 5, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1586325858.920 epoch_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 514}}
0: Starting epoch 4
0: Executing preallocation
0: Sampler for epoch 4 uses seed 3979826307
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [4][0/3880]	Time 0.522 (0.522)	Data 3.35e-01 (3.35e-01)	Tok/s 31948 (31948)	Loss/tok 3.1122 (3.1122)	LR 1.250e-04
0: TRAIN [4][10/3880]	Time 0.070 (0.176)	Data 1.41e-04 (3.06e-02)	Tok/s 75354 (81352)	Loss/tok 2.5596 (2.9782)	LR 1.250e-04
0: TRAIN [4][20/3880]	Time 0.125 (0.154)	Data 1.32e-04 (1.61e-02)	Tok/s 81973 (82621)	Loss/tok 2.8906 (2.9750)	LR 1.250e-04
0: TRAIN [4][30/3880]	Time 0.182 (0.160)	Data 1.32e-04 (1.09e-02)	Tok/s 92886 (84729)	Loss/tok 2.9945 (3.0124)	LR 1.250e-04
0: TRAIN [4][40/3880]	Time 0.124 (0.157)	Data 1.18e-04 (8.29e-03)	Tok/s 85175 (85409)	Loss/tok 2.8873 (3.0091)	LR 1.250e-04
0: TRAIN [4][50/3880]	Time 0.066 (0.154)	Data 1.22e-04 (6.69e-03)	Tok/s 79410 (85539)	Loss/tok 2.4688 (3.0040)	LR 1.250e-04
0: TRAIN [4][60/3880]	Time 0.183 (0.153)	Data 1.24e-04 (5.61e-03)	Tok/s 93477 (85792)	Loss/tok 3.0670 (2.9994)	LR 1.250e-04
0: TRAIN [4][70/3880]	Time 0.068 (0.154)	Data 1.18e-04 (4.84e-03)	Tok/s 76794 (85904)	Loss/tok 2.5147 (3.0146)	LR 1.250e-04
0: TRAIN [4][80/3880]	Time 0.124 (0.152)	Data 1.16e-04 (4.26e-03)	Tok/s 84080 (85759)	Loss/tok 2.7979 (3.0073)	LR 1.250e-04
0: TRAIN [4][90/3880]	Time 0.124 (0.152)	Data 1.21e-04 (3.81e-03)	Tok/s 82722 (85842)	Loss/tok 2.8074 (3.0106)	LR 1.250e-04
0: TRAIN [4][100/3880]	Time 0.183 (0.149)	Data 1.21e-04 (3.44e-03)	Tok/s 91302 (85675)	Loss/tok 3.1825 (3.0034)	LR 1.250e-04
0: TRAIN [4][110/3880]	Time 0.124 (0.152)	Data 1.29e-04 (3.14e-03)	Tok/s 83395 (85932)	Loss/tok 2.9134 (3.0147)	LR 1.250e-04
0: TRAIN [4][120/3880]	Time 0.124 (0.153)	Data 1.38e-04 (2.89e-03)	Tok/s 83482 (86026)	Loss/tok 3.0107 (3.0173)	LR 1.250e-04
0: TRAIN [4][130/3880]	Time 0.243 (0.155)	Data 1.16e-04 (2.68e-03)	Tok/s 96124 (86287)	Loss/tok 3.1720 (3.0254)	LR 1.250e-04
0: TRAIN [4][140/3880]	Time 0.128 (0.157)	Data 1.27e-04 (2.50e-03)	Tok/s 80321 (86496)	Loss/tok 2.8823 (3.0341)	LR 1.250e-04
0: TRAIN [4][150/3880]	Time 0.245 (0.158)	Data 1.27e-04 (2.34e-03)	Tok/s 96432 (86632)	Loss/tok 3.2125 (3.0364)	LR 1.250e-04
0: TRAIN [4][160/3880]	Time 0.068 (0.157)	Data 1.22e-04 (2.21e-03)	Tok/s 79298 (86732)	Loss/tok 2.4388 (3.0374)	LR 1.250e-04
0: TRAIN [4][170/3880]	Time 0.068 (0.157)	Data 1.49e-04 (2.09e-03)	Tok/s 76853 (86741)	Loss/tok 2.4429 (3.0351)	LR 1.250e-04
0: TRAIN [4][180/3880]	Time 0.246 (0.160)	Data 1.19e-04 (1.98e-03)	Tok/s 94427 (86923)	Loss/tok 3.2275 (3.0462)	LR 1.250e-04
0: TRAIN [4][190/3880]	Time 0.124 (0.160)	Data 1.13e-04 (1.88e-03)	Tok/s 81817 (86973)	Loss/tok 2.8709 (3.0464)	LR 1.250e-04
0: TRAIN [4][200/3880]	Time 0.243 (0.161)	Data 1.26e-04 (1.79e-03)	Tok/s 94247 (87026)	Loss/tok 3.3509 (3.0510)	LR 1.250e-04
0: TRAIN [4][210/3880]	Time 0.125 (0.161)	Data 1.26e-04 (1.71e-03)	Tok/s 84187 (87147)	Loss/tok 2.7978 (3.0514)	LR 1.250e-04
0: TRAIN [4][220/3880]	Time 0.181 (0.162)	Data 1.27e-04 (1.64e-03)	Tok/s 92706 (87174)	Loss/tok 3.1301 (3.0547)	LR 1.250e-04
0: TRAIN [4][230/3880]	Time 0.183 (0.162)	Data 1.16e-04 (1.58e-03)	Tok/s 90298 (87275)	Loss/tok 3.1987 (3.0596)	LR 1.250e-04
0: TRAIN [4][240/3880]	Time 0.124 (0.163)	Data 1.29e-04 (1.52e-03)	Tok/s 85013 (87326)	Loss/tok 2.8966 (3.0603)	LR 1.250e-04
0: TRAIN [4][250/3880]	Time 0.067 (0.161)	Data 1.18e-04 (1.46e-03)	Tok/s 79805 (87219)	Loss/tok 2.4468 (3.0562)	LR 1.250e-04
0: TRAIN [4][260/3880]	Time 0.247 (0.161)	Data 1.35e-04 (1.41e-03)	Tok/s 95026 (87205)	Loss/tok 3.1608 (3.0552)	LR 1.250e-04
0: TRAIN [4][270/3880]	Time 0.182 (0.160)	Data 1.16e-04 (1.36e-03)	Tok/s 91737 (87147)	Loss/tok 3.1245 (3.0522)	LR 1.250e-04
0: TRAIN [4][280/3880]	Time 0.320 (0.161)	Data 1.57e-04 (1.32e-03)	Tok/s 92771 (87215)	Loss/tok 3.5641 (3.0618)	LR 1.250e-04
0: TRAIN [4][290/3880]	Time 0.181 (0.161)	Data 1.17e-04 (1.28e-03)	Tok/s 92545 (87188)	Loss/tok 3.1094 (3.0602)	LR 1.250e-04
0: TRAIN [4][300/3880]	Time 0.246 (0.161)	Data 1.46e-04 (1.24e-03)	Tok/s 93785 (87226)	Loss/tok 3.3437 (3.0638)	LR 1.250e-04
0: TRAIN [4][310/3880]	Time 0.124 (0.160)	Data 1.13e-04 (1.20e-03)	Tok/s 84168 (87091)	Loss/tok 3.0181 (3.0604)	LR 1.250e-04
0: TRAIN [4][320/3880]	Time 0.125 (0.160)	Data 1.36e-04 (1.17e-03)	Tok/s 81905 (87144)	Loss/tok 2.9813 (3.0606)	LR 1.250e-04
0: TRAIN [4][330/3880]	Time 0.126 (0.161)	Data 1.20e-04 (1.14e-03)	Tok/s 80504 (87223)	Loss/tok 2.8126 (3.0648)	LR 1.250e-04
0: TRAIN [4][340/3880]	Time 0.183 (0.161)	Data 1.19e-04 (1.11e-03)	Tok/s 91987 (87237)	Loss/tok 3.0065 (3.0639)	LR 1.250e-04
0: TRAIN [4][350/3880]	Time 0.123 (0.161)	Data 1.14e-04 (1.08e-03)	Tok/s 86032 (87255)	Loss/tok 2.8445 (3.0649)	LR 1.250e-04
0: TRAIN [4][360/3880]	Time 0.180 (0.161)	Data 1.27e-04 (1.05e-03)	Tok/s 92836 (87238)	Loss/tok 3.1264 (3.0651)	LR 1.250e-04
0: TRAIN [4][370/3880]	Time 0.066 (0.161)	Data 1.34e-04 (1.03e-03)	Tok/s 76026 (87243)	Loss/tok 2.4293 (3.0640)	LR 1.250e-04
0: TRAIN [4][380/3880]	Time 0.123 (0.160)	Data 1.14e-04 (1.00e-03)	Tok/s 84598 (87226)	Loss/tok 2.9190 (3.0619)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][390/3880]	Time 0.124 (0.161)	Data 1.44e-04 (9.83e-04)	Tok/s 83149 (87240)	Loss/tok 2.9323 (3.0639)	LR 1.250e-04
0: TRAIN [4][400/3880]	Time 0.125 (0.161)	Data 1.15e-04 (9.61e-04)	Tok/s 81570 (87260)	Loss/tok 2.8537 (3.0649)	LR 1.250e-04
0: TRAIN [4][410/3880]	Time 0.125 (0.160)	Data 1.34e-04 (9.41e-04)	Tok/s 82972 (87180)	Loss/tok 2.8696 (3.0621)	LR 1.250e-04
0: TRAIN [4][420/3880]	Time 0.183 (0.159)	Data 1.13e-04 (9.22e-04)	Tok/s 90536 (87119)	Loss/tok 3.0861 (3.0603)	LR 1.250e-04
0: TRAIN [4][430/3880]	Time 0.246 (0.161)	Data 1.38e-04 (9.03e-04)	Tok/s 94714 (87246)	Loss/tok 3.2305 (3.0649)	LR 1.250e-04
0: TRAIN [4][440/3880]	Time 0.128 (0.161)	Data 1.27e-04 (8.85e-04)	Tok/s 79391 (87268)	Loss/tok 2.8269 (3.0669)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][450/3880]	Time 0.125 (0.162)	Data 1.37e-04 (8.68e-04)	Tok/s 81696 (87304)	Loss/tok 2.9237 (3.0702)	LR 1.250e-04
0: TRAIN [4][460/3880]	Time 0.123 (0.161)	Data 1.28e-04 (8.52e-04)	Tok/s 84439 (87310)	Loss/tok 2.8712 (3.0692)	LR 1.250e-04
0: TRAIN [4][470/3880]	Time 0.185 (0.161)	Data 1.29e-04 (8.37e-04)	Tok/s 90097 (87269)	Loss/tok 3.1768 (3.0686)	LR 1.250e-04
0: TRAIN [4][480/3880]	Time 0.316 (0.161)	Data 1.19e-04 (8.22e-04)	Tok/s 93288 (87264)	Loss/tok 3.4923 (3.0717)	LR 1.250e-04
0: TRAIN [4][490/3880]	Time 0.122 (0.162)	Data 1.35e-04 (8.08e-04)	Tok/s 83315 (87281)	Loss/tok 2.9918 (3.0726)	LR 1.250e-04
0: TRAIN [4][500/3880]	Time 0.068 (0.161)	Data 1.40e-04 (7.94e-04)	Tok/s 78854 (87232)	Loss/tok 2.5318 (3.0716)	LR 1.250e-04
0: TRAIN [4][510/3880]	Time 0.245 (0.161)	Data 1.13e-04 (7.81e-04)	Tok/s 95976 (87261)	Loss/tok 3.3172 (3.0746)	LR 1.250e-04
0: TRAIN [4][520/3880]	Time 0.122 (0.161)	Data 1.26e-04 (7.69e-04)	Tok/s 83611 (87254)	Loss/tok 2.8572 (3.0736)	LR 1.250e-04
0: TRAIN [4][530/3880]	Time 0.241 (0.161)	Data 1.18e-04 (7.56e-04)	Tok/s 96227 (87277)	Loss/tok 3.1999 (3.0735)	LR 1.250e-04
0: TRAIN [4][540/3880]	Time 0.124 (0.161)	Data 1.33e-04 (7.45e-04)	Tok/s 82425 (87314)	Loss/tok 2.9122 (3.0737)	LR 1.250e-04
0: TRAIN [4][550/3880]	Time 0.245 (0.161)	Data 1.09e-04 (7.33e-04)	Tok/s 95236 (87304)	Loss/tok 3.1926 (3.0739)	LR 1.250e-04
0: TRAIN [4][560/3880]	Time 0.313 (0.162)	Data 1.08e-04 (7.22e-04)	Tok/s 96130 (87372)	Loss/tok 3.3631 (3.0760)	LR 1.250e-04
0: TRAIN [4][570/3880]	Time 0.182 (0.162)	Data 1.46e-04 (7.12e-04)	Tok/s 91612 (87357)	Loss/tok 3.0995 (3.0765)	LR 1.250e-04
0: TRAIN [4][580/3880]	Time 0.124 (0.163)	Data 1.18e-04 (7.02e-04)	Tok/s 82434 (87396)	Loss/tok 2.8532 (3.0784)	LR 1.250e-04
0: TRAIN [4][590/3880]	Time 0.245 (0.163)	Data 1.14e-04 (6.92e-04)	Tok/s 94028 (87439)	Loss/tok 3.3403 (3.0795)	LR 1.250e-04
0: TRAIN [4][600/3880]	Time 0.317 (0.164)	Data 1.33e-04 (6.82e-04)	Tok/s 94018 (87459)	Loss/tok 3.3798 (3.0806)	LR 1.250e-04
0: TRAIN [4][610/3880]	Time 0.123 (0.163)	Data 1.09e-04 (6.73e-04)	Tok/s 83713 (87450)	Loss/tok 2.9496 (3.0793)	LR 1.250e-04
0: TRAIN [4][620/3880]	Time 0.182 (0.163)	Data 1.14e-04 (6.64e-04)	Tok/s 93751 (87455)	Loss/tok 3.0701 (3.0787)	LR 1.250e-04
0: TRAIN [4][630/3880]	Time 0.183 (0.163)	Data 1.49e-04 (6.56e-04)	Tok/s 90974 (87446)	Loss/tok 3.1544 (3.0777)	LR 1.250e-04
0: TRAIN [4][640/3880]	Time 0.125 (0.163)	Data 1.20e-04 (6.47e-04)	Tok/s 82613 (87422)	Loss/tok 2.8624 (3.0768)	LR 1.250e-04
0: TRAIN [4][650/3880]	Time 0.124 (0.162)	Data 1.33e-04 (6.39e-04)	Tok/s 83408 (87399)	Loss/tok 2.8431 (3.0770)	LR 1.250e-04
0: TRAIN [4][660/3880]	Time 0.185 (0.163)	Data 1.13e-04 (6.31e-04)	Tok/s 90744 (87412)	Loss/tok 3.0912 (3.0772)	LR 1.250e-04
0: TRAIN [4][670/3880]	Time 0.184 (0.162)	Data 1.28e-04 (6.24e-04)	Tok/s 90434 (87391)	Loss/tok 3.1382 (3.0758)	LR 1.250e-04
0: TRAIN [4][680/3880]	Time 0.126 (0.162)	Data 1.24e-04 (6.16e-04)	Tok/s 82177 (87341)	Loss/tok 2.7989 (3.0745)	LR 1.250e-04
0: TRAIN [4][690/3880]	Time 0.245 (0.162)	Data 1.24e-04 (6.09e-04)	Tok/s 96040 (87352)	Loss/tok 3.2002 (3.0753)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][700/3880]	Time 0.182 (0.162)	Data 1.18e-04 (6.02e-04)	Tok/s 92525 (87347)	Loss/tok 3.1369 (3.0756)	LR 1.250e-04
0: TRAIN [4][710/3880]	Time 0.243 (0.162)	Data 1.16e-04 (5.96e-04)	Tok/s 95600 (87378)	Loss/tok 3.2873 (3.0767)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][720/3880]	Time 0.120 (0.162)	Data 1.18e-04 (5.89e-04)	Tok/s 84523 (87360)	Loss/tok 2.9175 (3.0760)	LR 1.250e-04
0: TRAIN [4][730/3880]	Time 0.181 (0.162)	Data 1.29e-04 (5.83e-04)	Tok/s 92970 (87378)	Loss/tok 3.1366 (3.0752)	LR 1.250e-04
0: TRAIN [4][740/3880]	Time 0.123 (0.162)	Data 1.29e-04 (5.76e-04)	Tok/s 82372 (87404)	Loss/tok 2.8785 (3.0766)	LR 1.250e-04
0: TRAIN [4][750/3880]	Time 0.183 (0.162)	Data 1.32e-04 (5.71e-04)	Tok/s 91278 (87430)	Loss/tok 2.9818 (3.0773)	LR 1.250e-04
0: TRAIN [4][760/3880]	Time 0.182 (0.162)	Data 1.28e-04 (5.65e-04)	Tok/s 92446 (87453)	Loss/tok 3.0670 (3.0777)	LR 1.250e-04
0: TRAIN [4][770/3880]	Time 0.124 (0.162)	Data 1.22e-04 (5.59e-04)	Tok/s 82645 (87428)	Loss/tok 3.0205 (3.0772)	LR 1.250e-04
0: TRAIN [4][780/3880]	Time 0.123 (0.162)	Data 1.36e-04 (5.53e-04)	Tok/s 84402 (87439)	Loss/tok 2.9496 (3.0769)	LR 1.250e-04
0: TRAIN [4][790/3880]	Time 0.125 (0.162)	Data 1.21e-04 (5.48e-04)	Tok/s 82745 (87440)	Loss/tok 2.8452 (3.0761)	LR 1.250e-04
0: TRAIN [4][800/3880]	Time 0.067 (0.162)	Data 1.32e-04 (5.43e-04)	Tok/s 78199 (87431)	Loss/tok 2.5634 (3.0767)	LR 1.250e-04
0: TRAIN [4][810/3880]	Time 0.067 (0.162)	Data 1.32e-04 (5.38e-04)	Tok/s 78612 (87403)	Loss/tok 2.5432 (3.0757)	LR 1.250e-04
0: TRAIN [4][820/3880]	Time 0.183 (0.162)	Data 1.33e-04 (5.33e-04)	Tok/s 91404 (87415)	Loss/tok 2.9882 (3.0760)	LR 1.250e-04
0: TRAIN [4][830/3880]	Time 0.182 (0.162)	Data 1.19e-04 (5.28e-04)	Tok/s 93208 (87420)	Loss/tok 3.0066 (3.0760)	LR 1.250e-04
0: TRAIN [4][840/3880]	Time 0.125 (0.162)	Data 1.13e-04 (5.23e-04)	Tok/s 80810 (87411)	Loss/tok 2.8510 (3.0759)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][850/3880]	Time 0.310 (0.162)	Data 1.20e-04 (5.19e-04)	Tok/s 94097 (87421)	Loss/tok 3.5856 (3.0771)	LR 1.250e-04
0: TRAIN [4][860/3880]	Time 0.123 (0.161)	Data 1.08e-04 (5.14e-04)	Tok/s 83874 (87397)	Loss/tok 2.9686 (3.0762)	LR 1.250e-04
0: TRAIN [4][870/3880]	Time 0.316 (0.162)	Data 1.14e-04 (5.09e-04)	Tok/s 94852 (87410)	Loss/tok 3.5439 (3.0775)	LR 1.250e-04
0: TRAIN [4][880/3880]	Time 0.186 (0.161)	Data 1.28e-04 (5.05e-04)	Tok/s 92494 (87390)	Loss/tok 3.0352 (3.0766)	LR 1.250e-04
0: TRAIN [4][890/3880]	Time 0.123 (0.161)	Data 1.08e-04 (5.01e-04)	Tok/s 84251 (87371)	Loss/tok 2.9574 (3.0766)	LR 1.250e-04
0: TRAIN [4][900/3880]	Time 0.126 (0.161)	Data 1.09e-04 (4.96e-04)	Tok/s 82055 (87348)	Loss/tok 2.8674 (3.0762)	LR 1.250e-04
0: TRAIN [4][910/3880]	Time 0.245 (0.161)	Data 1.21e-04 (4.92e-04)	Tok/s 95740 (87355)	Loss/tok 3.2096 (3.0763)	LR 1.250e-04
0: TRAIN [4][920/3880]	Time 0.245 (0.161)	Data 1.15e-04 (4.88e-04)	Tok/s 94349 (87347)	Loss/tok 3.1821 (3.0765)	LR 1.250e-04
0: TRAIN [4][930/3880]	Time 0.181 (0.161)	Data 1.28e-04 (4.84e-04)	Tok/s 92765 (87346)	Loss/tok 3.0452 (3.0765)	LR 1.250e-04
0: TRAIN [4][940/3880]	Time 0.317 (0.161)	Data 1.28e-04 (4.80e-04)	Tok/s 95205 (87350)	Loss/tok 3.3377 (3.0766)	LR 1.250e-04
0: TRAIN [4][950/3880]	Time 0.126 (0.161)	Data 1.16e-04 (4.76e-04)	Tok/s 83475 (87317)	Loss/tok 2.9264 (3.0757)	LR 1.250e-04
0: TRAIN [4][960/3880]	Time 0.183 (0.160)	Data 1.06e-04 (4.73e-04)	Tok/s 93417 (87300)	Loss/tok 3.1348 (3.0746)	LR 1.250e-04
0: TRAIN [4][970/3880]	Time 0.123 (0.160)	Data 1.31e-04 (4.69e-04)	Tok/s 85114 (87270)	Loss/tok 2.8176 (3.0737)	LR 1.250e-04
0: TRAIN [4][980/3880]	Time 0.066 (0.160)	Data 1.10e-04 (4.65e-04)	Tok/s 80124 (87227)	Loss/tok 2.5144 (3.0726)	LR 1.250e-04
0: TRAIN [4][990/3880]	Time 0.123 (0.160)	Data 1.27e-04 (4.62e-04)	Tok/s 83265 (87216)	Loss/tok 2.9019 (3.0721)	LR 1.250e-04
0: TRAIN [4][1000/3880]	Time 0.067 (0.160)	Data 1.18e-04 (4.58e-04)	Tok/s 78428 (87208)	Loss/tok 2.4858 (3.0723)	LR 1.250e-04
0: TRAIN [4][1010/3880]	Time 0.124 (0.160)	Data 1.26e-04 (4.55e-04)	Tok/s 82433 (87235)	Loss/tok 2.7939 (3.0732)	LR 1.250e-04
0: TRAIN [4][1020/3880]	Time 0.127 (0.160)	Data 1.16e-04 (4.52e-04)	Tok/s 81618 (87229)	Loss/tok 2.9414 (3.0727)	LR 1.250e-04
0: TRAIN [4][1030/3880]	Time 0.243 (0.160)	Data 1.07e-04 (4.48e-04)	Tok/s 97582 (87214)	Loss/tok 3.1464 (3.0719)	LR 1.250e-04
0: TRAIN [4][1040/3880]	Time 0.123 (0.160)	Data 1.34e-04 (4.45e-04)	Tok/s 85159 (87249)	Loss/tok 2.8130 (3.0736)	LR 1.250e-04
0: TRAIN [4][1050/3880]	Time 0.126 (0.160)	Data 1.39e-04 (4.42e-04)	Tok/s 80474 (87236)	Loss/tok 2.9024 (3.0733)	LR 1.250e-04
0: TRAIN [4][1060/3880]	Time 0.183 (0.160)	Data 1.20e-04 (4.39e-04)	Tok/s 92157 (87227)	Loss/tok 3.1610 (3.0724)	LR 1.250e-04
0: TRAIN [4][1070/3880]	Time 0.183 (0.160)	Data 1.43e-04 (4.37e-04)	Tok/s 92047 (87223)	Loss/tok 3.0492 (3.0723)	LR 1.250e-04
0: TRAIN [4][1080/3880]	Time 0.245 (0.161)	Data 1.21e-04 (4.34e-04)	Tok/s 95077 (87266)	Loss/tok 3.2922 (3.0758)	LR 1.250e-04
0: TRAIN [4][1090/3880]	Time 0.126 (0.161)	Data 1.21e-04 (4.31e-04)	Tok/s 82381 (87274)	Loss/tok 2.8260 (3.0764)	LR 1.250e-04
0: TRAIN [4][1100/3880]	Time 0.066 (0.161)	Data 1.13e-04 (4.28e-04)	Tok/s 80421 (87252)	Loss/tok 2.5043 (3.0757)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [4][1110/3880]	Time 0.124 (0.161)	Data 1.34e-04 (4.25e-04)	Tok/s 83527 (87260)	Loss/tok 2.8770 (3.0752)	LR 1.250e-04
0: TRAIN [4][1120/3880]	Time 0.183 (0.161)	Data 1.30e-04 (4.23e-04)	Tok/s 90892 (87277)	Loss/tok 3.0500 (3.0755)	LR 1.250e-04
0: TRAIN [4][1130/3880]	Time 0.123 (0.161)	Data 1.13e-04 (4.20e-04)	Tok/s 83406 (87269)	Loss/tok 2.9774 (3.0753)	LR 1.250e-04
0: TRAIN [4][1140/3880]	Time 0.067 (0.160)	Data 1.24e-04 (4.17e-04)	Tok/s 79103 (87266)	Loss/tok 2.5439 (3.0746)	LR 1.250e-04
0: TRAIN [4][1150/3880]	Time 0.124 (0.161)	Data 1.31e-04 (4.15e-04)	Tok/s 83438 (87272)	Loss/tok 2.8036 (3.0761)	LR 1.250e-04
0: TRAIN [4][1160/3880]	Time 0.181 (0.161)	Data 1.27e-04 (4.12e-04)	Tok/s 91756 (87283)	Loss/tok 3.1662 (3.0763)	LR 1.250e-04
0: TRAIN [4][1170/3880]	Time 0.185 (0.161)	Data 1.13e-04 (4.10e-04)	Tok/s 90816 (87283)	Loss/tok 3.0506 (3.0767)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1180/3880]	Time 0.067 (0.161)	Data 1.10e-04 (4.07e-04)	Tok/s 77899 (87253)	Loss/tok 2.5631 (3.0765)	LR 1.250e-04
0: TRAIN [4][1190/3880]	Time 0.244 (0.161)	Data 1.13e-04 (4.05e-04)	Tok/s 95154 (87257)	Loss/tok 3.2732 (3.0765)	LR 1.250e-04
0: TRAIN [4][1200/3880]	Time 0.186 (0.161)	Data 1.39e-04 (4.03e-04)	Tok/s 89730 (87247)	Loss/tok 3.0943 (3.0762)	LR 1.250e-04
0: TRAIN [4][1210/3880]	Time 0.317 (0.161)	Data 1.11e-04 (4.00e-04)	Tok/s 94146 (87233)	Loss/tok 3.4050 (3.0762)	LR 1.250e-04
0: TRAIN [4][1220/3880]	Time 0.184 (0.161)	Data 1.27e-04 (3.98e-04)	Tok/s 91033 (87235)	Loss/tok 2.9954 (3.0764)	LR 1.250e-04
0: TRAIN [4][1230/3880]	Time 0.124 (0.161)	Data 1.31e-04 (3.96e-04)	Tok/s 83616 (87219)	Loss/tok 2.9752 (3.0758)	LR 1.250e-04
0: TRAIN [4][1240/3880]	Time 0.125 (0.160)	Data 1.27e-04 (3.93e-04)	Tok/s 83215 (87190)	Loss/tok 2.9010 (3.0754)	LR 1.250e-04
0: TRAIN [4][1250/3880]	Time 0.185 (0.160)	Data 1.17e-04 (3.91e-04)	Tok/s 91392 (87192)	Loss/tok 3.0821 (3.0753)	LR 1.250e-04
0: TRAIN [4][1260/3880]	Time 0.243 (0.160)	Data 1.29e-04 (3.89e-04)	Tok/s 95451 (87184)	Loss/tok 3.2937 (3.0752)	LR 1.250e-04
0: TRAIN [4][1270/3880]	Time 0.125 (0.161)	Data 1.19e-04 (3.87e-04)	Tok/s 82983 (87200)	Loss/tok 2.9289 (3.0758)	LR 1.250e-04
0: TRAIN [4][1280/3880]	Time 0.240 (0.161)	Data 1.32e-04 (3.85e-04)	Tok/s 98078 (87212)	Loss/tok 3.1889 (3.0761)	LR 1.250e-04
0: TRAIN [4][1290/3880]	Time 0.244 (0.161)	Data 1.45e-04 (3.83e-04)	Tok/s 95328 (87228)	Loss/tok 3.3053 (3.0769)	LR 1.250e-04
0: TRAIN [4][1300/3880]	Time 0.183 (0.161)	Data 1.39e-04 (3.81e-04)	Tok/s 90724 (87194)	Loss/tok 3.0738 (3.0759)	LR 1.250e-04
0: TRAIN [4][1310/3880]	Time 0.184 (0.160)	Data 1.15e-04 (3.79e-04)	Tok/s 91109 (87174)	Loss/tok 3.0754 (3.0753)	LR 1.250e-04
0: TRAIN [4][1320/3880]	Time 0.316 (0.161)	Data 1.25e-04 (3.77e-04)	Tok/s 94732 (87171)	Loss/tok 3.3565 (3.0763)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1330/3880]	Time 0.317 (0.161)	Data 1.49e-04 (3.75e-04)	Tok/s 94680 (87188)	Loss/tok 3.3628 (3.0765)	LR 1.250e-04
0: TRAIN [4][1340/3880]	Time 0.183 (0.161)	Data 1.48e-04 (3.73e-04)	Tok/s 91898 (87195)	Loss/tok 3.2171 (3.0770)	LR 1.250e-04
0: TRAIN [4][1350/3880]	Time 0.127 (0.161)	Data 1.26e-04 (3.72e-04)	Tok/s 82066 (87202)	Loss/tok 2.9983 (3.0771)	LR 1.250e-04
0: TRAIN [4][1360/3880]	Time 0.246 (0.161)	Data 1.31e-04 (3.70e-04)	Tok/s 95425 (87199)	Loss/tok 3.1071 (3.0768)	LR 1.250e-04
0: TRAIN [4][1370/3880]	Time 0.183 (0.161)	Data 1.23e-04 (3.68e-04)	Tok/s 91217 (87219)	Loss/tok 3.0154 (3.0772)	LR 1.250e-04
0: TRAIN [4][1380/3880]	Time 0.244 (0.161)	Data 1.23e-04 (3.66e-04)	Tok/s 96994 (87259)	Loss/tok 3.2081 (3.0780)	LR 1.250e-04
0: TRAIN [4][1390/3880]	Time 0.181 (0.161)	Data 1.16e-04 (3.65e-04)	Tok/s 93642 (87255)	Loss/tok 2.9993 (3.0779)	LR 1.250e-04
0: TRAIN [4][1400/3880]	Time 0.125 (0.161)	Data 1.13e-04 (3.63e-04)	Tok/s 82631 (87217)	Loss/tok 2.9109 (3.0768)	LR 1.250e-04
0: TRAIN [4][1410/3880]	Time 0.182 (0.161)	Data 1.27e-04 (3.61e-04)	Tok/s 92210 (87213)	Loss/tok 3.0630 (3.0763)	LR 1.250e-04
0: TRAIN [4][1420/3880]	Time 0.312 (0.161)	Data 1.33e-04 (3.60e-04)	Tok/s 95576 (87189)	Loss/tok 3.4574 (3.0759)	LR 1.250e-04
0: TRAIN [4][1430/3880]	Time 0.126 (0.161)	Data 1.40e-04 (3.58e-04)	Tok/s 83658 (87200)	Loss/tok 2.9182 (3.0765)	LR 1.250e-04
0: TRAIN [4][1440/3880]	Time 0.125 (0.161)	Data 1.18e-04 (3.56e-04)	Tok/s 82377 (87196)	Loss/tok 2.8606 (3.0765)	LR 1.250e-04
0: TRAIN [4][1450/3880]	Time 0.124 (0.161)	Data 1.14e-04 (3.55e-04)	Tok/s 82738 (87205)	Loss/tok 2.7825 (3.0770)	LR 1.250e-04
0: TRAIN [4][1460/3880]	Time 0.123 (0.161)	Data 1.21e-04 (3.53e-04)	Tok/s 82758 (87209)	Loss/tok 2.9579 (3.0770)	LR 1.250e-04
0: TRAIN [4][1470/3880]	Time 0.125 (0.161)	Data 1.10e-04 (3.52e-04)	Tok/s 83415 (87203)	Loss/tok 2.8523 (3.0764)	LR 1.250e-04
0: TRAIN [4][1480/3880]	Time 0.244 (0.161)	Data 1.21e-04 (3.50e-04)	Tok/s 95776 (87208)	Loss/tok 3.2030 (3.0764)	LR 1.250e-04
0: TRAIN [4][1490/3880]	Time 0.317 (0.161)	Data 1.05e-04 (3.48e-04)	Tok/s 93874 (87194)	Loss/tok 3.4288 (3.0763)	LR 1.250e-04
0: TRAIN [4][1500/3880]	Time 0.123 (0.161)	Data 1.32e-04 (3.47e-04)	Tok/s 85029 (87197)	Loss/tok 3.0165 (3.0761)	LR 1.250e-04
0: TRAIN [4][1510/3880]	Time 0.183 (0.161)	Data 1.21e-04 (3.45e-04)	Tok/s 91273 (87220)	Loss/tok 3.0781 (3.0766)	LR 1.250e-04
0: TRAIN [4][1520/3880]	Time 0.123 (0.161)	Data 1.33e-04 (3.44e-04)	Tok/s 83776 (87236)	Loss/tok 2.9570 (3.0765)	LR 1.250e-04
0: TRAIN [4][1530/3880]	Time 0.317 (0.161)	Data 1.35e-04 (3.43e-04)	Tok/s 92920 (87243)	Loss/tok 3.4377 (3.0769)	LR 1.250e-04
0: TRAIN [4][1540/3880]	Time 0.185 (0.161)	Data 1.23e-04 (3.41e-04)	Tok/s 91023 (87246)	Loss/tok 3.0420 (3.0767)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1550/3880]	Time 0.183 (0.161)	Data 1.13e-04 (3.40e-04)	Tok/s 92177 (87241)	Loss/tok 3.1224 (3.0762)	LR 1.250e-04
0: TRAIN [4][1560/3880]	Time 0.183 (0.161)	Data 1.34e-04 (3.38e-04)	Tok/s 92730 (87259)	Loss/tok 2.9864 (3.0764)	LR 1.250e-04
0: TRAIN [4][1570/3880]	Time 0.124 (0.161)	Data 1.30e-04 (3.37e-04)	Tok/s 82919 (87267)	Loss/tok 3.0191 (3.0764)	LR 1.250e-04
0: TRAIN [4][1580/3880]	Time 0.184 (0.161)	Data 1.23e-04 (3.36e-04)	Tok/s 89715 (87274)	Loss/tok 3.0850 (3.0763)	LR 1.250e-04
0: TRAIN [4][1590/3880]	Time 0.182 (0.161)	Data 1.26e-04 (3.34e-04)	Tok/s 91736 (87280)	Loss/tok 3.0542 (3.0764)	LR 1.250e-04
0: TRAIN [4][1600/3880]	Time 0.186 (0.161)	Data 1.16e-04 (3.33e-04)	Tok/s 90639 (87275)	Loss/tok 3.0983 (3.0762)	LR 1.250e-04
0: TRAIN [4][1610/3880]	Time 0.313 (0.161)	Data 1.13e-04 (3.32e-04)	Tok/s 94572 (87267)	Loss/tok 3.4733 (3.0767)	LR 1.250e-04
0: TRAIN [4][1620/3880]	Time 0.182 (0.161)	Data 1.08e-04 (3.30e-04)	Tok/s 91283 (87254)	Loss/tok 3.0727 (3.0761)	LR 1.250e-04
0: TRAIN [4][1630/3880]	Time 0.127 (0.161)	Data 1.34e-04 (3.29e-04)	Tok/s 81838 (87265)	Loss/tok 2.8901 (3.0766)	LR 1.250e-04
0: TRAIN [4][1640/3880]	Time 0.067 (0.161)	Data 1.08e-04 (3.28e-04)	Tok/s 78046 (87248)	Loss/tok 2.5026 (3.0768)	LR 1.250e-04
0: TRAIN [4][1650/3880]	Time 0.126 (0.161)	Data 1.25e-04 (3.26e-04)	Tok/s 80859 (87243)	Loss/tok 2.7370 (3.0764)	LR 1.250e-04
0: TRAIN [4][1660/3880]	Time 0.068 (0.161)	Data 1.13e-04 (3.25e-04)	Tok/s 78628 (87231)	Loss/tok 2.5083 (3.0763)	LR 1.250e-04
0: TRAIN [4][1670/3880]	Time 0.067 (0.161)	Data 1.10e-04 (3.24e-04)	Tok/s 78916 (87234)	Loss/tok 2.4958 (3.0764)	LR 1.250e-04
0: TRAIN [4][1680/3880]	Time 0.126 (0.161)	Data 1.10e-04 (3.23e-04)	Tok/s 82323 (87233)	Loss/tok 2.9933 (3.0769)	LR 1.250e-04
0: TRAIN [4][1690/3880]	Time 0.244 (0.161)	Data 1.20e-04 (3.21e-04)	Tok/s 96297 (87246)	Loss/tok 3.2123 (3.0780)	LR 1.250e-04
0: TRAIN [4][1700/3880]	Time 0.127 (0.161)	Data 1.28e-04 (3.20e-04)	Tok/s 81760 (87242)	Loss/tok 2.8541 (3.0778)	LR 1.250e-04
0: TRAIN [4][1710/3880]	Time 0.185 (0.161)	Data 1.13e-04 (3.19e-04)	Tok/s 92191 (87234)	Loss/tok 3.0491 (3.0775)	LR 1.250e-04
0: TRAIN [4][1720/3880]	Time 0.184 (0.161)	Data 1.41e-04 (3.18e-04)	Tok/s 90662 (87245)	Loss/tok 3.1835 (3.0775)	LR 1.250e-04
0: TRAIN [4][1730/3880]	Time 0.244 (0.161)	Data 1.23e-04 (3.17e-04)	Tok/s 96653 (87260)	Loss/tok 3.1854 (3.0778)	LR 1.250e-04
0: TRAIN [4][1740/3880]	Time 0.182 (0.161)	Data 1.20e-04 (3.16e-04)	Tok/s 90912 (87269)	Loss/tok 3.1082 (3.0781)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1750/3880]	Time 0.123 (0.161)	Data 1.30e-04 (3.15e-04)	Tok/s 85071 (87264)	Loss/tok 2.9439 (3.0782)	LR 1.250e-04
0: TRAIN [4][1760/3880]	Time 0.123 (0.161)	Data 1.24e-04 (3.14e-04)	Tok/s 85277 (87253)	Loss/tok 2.8607 (3.0776)	LR 1.250e-04
0: TRAIN [4][1770/3880]	Time 0.314 (0.161)	Data 1.26e-04 (3.13e-04)	Tok/s 94744 (87266)	Loss/tok 3.4320 (3.0789)	LR 1.250e-04
0: TRAIN [4][1780/3880]	Time 0.246 (0.162)	Data 1.35e-04 (3.11e-04)	Tok/s 95187 (87274)	Loss/tok 3.2250 (3.0792)	LR 1.250e-04
0: TRAIN [4][1790/3880]	Time 0.245 (0.162)	Data 1.17e-04 (3.10e-04)	Tok/s 94978 (87298)	Loss/tok 3.1783 (3.0798)	LR 1.250e-04
0: TRAIN [4][1800/3880]	Time 0.126 (0.162)	Data 1.16e-04 (3.09e-04)	Tok/s 81267 (87287)	Loss/tok 2.8892 (3.0792)	LR 1.250e-04
0: TRAIN [4][1810/3880]	Time 0.122 (0.162)	Data 1.20e-04 (3.08e-04)	Tok/s 84233 (87276)	Loss/tok 3.0120 (3.0789)	LR 1.250e-04
0: TRAIN [4][1820/3880]	Time 0.184 (0.162)	Data 1.20e-04 (3.07e-04)	Tok/s 90629 (87287)	Loss/tok 3.1671 (3.0792)	LR 1.250e-04
0: TRAIN [4][1830/3880]	Time 0.316 (0.162)	Data 1.22e-04 (3.06e-04)	Tok/s 94163 (87283)	Loss/tok 3.4087 (3.0796)	LR 1.250e-04
0: TRAIN [4][1840/3880]	Time 0.126 (0.162)	Data 1.27e-04 (3.05e-04)	Tok/s 81865 (87290)	Loss/tok 2.9942 (3.0796)	LR 1.250e-04
0: TRAIN [4][1850/3880]	Time 0.124 (0.162)	Data 1.36e-04 (3.05e-04)	Tok/s 84939 (87295)	Loss/tok 2.7732 (3.0798)	LR 1.250e-04
0: TRAIN [4][1860/3880]	Time 0.246 (0.162)	Data 1.51e-04 (3.04e-04)	Tok/s 94895 (87303)	Loss/tok 3.2828 (3.0801)	LR 1.250e-04
0: TRAIN [4][1870/3880]	Time 0.185 (0.162)	Data 1.38e-04 (3.03e-04)	Tok/s 92384 (87314)	Loss/tok 3.1183 (3.0802)	LR 1.250e-04
0: TRAIN [4][1880/3880]	Time 0.245 (0.162)	Data 1.21e-04 (3.02e-04)	Tok/s 94886 (87325)	Loss/tok 3.2733 (3.0805)	LR 1.250e-04
0: TRAIN [4][1890/3880]	Time 0.067 (0.162)	Data 1.12e-04 (3.01e-04)	Tok/s 80709 (87329)	Loss/tok 2.5303 (3.0811)	LR 1.250e-04
0: TRAIN [4][1900/3880]	Time 0.247 (0.162)	Data 1.49e-04 (3.00e-04)	Tok/s 94027 (87337)	Loss/tok 3.1606 (3.0813)	LR 1.250e-04
0: TRAIN [4][1910/3880]	Time 0.125 (0.162)	Data 2.24e-04 (2.99e-04)	Tok/s 84022 (87344)	Loss/tok 2.8730 (3.0819)	LR 1.250e-04
0: TRAIN [4][1920/3880]	Time 0.125 (0.162)	Data 1.35e-04 (2.98e-04)	Tok/s 84970 (87337)	Loss/tok 2.8250 (3.0815)	LR 1.250e-04
0: TRAIN [4][1930/3880]	Time 0.184 (0.162)	Data 1.30e-04 (2.97e-04)	Tok/s 90438 (87335)	Loss/tok 3.1775 (3.0815)	LR 1.250e-04
0: TRAIN [4][1940/3880]	Time 0.127 (0.163)	Data 1.23e-04 (2.96e-04)	Tok/s 80559 (87342)	Loss/tok 2.9515 (3.0821)	LR 1.250e-04
0: TRAIN [4][1950/3880]	Time 0.183 (0.163)	Data 1.27e-04 (2.95e-04)	Tok/s 91985 (87344)	Loss/tok 3.1224 (3.0822)	LR 1.250e-04
0: TRAIN [4][1960/3880]	Time 0.184 (0.162)	Data 1.28e-04 (2.95e-04)	Tok/s 91597 (87334)	Loss/tok 3.2232 (3.0820)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][1970/3880]	Time 0.315 (0.162)	Data 1.20e-04 (2.94e-04)	Tok/s 95075 (87322)	Loss/tok 3.3766 (3.0818)	LR 1.250e-04
0: TRAIN [4][1980/3880]	Time 0.125 (0.162)	Data 1.25e-04 (2.93e-04)	Tok/s 82277 (87329)	Loss/tok 2.8480 (3.0818)	LR 1.250e-04
0: TRAIN [4][1990/3880]	Time 0.246 (0.163)	Data 1.11e-04 (2.92e-04)	Tok/s 95078 (87358)	Loss/tok 3.2403 (3.0831)	LR 1.250e-04
0: TRAIN [4][2000/3880]	Time 0.247 (0.163)	Data 1.14e-04 (2.91e-04)	Tok/s 95187 (87355)	Loss/tok 3.1944 (3.0829)	LR 1.250e-04
0: TRAIN [4][2010/3880]	Time 0.068 (0.163)	Data 1.13e-04 (2.90e-04)	Tok/s 79509 (87355)	Loss/tok 2.5388 (3.0827)	LR 1.250e-04
0: TRAIN [4][2020/3880]	Time 0.125 (0.163)	Data 1.15e-04 (2.89e-04)	Tok/s 82486 (87342)	Loss/tok 2.9260 (3.0822)	LR 1.250e-04
0: TRAIN [4][2030/3880]	Time 0.124 (0.163)	Data 1.13e-04 (2.89e-04)	Tok/s 81941 (87342)	Loss/tok 2.9690 (3.0821)	LR 1.250e-04
0: TRAIN [4][2040/3880]	Time 0.244 (0.163)	Data 1.33e-04 (2.88e-04)	Tok/s 95420 (87352)	Loss/tok 3.3306 (3.0824)	LR 1.250e-04
0: TRAIN [4][2050/3880]	Time 0.181 (0.163)	Data 1.15e-04 (2.87e-04)	Tok/s 93442 (87350)	Loss/tok 3.0066 (3.0819)	LR 1.250e-04
0: TRAIN [4][2060/3880]	Time 0.125 (0.163)	Data 1.32e-04 (2.86e-04)	Tok/s 84964 (87349)	Loss/tok 2.8506 (3.0819)	LR 1.250e-04
0: TRAIN [4][2070/3880]	Time 0.244 (0.163)	Data 1.15e-04 (2.85e-04)	Tok/s 95148 (87347)	Loss/tok 3.2836 (3.0818)	LR 1.250e-04
0: TRAIN [4][2080/3880]	Time 0.066 (0.163)	Data 1.22e-04 (2.85e-04)	Tok/s 78832 (87333)	Loss/tok 2.5168 (3.0818)	LR 1.250e-04
0: TRAIN [4][2090/3880]	Time 0.126 (0.163)	Data 1.36e-04 (2.84e-04)	Tok/s 81626 (87342)	Loss/tok 2.8722 (3.0823)	LR 1.250e-04
0: TRAIN [4][2100/3880]	Time 0.126 (0.163)	Data 1.33e-04 (2.83e-04)	Tok/s 81952 (87335)	Loss/tok 2.9690 (3.0819)	LR 1.250e-04
0: TRAIN [4][2110/3880]	Time 0.185 (0.163)	Data 1.43e-04 (2.82e-04)	Tok/s 91136 (87338)	Loss/tok 3.1459 (3.0821)	LR 1.250e-04
0: TRAIN [4][2120/3880]	Time 0.181 (0.163)	Data 1.21e-04 (2.82e-04)	Tok/s 93973 (87341)	Loss/tok 3.0497 (3.0827)	LR 1.250e-04
0: TRAIN [4][2130/3880]	Time 0.182 (0.163)	Data 1.35e-04 (2.81e-04)	Tok/s 92324 (87342)	Loss/tok 3.0882 (3.0824)	LR 1.250e-04
0: TRAIN [4][2140/3880]	Time 0.067 (0.163)	Data 1.42e-04 (2.80e-04)	Tok/s 77297 (87331)	Loss/tok 2.4698 (3.0821)	LR 1.250e-04
0: TRAIN [4][2150/3880]	Time 0.123 (0.163)	Data 1.13e-04 (2.79e-04)	Tok/s 83970 (87331)	Loss/tok 2.9122 (3.0820)	LR 1.250e-04
0: TRAIN [4][2160/3880]	Time 0.243 (0.163)	Data 1.28e-04 (2.79e-04)	Tok/s 93917 (87345)	Loss/tok 3.3402 (3.0825)	LR 1.250e-04
0: TRAIN [4][2170/3880]	Time 0.127 (0.163)	Data 1.35e-04 (2.78e-04)	Tok/s 80821 (87332)	Loss/tok 2.9762 (3.0823)	LR 1.250e-04
0: TRAIN [4][2180/3880]	Time 0.241 (0.163)	Data 1.14e-04 (2.77e-04)	Tok/s 97929 (87338)	Loss/tok 3.1920 (3.0823)	LR 1.250e-04
0: TRAIN [4][2190/3880]	Time 0.125 (0.163)	Data 1.10e-04 (2.77e-04)	Tok/s 81404 (87349)	Loss/tok 2.7579 (3.0831)	LR 1.250e-04
0: TRAIN [4][2200/3880]	Time 0.124 (0.163)	Data 1.24e-04 (2.76e-04)	Tok/s 83145 (87342)	Loss/tok 2.9688 (3.0830)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2210/3880]	Time 0.126 (0.163)	Data 1.19e-04 (2.75e-04)	Tok/s 81169 (87343)	Loss/tok 2.9615 (3.0831)	LR 1.250e-04
0: TRAIN [4][2220/3880]	Time 0.185 (0.163)	Data 1.16e-04 (2.74e-04)	Tok/s 91849 (87334)	Loss/tok 3.1052 (3.0828)	LR 1.250e-04
0: TRAIN [4][2230/3880]	Time 0.125 (0.163)	Data 1.33e-04 (2.74e-04)	Tok/s 81079 (87328)	Loss/tok 2.8777 (3.0827)	LR 1.250e-04
0: TRAIN [4][2240/3880]	Time 0.067 (0.163)	Data 1.10e-04 (2.73e-04)	Tok/s 79369 (87322)	Loss/tok 2.5795 (3.0830)	LR 1.250e-04
0: TRAIN [4][2250/3880]	Time 0.125 (0.163)	Data 1.27e-04 (2.72e-04)	Tok/s 82165 (87307)	Loss/tok 2.9119 (3.0827)	LR 1.250e-04
0: TRAIN [4][2260/3880]	Time 0.128 (0.163)	Data 1.46e-04 (2.72e-04)	Tok/s 81483 (87319)	Loss/tok 2.8160 (3.0831)	LR 1.250e-04
0: TRAIN [4][2270/3880]	Time 0.245 (0.163)	Data 1.25e-04 (2.71e-04)	Tok/s 95702 (87324)	Loss/tok 3.2391 (3.0835)	LR 1.250e-04
0: TRAIN [4][2280/3880]	Time 0.125 (0.163)	Data 1.17e-04 (2.71e-04)	Tok/s 82490 (87324)	Loss/tok 2.8434 (3.0840)	LR 1.250e-04
0: TRAIN [4][2290/3880]	Time 0.128 (0.163)	Data 2.07e-04 (2.70e-04)	Tok/s 80239 (87316)	Loss/tok 2.8698 (3.0838)	LR 1.250e-04
0: TRAIN [4][2300/3880]	Time 0.068 (0.163)	Data 1.24e-04 (2.69e-04)	Tok/s 75548 (87309)	Loss/tok 2.4413 (3.0840)	LR 1.250e-04
0: TRAIN [4][2310/3880]	Time 0.245 (0.163)	Data 1.25e-04 (2.69e-04)	Tok/s 94655 (87316)	Loss/tok 3.2630 (3.0850)	LR 1.250e-04
0: TRAIN [4][2320/3880]	Time 0.125 (0.163)	Data 1.40e-04 (2.68e-04)	Tok/s 80715 (87323)	Loss/tok 2.9091 (3.0855)	LR 1.250e-04
0: TRAIN [4][2330/3880]	Time 0.126 (0.163)	Data 1.15e-04 (2.68e-04)	Tok/s 81762 (87320)	Loss/tok 2.8983 (3.0853)	LR 1.250e-04
0: TRAIN [4][2340/3880]	Time 0.126 (0.163)	Data 1.30e-04 (2.67e-04)	Tok/s 80887 (87333)	Loss/tok 2.9960 (3.0858)	LR 1.250e-04
0: TRAIN [4][2350/3880]	Time 0.185 (0.163)	Data 1.18e-04 (2.66e-04)	Tok/s 90222 (87327)	Loss/tok 3.1213 (3.0858)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2360/3880]	Time 0.315 (0.163)	Data 1.31e-04 (2.66e-04)	Tok/s 94143 (87333)	Loss/tok 3.4541 (3.0861)	LR 1.250e-04
0: TRAIN [4][2370/3880]	Time 0.125 (0.163)	Data 1.20e-04 (2.65e-04)	Tok/s 83909 (87335)	Loss/tok 2.9012 (3.0862)	LR 1.250e-04
0: TRAIN [4][2380/3880]	Time 0.181 (0.163)	Data 1.37e-04 (2.64e-04)	Tok/s 93614 (87326)	Loss/tok 3.1085 (3.0858)	LR 1.250e-04
0: TRAIN [4][2390/3880]	Time 0.246 (0.163)	Data 1.33e-04 (2.64e-04)	Tok/s 94404 (87332)	Loss/tok 3.2524 (3.0861)	LR 1.250e-04
0: TRAIN [4][2400/3880]	Time 0.184 (0.163)	Data 1.27e-04 (2.63e-04)	Tok/s 92056 (87323)	Loss/tok 3.0825 (3.0858)	LR 1.250e-04
0: TRAIN [4][2410/3880]	Time 0.123 (0.163)	Data 1.24e-04 (2.63e-04)	Tok/s 82626 (87316)	Loss/tok 2.9369 (3.0854)	LR 1.250e-04
0: TRAIN [4][2420/3880]	Time 0.244 (0.163)	Data 1.37e-04 (2.62e-04)	Tok/s 95702 (87331)	Loss/tok 3.1533 (3.0861)	LR 1.250e-04
0: TRAIN [4][2430/3880]	Time 0.068 (0.163)	Data 1.34e-04 (2.62e-04)	Tok/s 76475 (87331)	Loss/tok 2.5974 (3.0860)	LR 1.250e-04
0: TRAIN [4][2440/3880]	Time 0.314 (0.163)	Data 1.37e-04 (2.61e-04)	Tok/s 94189 (87335)	Loss/tok 3.3938 (3.0861)	LR 1.250e-04
0: TRAIN [4][2450/3880]	Time 0.066 (0.163)	Data 1.11e-04 (2.61e-04)	Tok/s 79368 (87326)	Loss/tok 2.5581 (3.0862)	LR 1.250e-04
0: TRAIN [4][2460/3880]	Time 0.126 (0.163)	Data 1.15e-04 (2.60e-04)	Tok/s 82616 (87333)	Loss/tok 2.9136 (3.0861)	LR 1.250e-04
0: TRAIN [4][2470/3880]	Time 0.184 (0.163)	Data 1.24e-04 (2.59e-04)	Tok/s 89940 (87336)	Loss/tok 3.0312 (3.0862)	LR 1.250e-04
0: TRAIN [4][2480/3880]	Time 0.128 (0.164)	Data 1.15e-04 (2.59e-04)	Tok/s 79762 (87337)	Loss/tok 2.9438 (3.0868)	LR 1.250e-04
0: TRAIN [4][2490/3880]	Time 0.182 (0.164)	Data 1.25e-04 (2.58e-04)	Tok/s 92602 (87334)	Loss/tok 3.1161 (3.0867)	LR 1.250e-04
0: TRAIN [4][2500/3880]	Time 0.182 (0.163)	Data 1.25e-04 (2.58e-04)	Tok/s 92229 (87324)	Loss/tok 3.1094 (3.0864)	LR 1.250e-04
0: TRAIN [4][2510/3880]	Time 0.185 (0.163)	Data 1.40e-04 (2.57e-04)	Tok/s 90940 (87333)	Loss/tok 3.0086 (3.0866)	LR 1.250e-04
0: TRAIN [4][2520/3880]	Time 0.124 (0.163)	Data 1.38e-04 (2.57e-04)	Tok/s 82873 (87330)	Loss/tok 2.8613 (3.0868)	LR 1.250e-04
0: TRAIN [4][2530/3880]	Time 0.243 (0.164)	Data 1.38e-04 (2.56e-04)	Tok/s 94687 (87333)	Loss/tok 3.2583 (3.0867)	LR 1.250e-04
0: TRAIN [4][2540/3880]	Time 0.127 (0.163)	Data 1.55e-04 (2.56e-04)	Tok/s 80434 (87322)	Loss/tok 2.8297 (3.0867)	LR 1.250e-04
0: TRAIN [4][2550/3880]	Time 0.185 (0.164)	Data 1.17e-04 (2.55e-04)	Tok/s 89398 (87329)	Loss/tok 3.1338 (3.0868)	LR 1.250e-04
0: TRAIN [4][2560/3880]	Time 0.183 (0.164)	Data 1.46e-04 (2.55e-04)	Tok/s 90096 (87336)	Loss/tok 3.0906 (3.0867)	LR 1.250e-04
0: TRAIN [4][2570/3880]	Time 0.185 (0.164)	Data 1.23e-04 (2.54e-04)	Tok/s 91479 (87332)	Loss/tok 3.1677 (3.0868)	LR 1.250e-04
0: TRAIN [4][2580/3880]	Time 0.125 (0.164)	Data 1.39e-04 (2.54e-04)	Tok/s 82843 (87334)	Loss/tok 2.8365 (3.0869)	LR 1.250e-04
0: TRAIN [4][2590/3880]	Time 0.126 (0.164)	Data 1.21e-04 (2.53e-04)	Tok/s 79741 (87337)	Loss/tok 2.9289 (3.0870)	LR 1.250e-04
0: TRAIN [4][2600/3880]	Time 0.067 (0.164)	Data 1.40e-04 (2.53e-04)	Tok/s 79848 (87332)	Loss/tok 2.5358 (3.0868)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2610/3880]	Time 0.124 (0.164)	Data 1.22e-04 (2.52e-04)	Tok/s 82108 (87325)	Loss/tok 2.8938 (3.0870)	LR 1.250e-04
0: TRAIN [4][2620/3880]	Time 0.183 (0.164)	Data 1.36e-04 (2.52e-04)	Tok/s 90206 (87324)	Loss/tok 3.1265 (3.0871)	LR 1.250e-04
0: TRAIN [4][2630/3880]	Time 0.184 (0.164)	Data 1.39e-04 (2.52e-04)	Tok/s 91024 (87321)	Loss/tok 3.1218 (3.0869)	LR 1.250e-04
0: TRAIN [4][2640/3880]	Time 0.184 (0.164)	Data 1.22e-04 (2.51e-04)	Tok/s 91930 (87318)	Loss/tok 2.9684 (3.0869)	LR 1.250e-04
0: TRAIN [4][2650/3880]	Time 0.125 (0.164)	Data 1.49e-04 (2.51e-04)	Tok/s 81859 (87325)	Loss/tok 2.8517 (3.0872)	LR 1.250e-04
0: TRAIN [4][2660/3880]	Time 0.127 (0.164)	Data 1.37e-04 (2.50e-04)	Tok/s 80331 (87318)	Loss/tok 2.8700 (3.0870)	LR 1.250e-04
0: TRAIN [4][2670/3880]	Time 0.125 (0.164)	Data 1.39e-04 (2.50e-04)	Tok/s 80723 (87313)	Loss/tok 2.9943 (3.0871)	LR 1.250e-04
0: TRAIN [4][2680/3880]	Time 0.124 (0.164)	Data 1.19e-04 (2.49e-04)	Tok/s 83498 (87303)	Loss/tok 2.9678 (3.0868)	LR 1.250e-04
0: TRAIN [4][2690/3880]	Time 0.182 (0.164)	Data 1.32e-04 (2.49e-04)	Tok/s 93341 (87312)	Loss/tok 2.9976 (3.0866)	LR 1.250e-04
0: TRAIN [4][2700/3880]	Time 0.243 (0.164)	Data 1.34e-04 (2.49e-04)	Tok/s 95072 (87322)	Loss/tok 3.1970 (3.0870)	LR 1.250e-04
0: TRAIN [4][2710/3880]	Time 0.185 (0.164)	Data 1.36e-04 (2.48e-04)	Tok/s 91315 (87319)	Loss/tok 2.9743 (3.0868)	LR 1.250e-04
0: TRAIN [4][2720/3880]	Time 0.245 (0.164)	Data 1.21e-04 (2.48e-04)	Tok/s 95648 (87320)	Loss/tok 3.1887 (3.0866)	LR 1.250e-04
0: TRAIN [4][2730/3880]	Time 0.184 (0.164)	Data 1.34e-04 (2.47e-04)	Tok/s 91617 (87320)	Loss/tok 2.9822 (3.0864)	LR 1.250e-04
0: TRAIN [4][2740/3880]	Time 0.124 (0.164)	Data 1.41e-04 (2.47e-04)	Tok/s 83561 (87316)	Loss/tok 2.8919 (3.0863)	LR 1.250e-04
0: TRAIN [4][2750/3880]	Time 0.124 (0.163)	Data 1.21e-04 (2.46e-04)	Tok/s 84460 (87300)	Loss/tok 2.9246 (3.0858)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2760/3880]	Time 0.182 (0.163)	Data 1.39e-04 (2.46e-04)	Tok/s 91820 (87310)	Loss/tok 3.0937 (3.0861)	LR 1.250e-04
0: TRAIN [4][2770/3880]	Time 0.066 (0.163)	Data 1.31e-04 (2.46e-04)	Tok/s 78150 (87298)	Loss/tok 2.5158 (3.0860)	LR 1.250e-04
0: TRAIN [4][2780/3880]	Time 0.184 (0.163)	Data 1.20e-04 (2.45e-04)	Tok/s 91109 (87295)	Loss/tok 3.1438 (3.0858)	LR 1.250e-04
0: TRAIN [4][2790/3880]	Time 0.067 (0.163)	Data 1.44e-04 (2.45e-04)	Tok/s 79186 (87301)	Loss/tok 2.4536 (3.0858)	LR 1.250e-04
0: TRAIN [4][2800/3880]	Time 0.123 (0.163)	Data 1.29e-04 (2.44e-04)	Tok/s 83249 (87308)	Loss/tok 2.9388 (3.0858)	LR 1.250e-04
0: TRAIN [4][2810/3880]	Time 0.126 (0.163)	Data 1.31e-04 (2.44e-04)	Tok/s 79715 (87319)	Loss/tok 2.8230 (3.0863)	LR 1.250e-04
0: TRAIN [4][2820/3880]	Time 0.125 (0.163)	Data 1.21e-04 (2.43e-04)	Tok/s 81370 (87321)	Loss/tok 2.9295 (3.0865)	LR 1.250e-04
0: TRAIN [4][2830/3880]	Time 0.247 (0.163)	Data 1.34e-04 (2.43e-04)	Tok/s 93391 (87319)	Loss/tok 3.2511 (3.0864)	LR 1.250e-04
0: TRAIN [4][2840/3880]	Time 0.125 (0.163)	Data 1.24e-04 (2.43e-04)	Tok/s 84294 (87303)	Loss/tok 2.8376 (3.0859)	LR 1.250e-04
0: TRAIN [4][2850/3880]	Time 0.126 (0.163)	Data 1.21e-04 (2.42e-04)	Tok/s 81834 (87312)	Loss/tok 2.8977 (3.0865)	LR 1.250e-04
0: TRAIN [4][2860/3880]	Time 0.184 (0.163)	Data 1.23e-04 (2.42e-04)	Tok/s 91406 (87319)	Loss/tok 3.0141 (3.0867)	LR 1.250e-04
0: TRAIN [4][2870/3880]	Time 0.124 (0.163)	Data 1.25e-04 (2.41e-04)	Tok/s 82985 (87316)	Loss/tok 2.8694 (3.0867)	LR 1.250e-04
0: TRAIN [4][2880/3880]	Time 0.241 (0.163)	Data 1.32e-04 (2.41e-04)	Tok/s 96646 (87314)	Loss/tok 3.2738 (3.0867)	LR 1.250e-04
0: TRAIN [4][2890/3880]	Time 0.126 (0.163)	Data 1.59e-04 (2.41e-04)	Tok/s 84243 (87321)	Loss/tok 2.9230 (3.0865)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][2900/3880]	Time 0.124 (0.164)	Data 1.20e-04 (2.40e-04)	Tok/s 85338 (87326)	Loss/tok 2.9313 (3.0869)	LR 1.250e-04
0: TRAIN [4][2910/3880]	Time 0.124 (0.163)	Data 1.18e-04 (2.40e-04)	Tok/s 82238 (87313)	Loss/tok 2.7979 (3.0869)	LR 1.250e-04
0: TRAIN [4][2920/3880]	Time 0.244 (0.163)	Data 1.41e-04 (2.39e-04)	Tok/s 95469 (87316)	Loss/tok 3.3102 (3.0867)	LR 1.250e-04
0: TRAIN [4][2930/3880]	Time 0.125 (0.163)	Data 1.23e-04 (2.39e-04)	Tok/s 82562 (87309)	Loss/tok 2.9767 (3.0866)	LR 1.250e-04
0: TRAIN [4][2940/3880]	Time 0.127 (0.163)	Data 1.40e-04 (2.39e-04)	Tok/s 81818 (87305)	Loss/tok 2.8292 (3.0865)	LR 1.250e-04
0: TRAIN [4][2950/3880]	Time 0.068 (0.163)	Data 1.32e-04 (2.38e-04)	Tok/s 77624 (87291)	Loss/tok 2.4825 (3.0864)	LR 1.250e-04
0: TRAIN [4][2960/3880]	Time 0.124 (0.163)	Data 1.26e-04 (2.38e-04)	Tok/s 84517 (87273)	Loss/tok 2.8748 (3.0859)	LR 1.250e-04
0: TRAIN [4][2970/3880]	Time 0.183 (0.163)	Data 1.17e-04 (2.38e-04)	Tok/s 91638 (87277)	Loss/tok 3.0877 (3.0858)	LR 1.250e-04
0: TRAIN [4][2980/3880]	Time 0.318 (0.163)	Data 1.29e-04 (2.37e-04)	Tok/s 94023 (87288)	Loss/tok 3.4621 (3.0860)	LR 1.250e-04
0: TRAIN [4][2990/3880]	Time 0.124 (0.163)	Data 1.36e-04 (2.37e-04)	Tok/s 82825 (87286)	Loss/tok 2.8688 (3.0859)	LR 1.250e-04
0: TRAIN [4][3000/3880]	Time 0.186 (0.163)	Data 1.24e-04 (2.36e-04)	Tok/s 90450 (87287)	Loss/tok 3.1469 (3.0859)	LR 1.250e-04
0: TRAIN [4][3010/3880]	Time 0.184 (0.163)	Data 1.11e-04 (2.36e-04)	Tok/s 91999 (87285)	Loss/tok 3.0873 (3.0857)	LR 1.250e-04
0: TRAIN [4][3020/3880]	Time 0.124 (0.163)	Data 1.20e-04 (2.36e-04)	Tok/s 81814 (87275)	Loss/tok 2.8246 (3.0855)	LR 1.250e-04
0: TRAIN [4][3030/3880]	Time 0.185 (0.163)	Data 1.51e-04 (2.35e-04)	Tok/s 90138 (87266)	Loss/tok 3.0629 (3.0852)	LR 1.250e-04
0: TRAIN [4][3040/3880]	Time 0.181 (0.163)	Data 1.36e-04 (2.35e-04)	Tok/s 92069 (87262)	Loss/tok 3.1407 (3.0849)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3050/3880]	Time 0.124 (0.163)	Data 1.17e-04 (2.35e-04)	Tok/s 82082 (87259)	Loss/tok 3.0124 (3.0848)	LR 1.250e-04
0: TRAIN [4][3060/3880]	Time 0.243 (0.163)	Data 1.15e-04 (2.34e-04)	Tok/s 96260 (87264)	Loss/tok 3.2609 (3.0852)	LR 1.250e-04
0: TRAIN [4][3070/3880]	Time 0.186 (0.163)	Data 1.29e-04 (2.34e-04)	Tok/s 90622 (87257)	Loss/tok 3.1478 (3.0849)	LR 1.250e-04
0: TRAIN [4][3080/3880]	Time 0.187 (0.163)	Data 1.28e-04 (2.33e-04)	Tok/s 91143 (87256)	Loss/tok 3.0317 (3.0849)	LR 1.250e-04
0: TRAIN [4][3090/3880]	Time 0.125 (0.163)	Data 1.27e-04 (2.33e-04)	Tok/s 82633 (87261)	Loss/tok 3.0851 (3.0852)	LR 1.250e-04
0: TRAIN [4][3100/3880]	Time 0.125 (0.163)	Data 1.19e-04 (2.33e-04)	Tok/s 82368 (87264)	Loss/tok 2.8565 (3.0852)	LR 1.250e-04
0: TRAIN [4][3110/3880]	Time 0.316 (0.163)	Data 1.27e-04 (2.32e-04)	Tok/s 94939 (87268)	Loss/tok 3.3731 (3.0853)	LR 1.250e-04
0: TRAIN [4][3120/3880]	Time 0.182 (0.163)	Data 1.16e-04 (2.32e-04)	Tok/s 92276 (87270)	Loss/tok 3.0460 (3.0850)	LR 1.250e-04
0: TRAIN [4][3130/3880]	Time 0.181 (0.163)	Data 1.39e-04 (2.32e-04)	Tok/s 94438 (87267)	Loss/tok 3.1019 (3.0848)	LR 1.250e-04
0: TRAIN [4][3140/3880]	Time 0.183 (0.163)	Data 1.28e-04 (2.31e-04)	Tok/s 91547 (87261)	Loss/tok 3.0806 (3.0847)	LR 1.250e-04
0: TRAIN [4][3150/3880]	Time 0.124 (0.163)	Data 1.19e-04 (2.31e-04)	Tok/s 84714 (87259)	Loss/tok 2.9681 (3.0845)	LR 1.250e-04
0: TRAIN [4][3160/3880]	Time 0.319 (0.163)	Data 1.27e-04 (2.31e-04)	Tok/s 91786 (87259)	Loss/tok 3.5582 (3.0846)	LR 1.250e-04
0: TRAIN [4][3170/3880]	Time 0.182 (0.163)	Data 1.31e-04 (2.30e-04)	Tok/s 93404 (87261)	Loss/tok 3.0345 (3.0844)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3180/3880]	Time 0.126 (0.163)	Data 1.34e-04 (2.30e-04)	Tok/s 80673 (87264)	Loss/tok 2.9235 (3.0844)	LR 1.250e-04
0: TRAIN [4][3190/3880]	Time 0.183 (0.163)	Data 1.42e-04 (2.30e-04)	Tok/s 92246 (87260)	Loss/tok 3.0105 (3.0843)	LR 1.250e-04
0: TRAIN [4][3200/3880]	Time 0.125 (0.163)	Data 1.36e-04 (2.29e-04)	Tok/s 82973 (87262)	Loss/tok 2.9261 (3.0842)	LR 1.250e-04
0: TRAIN [4][3210/3880]	Time 0.124 (0.163)	Data 1.20e-04 (2.29e-04)	Tok/s 83326 (87261)	Loss/tok 2.9483 (3.0842)	LR 1.250e-04
0: TRAIN [4][3220/3880]	Time 0.184 (0.163)	Data 1.18e-04 (2.29e-04)	Tok/s 92199 (87263)	Loss/tok 3.1210 (3.0841)	LR 1.250e-04
0: TRAIN [4][3230/3880]	Time 0.126 (0.163)	Data 1.38e-04 (2.29e-04)	Tok/s 82466 (87256)	Loss/tok 2.9001 (3.0842)	LR 1.250e-04
0: TRAIN [4][3240/3880]	Time 0.126 (0.163)	Data 1.17e-04 (2.28e-04)	Tok/s 79679 (87261)	Loss/tok 2.9317 (3.0842)	LR 1.250e-04
0: TRAIN [4][3250/3880]	Time 0.317 (0.163)	Data 1.25e-04 (2.28e-04)	Tok/s 93206 (87261)	Loss/tok 3.5449 (3.0847)	LR 1.250e-04
0: TRAIN [4][3260/3880]	Time 0.183 (0.163)	Data 1.35e-04 (2.28e-04)	Tok/s 91817 (87272)	Loss/tok 2.9904 (3.0848)	LR 1.250e-04
0: TRAIN [4][3270/3880]	Time 0.319 (0.163)	Data 1.26e-04 (2.27e-04)	Tok/s 94287 (87270)	Loss/tok 3.4371 (3.0849)	LR 1.250e-04
0: TRAIN [4][3280/3880]	Time 0.244 (0.163)	Data 1.25e-04 (2.27e-04)	Tok/s 96409 (87277)	Loss/tok 3.1617 (3.0847)	LR 1.250e-04
0: TRAIN [4][3290/3880]	Time 0.123 (0.163)	Data 1.13e-04 (2.27e-04)	Tok/s 83386 (87274)	Loss/tok 2.9382 (3.0844)	LR 1.250e-04
0: TRAIN [4][3300/3880]	Time 0.125 (0.163)	Data 1.43e-04 (2.26e-04)	Tok/s 81777 (87281)	Loss/tok 2.8816 (3.0846)	LR 1.250e-04
0: TRAIN [4][3310/3880]	Time 0.123 (0.163)	Data 1.33e-04 (2.26e-04)	Tok/s 81233 (87281)	Loss/tok 2.8946 (3.0844)	LR 1.250e-04
0: TRAIN [4][3320/3880]	Time 0.184 (0.163)	Data 1.47e-04 (2.26e-04)	Tok/s 91898 (87281)	Loss/tok 3.0785 (3.0844)	LR 1.250e-04
0: TRAIN [4][3330/3880]	Time 0.126 (0.163)	Data 1.18e-04 (2.25e-04)	Tok/s 80620 (87279)	Loss/tok 2.9445 (3.0843)	LR 1.250e-04
0: TRAIN [4][3340/3880]	Time 0.124 (0.163)	Data 1.14e-04 (2.25e-04)	Tok/s 83361 (87276)	Loss/tok 3.0262 (3.0843)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3350/3880]	Time 0.242 (0.163)	Data 1.37e-04 (2.25e-04)	Tok/s 97113 (87282)	Loss/tok 3.2721 (3.0846)	LR 1.250e-04
0: TRAIN [4][3360/3880]	Time 0.184 (0.163)	Data 1.30e-04 (2.25e-04)	Tok/s 90648 (87281)	Loss/tok 3.0412 (3.0845)	LR 1.250e-04
0: TRAIN [4][3370/3880]	Time 0.319 (0.163)	Data 1.26e-04 (2.24e-04)	Tok/s 94673 (87278)	Loss/tok 3.4195 (3.0846)	LR 1.250e-04
0: TRAIN [4][3380/3880]	Time 0.067 (0.163)	Data 1.31e-04 (2.24e-04)	Tok/s 79843 (87276)	Loss/tok 2.5537 (3.0844)	LR 1.250e-04
0: TRAIN [4][3390/3880]	Time 0.183 (0.163)	Data 1.23e-04 (2.24e-04)	Tok/s 90849 (87276)	Loss/tok 3.1457 (3.0844)	LR 1.250e-04
0: TRAIN [4][3400/3880]	Time 0.186 (0.163)	Data 1.20e-04 (2.23e-04)	Tok/s 90221 (87280)	Loss/tok 3.1049 (3.0844)	LR 1.250e-04
0: TRAIN [4][3410/3880]	Time 0.243 (0.163)	Data 1.35e-04 (2.23e-04)	Tok/s 96186 (87292)	Loss/tok 3.2941 (3.0847)	LR 1.250e-04
0: TRAIN [4][3420/3880]	Time 0.067 (0.163)	Data 1.45e-04 (2.23e-04)	Tok/s 78323 (87292)	Loss/tok 2.4977 (3.0848)	LR 1.250e-04
0: TRAIN [4][3430/3880]	Time 0.318 (0.163)	Data 1.40e-04 (2.23e-04)	Tok/s 93000 (87303)	Loss/tok 3.3819 (3.0851)	LR 1.250e-04
0: TRAIN [4][3440/3880]	Time 0.245 (0.163)	Data 1.37e-04 (2.22e-04)	Tok/s 93840 (87306)	Loss/tok 3.3090 (3.0851)	LR 1.250e-04
0: TRAIN [4][3450/3880]	Time 0.124 (0.163)	Data 1.39e-04 (2.22e-04)	Tok/s 84202 (87306)	Loss/tok 2.9384 (3.0850)	LR 1.250e-04
0: TRAIN [4][3460/3880]	Time 0.243 (0.163)	Data 1.33e-04 (2.22e-04)	Tok/s 96015 (87305)	Loss/tok 3.1569 (3.0848)	LR 1.250e-04
0: TRAIN [4][3470/3880]	Time 0.185 (0.163)	Data 1.28e-04 (2.22e-04)	Tok/s 89888 (87308)	Loss/tok 3.0872 (3.0848)	LR 1.250e-04
0: TRAIN [4][3480/3880]	Time 0.125 (0.163)	Data 1.38e-04 (2.21e-04)	Tok/s 81572 (87314)	Loss/tok 2.8554 (3.0850)	LR 1.250e-04
0: TRAIN [4][3490/3880]	Time 0.125 (0.163)	Data 1.40e-04 (2.21e-04)	Tok/s 81275 (87305)	Loss/tok 2.8799 (3.0847)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3500/3880]	Time 0.184 (0.163)	Data 1.37e-04 (2.21e-04)	Tok/s 90587 (87304)	Loss/tok 3.1263 (3.0848)	LR 1.250e-04
0: TRAIN [4][3510/3880]	Time 0.128 (0.163)	Data 1.22e-04 (2.21e-04)	Tok/s 80240 (87304)	Loss/tok 2.9545 (3.0847)	LR 1.250e-04
0: TRAIN [4][3520/3880]	Time 0.126 (0.163)	Data 1.46e-04 (2.20e-04)	Tok/s 82407 (87297)	Loss/tok 2.8405 (3.0845)	LR 1.250e-04
0: TRAIN [4][3530/3880]	Time 0.244 (0.163)	Data 1.51e-04 (2.20e-04)	Tok/s 94481 (87306)	Loss/tok 3.3685 (3.0847)	LR 1.250e-04
0: TRAIN [4][3540/3880]	Time 0.124 (0.163)	Data 1.54e-04 (2.20e-04)	Tok/s 81905 (87309)	Loss/tok 2.8639 (3.0848)	LR 1.250e-04
0: TRAIN [4][3550/3880]	Time 0.128 (0.163)	Data 1.42e-04 (2.20e-04)	Tok/s 78285 (87310)	Loss/tok 2.8606 (3.0847)	LR 1.250e-04
0: TRAIN [4][3560/3880]	Time 0.125 (0.163)	Data 1.19e-04 (2.19e-04)	Tok/s 82105 (87306)	Loss/tok 2.8947 (3.0845)	LR 1.250e-04
0: TRAIN [4][3570/3880]	Time 0.067 (0.163)	Data 1.39e-04 (2.19e-04)	Tok/s 78141 (87297)	Loss/tok 2.4709 (3.0843)	LR 1.250e-04
0: TRAIN [4][3580/3880]	Time 0.185 (0.163)	Data 1.44e-04 (2.19e-04)	Tok/s 91971 (87296)	Loss/tok 3.0571 (3.0842)	LR 1.250e-04
0: TRAIN [4][3590/3880]	Time 0.185 (0.163)	Data 1.42e-04 (2.19e-04)	Tok/s 91332 (87294)	Loss/tok 3.1086 (3.0841)	LR 1.250e-04
0: TRAIN [4][3600/3880]	Time 0.246 (0.163)	Data 1.20e-04 (2.18e-04)	Tok/s 96270 (87303)	Loss/tok 3.2065 (3.0845)	LR 1.250e-04
0: TRAIN [4][3610/3880]	Time 0.183 (0.163)	Data 1.44e-04 (2.18e-04)	Tok/s 91836 (87307)	Loss/tok 3.0382 (3.0846)	LR 1.250e-04
0: TRAIN [4][3620/3880]	Time 0.184 (0.163)	Data 1.34e-04 (2.18e-04)	Tok/s 90422 (87315)	Loss/tok 3.1048 (3.0849)	LR 1.250e-04
0: TRAIN [4][3630/3880]	Time 0.127 (0.163)	Data 1.34e-04 (2.18e-04)	Tok/s 80530 (87315)	Loss/tok 2.9329 (3.0850)	LR 1.250e-04
0: TRAIN [4][3640/3880]	Time 0.124 (0.163)	Data 1.38e-04 (2.17e-04)	Tok/s 84336 (87314)	Loss/tok 2.9440 (3.0850)	LR 1.250e-04
0: TRAIN [4][3650/3880]	Time 0.186 (0.163)	Data 1.13e-04 (2.17e-04)	Tok/s 90890 (87318)	Loss/tok 3.0829 (3.0848)	LR 1.250e-04
0: TRAIN [4][3660/3880]	Time 0.068 (0.164)	Data 1.27e-04 (2.17e-04)	Tok/s 77771 (87320)	Loss/tok 2.5755 (3.0848)	LR 1.250e-04
0: TRAIN [4][3670/3880]	Time 0.243 (0.164)	Data 1.29e-04 (2.17e-04)	Tok/s 96377 (87324)	Loss/tok 3.2066 (3.0847)	LR 1.250e-04
0: TRAIN [4][3680/3880]	Time 0.183 (0.163)	Data 1.27e-04 (2.16e-04)	Tok/s 91541 (87314)	Loss/tok 3.1866 (3.0846)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [4][3690/3880]	Time 0.245 (0.164)	Data 1.33e-04 (2.16e-04)	Tok/s 94829 (87322)	Loss/tok 3.2509 (3.0849)	LR 1.250e-04
0: TRAIN [4][3700/3880]	Time 0.126 (0.164)	Data 1.33e-04 (2.16e-04)	Tok/s 80129 (87327)	Loss/tok 2.9136 (3.0854)	LR 1.250e-04
0: TRAIN [4][3710/3880]	Time 0.185 (0.164)	Data 1.30e-04 (2.16e-04)	Tok/s 90961 (87328)	Loss/tok 3.1104 (3.0854)	LR 1.250e-04
0: TRAIN [4][3720/3880]	Time 0.125 (0.164)	Data 1.13e-04 (2.15e-04)	Tok/s 81930 (87327)	Loss/tok 2.9956 (3.0854)	LR 1.250e-04
0: TRAIN [4][3730/3880]	Time 0.242 (0.164)	Data 1.37e-04 (2.15e-04)	Tok/s 97116 (87329)	Loss/tok 3.1209 (3.0854)	LR 1.250e-04
0: TRAIN [4][3740/3880]	Time 0.125 (0.164)	Data 1.31e-04 (2.15e-04)	Tok/s 80930 (87326)	Loss/tok 3.0047 (3.0856)	LR 1.250e-04
0: TRAIN [4][3750/3880]	Time 0.066 (0.164)	Data 1.28e-04 (2.15e-04)	Tok/s 80474 (87317)	Loss/tok 2.4636 (3.0855)	LR 1.250e-04
0: TRAIN [4][3760/3880]	Time 0.182 (0.164)	Data 1.20e-04 (2.14e-04)	Tok/s 93017 (87327)	Loss/tok 3.0754 (3.0857)	LR 1.250e-04
0: TRAIN [4][3770/3880]	Time 0.126 (0.164)	Data 1.12e-04 (2.14e-04)	Tok/s 81767 (87326)	Loss/tok 2.9219 (3.0855)	LR 1.250e-04
0: TRAIN [4][3780/3880]	Time 0.183 (0.164)	Data 1.32e-04 (2.14e-04)	Tok/s 92194 (87328)	Loss/tok 3.1757 (3.0854)	LR 1.250e-04
0: TRAIN [4][3790/3880]	Time 0.314 (0.164)	Data 1.14e-04 (2.14e-04)	Tok/s 95721 (87328)	Loss/tok 3.3293 (3.0854)	LR 1.250e-04
0: TRAIN [4][3800/3880]	Time 0.124 (0.164)	Data 1.14e-04 (2.13e-04)	Tok/s 83348 (87329)	Loss/tok 2.8442 (3.0853)	LR 1.250e-04
0: TRAIN [4][3810/3880]	Time 0.184 (0.164)	Data 1.11e-04 (2.13e-04)	Tok/s 92559 (87335)	Loss/tok 3.0626 (3.0853)	LR 1.250e-04
0: TRAIN [4][3820/3880]	Time 0.127 (0.164)	Data 1.34e-04 (2.13e-04)	Tok/s 81689 (87332)	Loss/tok 2.8114 (3.0853)	LR 1.250e-04
0: TRAIN [4][3830/3880]	Time 0.124 (0.164)	Data 1.31e-04 (2.13e-04)	Tok/s 80899 (87336)	Loss/tok 2.8843 (3.0852)	LR 1.250e-04
0: TRAIN [4][3840/3880]	Time 0.127 (0.164)	Data 1.23e-04 (2.13e-04)	Tok/s 82964 (87336)	Loss/tok 2.9258 (3.0855)	LR 1.250e-04
0: TRAIN [4][3850/3880]	Time 0.184 (0.164)	Data 1.36e-04 (2.12e-04)	Tok/s 89219 (87337)	Loss/tok 3.1887 (3.0855)	LR 1.250e-04
0: TRAIN [4][3860/3880]	Time 0.067 (0.164)	Data 1.44e-04 (2.12e-04)	Tok/s 79969 (87334)	Loss/tok 2.4375 (3.0854)	LR 1.250e-04
0: TRAIN [4][3870/3880]	Time 0.125 (0.164)	Data 1.41e-04 (2.12e-04)	Tok/s 84773 (87325)	Loss/tok 2.8667 (3.0851)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
:::MLL 1586326494.550 epoch_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 524}}
:::MLL 1586326494.550 eval_start: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [4][0/6]	Time 0.730 (0.730)	Decoder iters 149.0 (149.0)	Tok/s 22573 (22573)
0: Running moses detokenizer
0: BLEU(score=23.870955792720107, counts=[37090, 18515, 10508, 6225], totals=[65583, 62580, 59577, 56579], precisions=[56.5542899836848, 29.58612975391499, 17.637678970072344, 11.002315346683398], bp=1.0, sys_len=65583, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586326497.544 eval_accuracy: {"value": 23.87, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 535}}
:::MLL 1586326497.545 eval_stop: {"value": null, "metadata": {"epoch_num": 5, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 4	Training Loss: 3.0855	Test BLEU: 23.87
0: Performance: Epoch: 4	Training: 349213 Tok/s
0: Finished epoch 4
:::MLL 1586326497.545 block_stop: {"value": null, "metadata": {"first_epoch_num": 5, "file": "train.py", "lineno": 557}}
:::MLL 1586326497.546 block_start: {"value": null, "metadata": {"first_epoch_num": 6, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1586326497.546 epoch_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 514}}
0: Starting epoch 5
0: Executing preallocation
0: Sampler for epoch 5 uses seed 2956348183
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [5][0/3880]	Time 0.428 (0.428)	Data 3.25e-01 (3.25e-01)	Tok/s 12537 (12537)	Loss/tok 2.4811 (2.4811)	LR 1.250e-04
0: TRAIN [5][10/3880]	Time 0.125 (0.217)	Data 1.16e-04 (2.97e-02)	Tok/s 83909 (83843)	Loss/tok 2.8769 (3.1444)	LR 1.250e-04
0: TRAIN [5][20/3880]	Time 0.123 (0.184)	Data 1.52e-04 (1.56e-02)	Tok/s 85112 (85460)	Loss/tok 2.8433 (3.0928)	LR 1.250e-04
0: TRAIN [5][30/3880]	Time 0.125 (0.175)	Data 1.14e-04 (1.06e-02)	Tok/s 82808 (85939)	Loss/tok 2.8632 (3.0829)	LR 1.250e-04
0: TRAIN [5][40/3880]	Time 0.184 (0.170)	Data 1.46e-04 (8.06e-03)	Tok/s 91979 (85970)	Loss/tok 3.1051 (3.0794)	LR 1.250e-04
0: TRAIN [5][50/3880]	Time 0.123 (0.166)	Data 1.25e-04 (6.50e-03)	Tok/s 82909 (85958)	Loss/tok 2.7508 (3.0742)	LR 1.250e-04
0: TRAIN [5][60/3880]	Time 0.185 (0.172)	Data 1.25e-04 (5.46e-03)	Tok/s 92179 (86614)	Loss/tok 3.0650 (3.1001)	LR 1.250e-04
0: TRAIN [5][70/3880]	Time 0.243 (0.171)	Data 1.10e-04 (4.70e-03)	Tok/s 96593 (86975)	Loss/tok 3.1033 (3.0897)	LR 1.250e-04
0: TRAIN [5][80/3880]	Time 0.122 (0.169)	Data 1.14e-04 (4.14e-03)	Tok/s 84308 (87031)	Loss/tok 2.8652 (3.0805)	LR 1.250e-04
0: TRAIN [5][90/3880]	Time 0.065 (0.167)	Data 1.08e-04 (3.70e-03)	Tok/s 81897 (87071)	Loss/tok 2.5164 (3.0725)	LR 1.250e-04
0: TRAIN [5][100/3880]	Time 0.124 (0.163)	Data 1.14e-04 (3.34e-03)	Tok/s 83561 (86801)	Loss/tok 2.8601 (3.0597)	LR 1.250e-04
0: TRAIN [5][110/3880]	Time 0.123 (0.161)	Data 1.13e-04 (3.05e-03)	Tok/s 84304 (86777)	Loss/tok 2.8824 (3.0564)	LR 1.250e-04
0: TRAIN [5][120/3880]	Time 0.124 (0.164)	Data 1.16e-04 (2.81e-03)	Tok/s 82387 (87029)	Loss/tok 2.9612 (3.0703)	LR 1.250e-04
0: TRAIN [5][130/3880]	Time 0.183 (0.164)	Data 1.11e-04 (2.60e-03)	Tok/s 91267 (87115)	Loss/tok 3.0762 (3.0676)	LR 1.250e-04
0: TRAIN [5][140/3880]	Time 0.183 (0.165)	Data 1.14e-04 (2.43e-03)	Tok/s 92505 (87192)	Loss/tok 3.0866 (3.0767)	LR 1.250e-04
0: TRAIN [5][150/3880]	Time 0.184 (0.164)	Data 1.16e-04 (2.27e-03)	Tok/s 92478 (87218)	Loss/tok 3.0727 (3.0707)	LR 1.250e-04
0: TRAIN [5][160/3880]	Time 0.184 (0.163)	Data 1.11e-04 (2.14e-03)	Tok/s 91434 (87139)	Loss/tok 3.0421 (3.0656)	LR 1.250e-04
0: TRAIN [5][170/3880]	Time 0.124 (0.164)	Data 1.07e-04 (2.02e-03)	Tok/s 83331 (87251)	Loss/tok 2.9567 (3.0668)	LR 1.250e-04
0: TRAIN [5][180/3880]	Time 0.244 (0.163)	Data 1.16e-04 (1.92e-03)	Tok/s 94789 (87241)	Loss/tok 3.1899 (3.0643)	LR 1.250e-04
0: TRAIN [5][190/3880]	Time 0.183 (0.162)	Data 1.12e-04 (1.82e-03)	Tok/s 92281 (87196)	Loss/tok 3.1149 (3.0605)	LR 1.250e-04
0: TRAIN [5][200/3880]	Time 0.249 (0.162)	Data 1.17e-04 (1.74e-03)	Tok/s 93193 (87143)	Loss/tok 3.1297 (3.0606)	LR 1.250e-04
0: TRAIN [5][210/3880]	Time 0.125 (0.162)	Data 1.14e-04 (1.66e-03)	Tok/s 82213 (87171)	Loss/tok 2.7268 (3.0626)	LR 1.250e-04
0: TRAIN [5][220/3880]	Time 0.180 (0.163)	Data 1.07e-04 (1.59e-03)	Tok/s 92534 (87268)	Loss/tok 3.0921 (3.0614)	LR 1.250e-04
0: TRAIN [5][230/3880]	Time 0.123 (0.162)	Data 1.09e-04 (1.53e-03)	Tok/s 84469 (87274)	Loss/tok 2.8842 (3.0620)	LR 1.250e-04
0: TRAIN [5][240/3880]	Time 0.183 (0.162)	Data 1.13e-04 (1.47e-03)	Tok/s 91645 (87259)	Loss/tok 3.1346 (3.0625)	LR 1.250e-04
0: TRAIN [5][250/3880]	Time 0.241 (0.162)	Data 1.14e-04 (1.41e-03)	Tok/s 95743 (87254)	Loss/tok 3.3415 (3.0603)	LR 1.250e-04
0: TRAIN [5][260/3880]	Time 0.316 (0.163)	Data 1.13e-04 (1.36e-03)	Tok/s 94986 (87374)	Loss/tok 3.4248 (3.0626)	LR 1.250e-04
0: TRAIN [5][270/3880]	Time 0.123 (0.163)	Data 1.20e-04 (1.32e-03)	Tok/s 83478 (87392)	Loss/tok 2.9115 (3.0619)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][280/3880]	Time 0.126 (0.163)	Data 1.17e-04 (1.28e-03)	Tok/s 81940 (87404)	Loss/tok 2.9199 (3.0648)	LR 1.250e-04
0: TRAIN [5][290/3880]	Time 0.123 (0.162)	Data 1.16e-04 (1.24e-03)	Tok/s 83942 (87339)	Loss/tok 2.8348 (3.0631)	LR 1.250e-04
0: TRAIN [5][300/3880]	Time 0.245 (0.162)	Data 1.13e-04 (1.20e-03)	Tok/s 94485 (87322)	Loss/tok 3.2459 (3.0625)	LR 1.250e-04
0: TRAIN [5][310/3880]	Time 0.124 (0.162)	Data 1.13e-04 (1.16e-03)	Tok/s 82688 (87303)	Loss/tok 2.8412 (3.0599)	LR 1.250e-04
0: TRAIN [5][320/3880]	Time 0.123 (0.161)	Data 1.13e-04 (1.13e-03)	Tok/s 84758 (87231)	Loss/tok 2.8918 (3.0574)	LR 1.250e-04
0: TRAIN [5][330/3880]	Time 0.124 (0.160)	Data 1.14e-04 (1.10e-03)	Tok/s 83226 (87181)	Loss/tok 2.9070 (3.0549)	LR 1.250e-04
0: TRAIN [5][340/3880]	Time 0.124 (0.161)	Data 1.08e-04 (1.07e-03)	Tok/s 82147 (87252)	Loss/tok 2.9367 (3.0583)	LR 1.250e-04
0: TRAIN [5][350/3880]	Time 0.067 (0.160)	Data 1.45e-04 (1.05e-03)	Tok/s 79063 (87144)	Loss/tok 2.6439 (3.0561)	LR 1.250e-04
0: TRAIN [5][360/3880]	Time 0.181 (0.160)	Data 1.14e-04 (1.02e-03)	Tok/s 92905 (87165)	Loss/tok 3.0483 (3.0552)	LR 1.250e-04
0: TRAIN [5][370/3880]	Time 0.249 (0.161)	Data 1.26e-04 (9.95e-04)	Tok/s 93111 (87278)	Loss/tok 3.3908 (3.0607)	LR 1.250e-04
0: TRAIN [5][380/3880]	Time 0.068 (0.161)	Data 1.11e-04 (9.73e-04)	Tok/s 78170 (87186)	Loss/tok 2.5128 (3.0598)	LR 1.250e-04
0: TRAIN [5][390/3880]	Time 0.067 (0.160)	Data 1.19e-04 (9.51e-04)	Tok/s 78469 (87205)	Loss/tok 2.5692 (3.0593)	LR 1.250e-04
0: TRAIN [5][400/3880]	Time 0.125 (0.160)	Data 1.23e-04 (9.30e-04)	Tok/s 83538 (87175)	Loss/tok 2.9834 (3.0577)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][410/3880]	Time 0.067 (0.159)	Data 1.20e-04 (9.10e-04)	Tok/s 80453 (87118)	Loss/tok 2.5446 (3.0559)	LR 1.250e-04
0: TRAIN [5][420/3880]	Time 0.183 (0.159)	Data 1.42e-04 (8.92e-04)	Tok/s 91163 (87113)	Loss/tok 3.0863 (3.0554)	LR 1.250e-04
0: TRAIN [5][430/3880]	Time 0.068 (0.159)	Data 1.29e-04 (8.74e-04)	Tok/s 76919 (87106)	Loss/tok 2.5447 (3.0558)	LR 1.250e-04
0: TRAIN [5][440/3880]	Time 0.124 (0.159)	Data 1.34e-04 (8.57e-04)	Tok/s 83728 (87125)	Loss/tok 2.9141 (3.0550)	LR 1.250e-04
0: TRAIN [5][450/3880]	Time 0.124 (0.159)	Data 1.14e-04 (8.41e-04)	Tok/s 84298 (87102)	Loss/tok 2.8291 (3.0536)	LR 1.250e-04
0: TRAIN [5][460/3880]	Time 0.128 (0.159)	Data 1.34e-04 (8.25e-04)	Tok/s 80352 (87098)	Loss/tok 2.8835 (3.0528)	LR 1.250e-04
0: TRAIN [5][470/3880]	Time 0.126 (0.159)	Data 1.29e-04 (8.10e-04)	Tok/s 80664 (87145)	Loss/tok 2.9429 (3.0564)	LR 1.250e-04
0: TRAIN [5][480/3880]	Time 0.182 (0.160)	Data 1.39e-04 (7.96e-04)	Tok/s 92940 (87182)	Loss/tok 3.0627 (3.0569)	LR 1.250e-04
0: TRAIN [5][490/3880]	Time 0.124 (0.161)	Data 1.37e-04 (7.83e-04)	Tok/s 82912 (87229)	Loss/tok 2.9688 (3.0616)	LR 1.250e-04
0: TRAIN [5][500/3880]	Time 0.243 (0.161)	Data 1.30e-04 (7.70e-04)	Tok/s 95960 (87250)	Loss/tok 3.2142 (3.0631)	LR 1.250e-04
0: TRAIN [5][510/3880]	Time 0.066 (0.160)	Data 1.13e-04 (7.57e-04)	Tok/s 80874 (87203)	Loss/tok 2.4715 (3.0622)	LR 1.250e-04
0: TRAIN [5][520/3880]	Time 0.124 (0.161)	Data 1.18e-04 (7.45e-04)	Tok/s 83937 (87231)	Loss/tok 2.8530 (3.0634)	LR 1.250e-04
0: TRAIN [5][530/3880]	Time 0.124 (0.161)	Data 1.37e-04 (7.33e-04)	Tok/s 83978 (87235)	Loss/tok 2.8091 (3.0634)	LR 1.250e-04
0: TRAIN [5][540/3880]	Time 0.126 (0.162)	Data 1.14e-04 (7.22e-04)	Tok/s 81818 (87284)	Loss/tok 2.9439 (3.0649)	LR 1.250e-04
0: TRAIN [5][550/3880]	Time 0.124 (0.162)	Data 1.21e-04 (7.11e-04)	Tok/s 83343 (87260)	Loss/tok 2.7868 (3.0643)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][560/3880]	Time 0.316 (0.163)	Data 1.14e-04 (7.01e-04)	Tok/s 93224 (87315)	Loss/tok 3.4473 (3.0685)	LR 1.250e-04
0: TRAIN [5][570/3880]	Time 0.125 (0.162)	Data 1.27e-04 (6.91e-04)	Tok/s 80897 (87311)	Loss/tok 2.8683 (3.0677)	LR 1.250e-04
0: TRAIN [5][580/3880]	Time 0.128 (0.162)	Data 1.32e-04 (6.81e-04)	Tok/s 82391 (87327)	Loss/tok 2.8540 (3.0675)	LR 1.250e-04
0: TRAIN [5][590/3880]	Time 0.244 (0.163)	Data 1.01e-04 (6.72e-04)	Tok/s 96602 (87396)	Loss/tok 3.1953 (3.0701)	LR 1.250e-04
0: TRAIN [5][600/3880]	Time 0.183 (0.164)	Data 1.18e-04 (6.62e-04)	Tok/s 93273 (87443)	Loss/tok 3.0059 (3.0714)	LR 1.250e-04
0: TRAIN [5][610/3880]	Time 0.068 (0.164)	Data 1.07e-04 (6.53e-04)	Tok/s 79134 (87455)	Loss/tok 2.5162 (3.0726)	LR 1.250e-04
0: TRAIN [5][620/3880]	Time 0.244 (0.164)	Data 1.10e-04 (6.45e-04)	Tok/s 95247 (87454)	Loss/tok 3.2635 (3.0729)	LR 1.250e-04
0: TRAIN [5][630/3880]	Time 0.129 (0.165)	Data 1.40e-04 (6.36e-04)	Tok/s 80470 (87466)	Loss/tok 2.8820 (3.0757)	LR 1.250e-04
0: TRAIN [5][640/3880]	Time 0.316 (0.166)	Data 1.18e-04 (6.28e-04)	Tok/s 93832 (87526)	Loss/tok 3.4263 (3.0789)	LR 1.250e-04
0: TRAIN [5][650/3880]	Time 0.125 (0.166)	Data 1.14e-04 (6.20e-04)	Tok/s 82081 (87492)	Loss/tok 2.9409 (3.0791)	LR 1.250e-04
0: TRAIN [5][660/3880]	Time 0.123 (0.165)	Data 1.24e-04 (6.13e-04)	Tok/s 82449 (87462)	Loss/tok 2.8545 (3.0777)	LR 1.250e-04
0: TRAIN [5][670/3880]	Time 0.126 (0.166)	Data 1.18e-04 (6.05e-04)	Tok/s 80399 (87482)	Loss/tok 2.9222 (3.0794)	LR 1.250e-04
0: TRAIN [5][680/3880]	Time 0.243 (0.165)	Data 1.15e-04 (5.98e-04)	Tok/s 96162 (87453)	Loss/tok 3.2169 (3.0797)	LR 1.250e-04
0: TRAIN [5][690/3880]	Time 0.181 (0.165)	Data 1.03e-04 (5.91e-04)	Tok/s 93404 (87436)	Loss/tok 2.9214 (3.0778)	LR 1.250e-04
0: TRAIN [5][700/3880]	Time 0.317 (0.165)	Data 9.87e-05 (5.84e-04)	Tok/s 93129 (87432)	Loss/tok 3.4529 (3.0784)	LR 1.250e-04
0: TRAIN [5][710/3880]	Time 0.125 (0.165)	Data 1.08e-04 (5.77e-04)	Tok/s 81579 (87425)	Loss/tok 2.8378 (3.0772)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][720/3880]	Time 0.318 (0.165)	Data 1.49e-04 (5.71e-04)	Tok/s 94104 (87446)	Loss/tok 3.3433 (3.0790)	LR 1.250e-04
0: TRAIN [5][730/3880]	Time 0.183 (0.165)	Data 1.09e-04 (5.65e-04)	Tok/s 92290 (87437)	Loss/tok 3.0514 (3.0781)	LR 1.250e-04
0: TRAIN [5][740/3880]	Time 0.126 (0.165)	Data 1.23e-04 (5.59e-04)	Tok/s 82995 (87467)	Loss/tok 2.8318 (3.0793)	LR 1.250e-04
0: TRAIN [5][750/3880]	Time 0.181 (0.166)	Data 1.08e-04 (5.53e-04)	Tok/s 92608 (87476)	Loss/tok 3.0302 (3.0798)	LR 1.250e-04
0: TRAIN [5][760/3880]	Time 0.069 (0.165)	Data 1.32e-04 (5.47e-04)	Tok/s 76665 (87454)	Loss/tok 2.4007 (3.0796)	LR 1.250e-04
0: TRAIN [5][770/3880]	Time 0.124 (0.165)	Data 1.12e-04 (5.41e-04)	Tok/s 83454 (87458)	Loss/tok 2.8857 (3.0785)	LR 1.250e-04
0: TRAIN [5][780/3880]	Time 0.125 (0.165)	Data 1.22e-04 (5.36e-04)	Tok/s 81533 (87430)	Loss/tok 2.9194 (3.0786)	LR 1.250e-04
0: TRAIN [5][790/3880]	Time 0.187 (0.165)	Data 1.02e-04 (5.30e-04)	Tok/s 89674 (87424)	Loss/tok 3.0855 (3.0776)	LR 1.250e-04
0: TRAIN [5][800/3880]	Time 0.123 (0.165)	Data 1.11e-04 (5.25e-04)	Tok/s 83293 (87433)	Loss/tok 3.0221 (3.0776)	LR 1.250e-04
0: TRAIN [5][810/3880]	Time 0.247 (0.165)	Data 1.14e-04 (5.20e-04)	Tok/s 94729 (87431)	Loss/tok 3.2651 (3.0775)	LR 1.250e-04
0: TRAIN [5][820/3880]	Time 0.181 (0.165)	Data 1.21e-04 (5.15e-04)	Tok/s 91667 (87436)	Loss/tok 3.0085 (3.0766)	LR 1.250e-04
0: TRAIN [5][830/3880]	Time 0.069 (0.165)	Data 1.01e-04 (5.11e-04)	Tok/s 76485 (87431)	Loss/tok 2.5596 (3.0773)	LR 1.250e-04
0: TRAIN [5][840/3880]	Time 0.181 (0.165)	Data 1.08e-04 (5.06e-04)	Tok/s 93054 (87416)	Loss/tok 2.9924 (3.0761)	LR 1.250e-04
0: TRAIN [5][850/3880]	Time 0.183 (0.165)	Data 1.12e-04 (5.01e-04)	Tok/s 93024 (87397)	Loss/tok 3.0791 (3.0757)	LR 1.250e-04
0: TRAIN [5][860/3880]	Time 0.185 (0.165)	Data 1.22e-04 (4.97e-04)	Tok/s 91778 (87410)	Loss/tok 3.0652 (3.0764)	LR 1.250e-04
0: TRAIN [5][870/3880]	Time 0.067 (0.165)	Data 1.23e-04 (4.92e-04)	Tok/s 77458 (87371)	Loss/tok 2.4588 (3.0757)	LR 1.250e-04
0: TRAIN [5][880/3880]	Time 0.242 (0.165)	Data 1.26e-04 (4.88e-04)	Tok/s 96849 (87381)	Loss/tok 3.3067 (3.0758)	LR 1.250e-04
0: TRAIN [5][890/3880]	Time 0.184 (0.165)	Data 1.15e-04 (4.84e-04)	Tok/s 90824 (87377)	Loss/tok 3.0107 (3.0753)	LR 1.250e-04
0: TRAIN [5][900/3880]	Time 0.183 (0.165)	Data 1.08e-04 (4.80e-04)	Tok/s 92658 (87388)	Loss/tok 3.1014 (3.0749)	LR 1.250e-04
0: TRAIN [5][910/3880]	Time 0.123 (0.164)	Data 1.31e-04 (4.76e-04)	Tok/s 84868 (87361)	Loss/tok 2.9048 (3.0738)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][920/3880]	Time 0.124 (0.165)	Data 1.20e-04 (4.72e-04)	Tok/s 82925 (87372)	Loss/tok 2.8429 (3.0739)	LR 1.250e-04
0: TRAIN [5][930/3880]	Time 0.181 (0.165)	Data 1.03e-04 (4.68e-04)	Tok/s 92696 (87382)	Loss/tok 3.0162 (3.0735)	LR 1.250e-04
0: TRAIN [5][940/3880]	Time 0.184 (0.165)	Data 1.04e-04 (4.64e-04)	Tok/s 91145 (87388)	Loss/tok 2.9787 (3.0726)	LR 1.250e-04
0: TRAIN [5][950/3880]	Time 0.181 (0.164)	Data 1.01e-04 (4.60e-04)	Tok/s 92761 (87371)	Loss/tok 3.0170 (3.0730)	LR 1.250e-04
0: TRAIN [5][960/3880]	Time 0.243 (0.164)	Data 1.09e-04 (4.57e-04)	Tok/s 96692 (87389)	Loss/tok 3.1946 (3.0723)	LR 1.250e-04
0: TRAIN [5][970/3880]	Time 0.186 (0.164)	Data 1.10e-04 (4.53e-04)	Tok/s 90992 (87380)	Loss/tok 3.1467 (3.0716)	LR 1.250e-04
0: TRAIN [5][980/3880]	Time 0.068 (0.164)	Data 1.32e-04 (4.50e-04)	Tok/s 75917 (87399)	Loss/tok 2.4647 (3.0712)	LR 1.250e-04
0: TRAIN [5][990/3880]	Time 0.183 (0.164)	Data 1.23e-04 (4.46e-04)	Tok/s 90479 (87388)	Loss/tok 3.0795 (3.0713)	LR 1.250e-04
0: TRAIN [5][1000/3880]	Time 0.122 (0.165)	Data 1.10e-04 (4.43e-04)	Tok/s 82542 (87409)	Loss/tok 2.8510 (3.0717)	LR 1.250e-04
0: TRAIN [5][1010/3880]	Time 0.124 (0.164)	Data 1.39e-04 (4.40e-04)	Tok/s 85021 (87399)	Loss/tok 2.8440 (3.0714)	LR 1.250e-04
0: TRAIN [5][1020/3880]	Time 0.125 (0.164)	Data 1.11e-04 (4.37e-04)	Tok/s 82545 (87402)	Loss/tok 2.9698 (3.0714)	LR 1.250e-04
0: TRAIN [5][1030/3880]	Time 0.066 (0.164)	Data 1.12e-04 (4.34e-04)	Tok/s 80503 (87392)	Loss/tok 2.6063 (3.0716)	LR 1.250e-04
0: TRAIN [5][1040/3880]	Time 0.183 (0.165)	Data 1.14e-04 (4.31e-04)	Tok/s 91768 (87409)	Loss/tok 3.1482 (3.0717)	LR 1.250e-04
0: TRAIN [5][1050/3880]	Time 0.122 (0.164)	Data 1.13e-04 (4.28e-04)	Tok/s 85031 (87386)	Loss/tok 2.9389 (3.0709)	LR 1.250e-04
0: TRAIN [5][1060/3880]	Time 0.123 (0.164)	Data 1.15e-04 (4.25e-04)	Tok/s 83537 (87384)	Loss/tok 2.6545 (3.0706)	LR 1.250e-04
0: TRAIN [5][1070/3880]	Time 0.182 (0.164)	Data 1.34e-04 (4.22e-04)	Tok/s 93421 (87379)	Loss/tok 3.0824 (3.0700)	LR 1.250e-04
0: TRAIN [5][1080/3880]	Time 0.187 (0.164)	Data 1.25e-04 (4.20e-04)	Tok/s 90854 (87386)	Loss/tok 3.0133 (3.0702)	LR 1.250e-04
0: TRAIN [5][1090/3880]	Time 0.123 (0.164)	Data 1.52e-04 (4.17e-04)	Tok/s 84083 (87364)	Loss/tok 2.8712 (3.0690)	LR 1.250e-04
0: TRAIN [5][1100/3880]	Time 0.244 (0.164)	Data 1.19e-04 (4.14e-04)	Tok/s 95664 (87370)	Loss/tok 3.2253 (3.0687)	LR 1.250e-04
0: TRAIN [5][1110/3880]	Time 0.243 (0.164)	Data 1.16e-04 (4.12e-04)	Tok/s 96166 (87388)	Loss/tok 3.2154 (3.0693)	LR 1.250e-04
0: TRAIN [5][1120/3880]	Time 0.125 (0.164)	Data 1.49e-04 (4.09e-04)	Tok/s 83231 (87373)	Loss/tok 2.9453 (3.0691)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][1130/3880]	Time 0.182 (0.164)	Data 1.14e-04 (4.07e-04)	Tok/s 92194 (87379)	Loss/tok 3.1403 (3.0695)	LR 1.250e-04
0: TRAIN [5][1140/3880]	Time 0.126 (0.164)	Data 1.24e-04 (4.04e-04)	Tok/s 81123 (87355)	Loss/tok 2.7765 (3.0694)	LR 1.250e-04
0: TRAIN [5][1150/3880]	Time 0.127 (0.164)	Data 1.25e-04 (4.02e-04)	Tok/s 82209 (87381)	Loss/tok 2.8180 (3.0702)	LR 1.250e-04
0: TRAIN [5][1160/3880]	Time 0.067 (0.164)	Data 1.22e-04 (4.00e-04)	Tok/s 77746 (87365)	Loss/tok 2.5088 (3.0696)	LR 1.250e-04
0: TRAIN [5][1170/3880]	Time 0.124 (0.164)	Data 1.51e-04 (3.97e-04)	Tok/s 82436 (87356)	Loss/tok 2.9248 (3.0695)	LR 1.250e-04
0: TRAIN [5][1180/3880]	Time 0.245 (0.164)	Data 1.36e-04 (3.95e-04)	Tok/s 94199 (87390)	Loss/tok 3.2617 (3.0702)	LR 1.250e-04
0: TRAIN [5][1190/3880]	Time 0.185 (0.164)	Data 1.27e-04 (3.93e-04)	Tok/s 90756 (87417)	Loss/tok 3.0284 (3.0708)	LR 1.250e-04
0: TRAIN [5][1200/3880]	Time 0.246 (0.164)	Data 1.22e-04 (3.91e-04)	Tok/s 95158 (87417)	Loss/tok 3.1665 (3.0705)	LR 1.250e-04
0: TRAIN [5][1210/3880]	Time 0.184 (0.164)	Data 1.26e-04 (3.88e-04)	Tok/s 89164 (87408)	Loss/tok 3.1416 (3.0701)	LR 1.250e-04
0: TRAIN [5][1220/3880]	Time 0.125 (0.164)	Data 1.33e-04 (3.86e-04)	Tok/s 83560 (87413)	Loss/tok 3.0117 (3.0707)	LR 1.250e-04
0: TRAIN [5][1230/3880]	Time 0.183 (0.164)	Data 1.22e-04 (3.84e-04)	Tok/s 91051 (87397)	Loss/tok 3.1125 (3.0700)	LR 1.250e-04
0: TRAIN [5][1240/3880]	Time 0.128 (0.165)	Data 1.42e-04 (3.82e-04)	Tok/s 81947 (87413)	Loss/tok 2.9051 (3.0709)	LR 1.250e-04
0: TRAIN [5][1250/3880]	Time 0.185 (0.165)	Data 1.42e-04 (3.80e-04)	Tok/s 89611 (87435)	Loss/tok 2.9741 (3.0715)	LR 1.250e-04
0: TRAIN [5][1260/3880]	Time 0.125 (0.165)	Data 1.30e-04 (3.78e-04)	Tok/s 82509 (87426)	Loss/tok 2.9011 (3.0712)	LR 1.250e-04
0: TRAIN [5][1270/3880]	Time 0.183 (0.165)	Data 1.40e-04 (3.76e-04)	Tok/s 91673 (87417)	Loss/tok 3.0459 (3.0708)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][1280/3880]	Time 0.244 (0.165)	Data 1.24e-04 (3.74e-04)	Tok/s 95810 (87416)	Loss/tok 3.2444 (3.0707)	LR 1.250e-04
0: TRAIN [5][1290/3880]	Time 0.125 (0.165)	Data 1.30e-04 (3.72e-04)	Tok/s 82816 (87412)	Loss/tok 2.8562 (3.0715)	LR 1.250e-04
0: TRAIN [5][1300/3880]	Time 0.244 (0.165)	Data 1.28e-04 (3.70e-04)	Tok/s 94961 (87426)	Loss/tok 3.2145 (3.0716)	LR 1.250e-04
0: TRAIN [5][1310/3880]	Time 0.246 (0.165)	Data 1.25e-04 (3.68e-04)	Tok/s 94543 (87415)	Loss/tok 3.3377 (3.0714)	LR 1.250e-04
0: TRAIN [5][1320/3880]	Time 0.125 (0.165)	Data 1.13e-04 (3.67e-04)	Tok/s 83036 (87409)	Loss/tok 2.9179 (3.0711)	LR 1.250e-04
0: TRAIN [5][1330/3880]	Time 0.124 (0.164)	Data 1.50e-04 (3.65e-04)	Tok/s 83977 (87407)	Loss/tok 2.8168 (3.0706)	LR 1.250e-04
0: TRAIN [5][1340/3880]	Time 0.126 (0.165)	Data 1.22e-04 (3.63e-04)	Tok/s 83102 (87417)	Loss/tok 2.9997 (3.0713)	LR 1.250e-04
0: TRAIN [5][1350/3880]	Time 0.124 (0.165)	Data 1.26e-04 (3.61e-04)	Tok/s 82353 (87405)	Loss/tok 2.8913 (3.0709)	LR 1.250e-04
0: TRAIN [5][1360/3880]	Time 0.184 (0.165)	Data 1.17e-04 (3.60e-04)	Tok/s 91256 (87418)	Loss/tok 3.0345 (3.0709)	LR 1.250e-04
0: TRAIN [5][1370/3880]	Time 0.124 (0.165)	Data 1.24e-04 (3.58e-04)	Tok/s 83358 (87412)	Loss/tok 2.9060 (3.0711)	LR 1.250e-04
0: TRAIN [5][1380/3880]	Time 0.183 (0.165)	Data 1.30e-04 (3.56e-04)	Tok/s 92400 (87398)	Loss/tok 3.0157 (3.0706)	LR 1.250e-04
0: TRAIN [5][1390/3880]	Time 0.184 (0.165)	Data 1.17e-04 (3.54e-04)	Tok/s 91419 (87416)	Loss/tok 3.1114 (3.0706)	LR 1.250e-04
0: TRAIN [5][1400/3880]	Time 0.182 (0.165)	Data 1.15e-04 (3.53e-04)	Tok/s 92299 (87409)	Loss/tok 3.1412 (3.0707)	LR 1.250e-04
0: TRAIN [5][1410/3880]	Time 0.124 (0.164)	Data 1.13e-04 (3.51e-04)	Tok/s 82424 (87390)	Loss/tok 2.7920 (3.0698)	LR 1.250e-04
0: TRAIN [5][1420/3880]	Time 0.185 (0.165)	Data 1.16e-04 (3.49e-04)	Tok/s 89187 (87398)	Loss/tok 3.0570 (3.0703)	LR 1.250e-04
0: TRAIN [5][1430/3880]	Time 0.123 (0.164)	Data 1.03e-04 (3.48e-04)	Tok/s 83948 (87383)	Loss/tok 2.7930 (3.0697)	LR 1.250e-04
0: TRAIN [5][1440/3880]	Time 0.243 (0.164)	Data 1.05e-04 (3.46e-04)	Tok/s 96629 (87393)	Loss/tok 3.2010 (3.0704)	LR 1.250e-04
0: TRAIN [5][1450/3880]	Time 0.123 (0.164)	Data 1.17e-04 (3.45e-04)	Tok/s 83452 (87366)	Loss/tok 2.8928 (3.0697)	LR 1.250e-04
0: TRAIN [5][1460/3880]	Time 0.123 (0.164)	Data 1.33e-04 (3.43e-04)	Tok/s 83987 (87358)	Loss/tok 2.8449 (3.0695)	LR 1.250e-04
0: TRAIN [5][1470/3880]	Time 0.069 (0.164)	Data 1.35e-04 (3.41e-04)	Tok/s 75908 (87351)	Loss/tok 2.4160 (3.0693)	LR 1.250e-04
0: TRAIN [5][1480/3880]	Time 0.124 (0.164)	Data 1.01e-04 (3.40e-04)	Tok/s 83573 (87352)	Loss/tok 2.8320 (3.0689)	LR 1.250e-04
0: TRAIN [5][1490/3880]	Time 0.244 (0.164)	Data 1.06e-04 (3.38e-04)	Tok/s 95901 (87349)	Loss/tok 3.2383 (3.0687)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][1500/3880]	Time 0.122 (0.164)	Data 1.13e-04 (3.37e-04)	Tok/s 84487 (87367)	Loss/tok 2.8237 (3.0696)	LR 1.250e-04
0: TRAIN [5][1510/3880]	Time 0.244 (0.164)	Data 1.11e-04 (3.35e-04)	Tok/s 96205 (87366)	Loss/tok 3.2493 (3.0695)	LR 1.250e-04
0: TRAIN [5][1520/3880]	Time 0.068 (0.164)	Data 1.05e-04 (3.34e-04)	Tok/s 77717 (87367)	Loss/tok 2.4954 (3.0695)	LR 1.250e-04
0: TRAIN [5][1530/3880]	Time 0.182 (0.164)	Data 9.13e-05 (3.32e-04)	Tok/s 91987 (87364)	Loss/tok 3.0527 (3.0692)	LR 1.250e-04
0: TRAIN [5][1540/3880]	Time 0.244 (0.164)	Data 9.18e-05 (3.31e-04)	Tok/s 94986 (87387)	Loss/tok 3.1403 (3.0695)	LR 1.250e-04
0: TRAIN [5][1550/3880]	Time 0.182 (0.164)	Data 1.04e-04 (3.29e-04)	Tok/s 91463 (87397)	Loss/tok 3.0601 (3.0700)	LR 1.250e-04
0: TRAIN [5][1560/3880]	Time 0.180 (0.164)	Data 1.23e-04 (3.28e-04)	Tok/s 93486 (87390)	Loss/tok 3.1342 (3.0698)	LR 1.250e-04
0: TRAIN [5][1570/3880]	Time 0.126 (0.164)	Data 1.27e-04 (3.27e-04)	Tok/s 81995 (87409)	Loss/tok 2.8593 (3.0701)	LR 1.250e-04
0: TRAIN [5][1580/3880]	Time 0.182 (0.164)	Data 1.46e-04 (3.25e-04)	Tok/s 92678 (87395)	Loss/tok 2.9611 (3.0697)	LR 1.250e-04
0: TRAIN [5][1590/3880]	Time 0.124 (0.164)	Data 1.23e-04 (3.24e-04)	Tok/s 83196 (87409)	Loss/tok 2.8424 (3.0702)	LR 1.250e-04
0: TRAIN [5][1600/3880]	Time 0.183 (0.164)	Data 1.35e-04 (3.23e-04)	Tok/s 92792 (87397)	Loss/tok 3.0271 (3.0695)	LR 1.250e-04
0: TRAIN [5][1610/3880]	Time 0.066 (0.164)	Data 9.44e-05 (3.21e-04)	Tok/s 78744 (87390)	Loss/tok 2.4764 (3.0694)	LR 1.250e-04
0: TRAIN [5][1620/3880]	Time 0.066 (0.164)	Data 1.38e-04 (3.20e-04)	Tok/s 79987 (87372)	Loss/tok 2.4412 (3.0690)	LR 1.250e-04
0: TRAIN [5][1630/3880]	Time 0.244 (0.164)	Data 1.07e-04 (3.19e-04)	Tok/s 94396 (87373)	Loss/tok 3.2915 (3.0689)	LR 1.250e-04
0: TRAIN [5][1640/3880]	Time 0.181 (0.164)	Data 1.05e-04 (3.17e-04)	Tok/s 92136 (87374)	Loss/tok 3.1148 (3.0687)	LR 1.250e-04
0: TRAIN [5][1650/3880]	Time 0.184 (0.164)	Data 9.68e-05 (3.16e-04)	Tok/s 90608 (87379)	Loss/tok 3.0900 (3.0696)	LR 1.250e-04
0: TRAIN [5][1660/3880]	Time 0.125 (0.164)	Data 1.18e-04 (3.15e-04)	Tok/s 82669 (87360)	Loss/tok 2.8569 (3.0689)	LR 1.250e-04
0: TRAIN [5][1670/3880]	Time 0.124 (0.164)	Data 1.03e-04 (3.14e-04)	Tok/s 82168 (87364)	Loss/tok 2.8936 (3.0689)	LR 1.250e-04
0: TRAIN [5][1680/3880]	Time 0.245 (0.164)	Data 9.68e-05 (3.12e-04)	Tok/s 95515 (87344)	Loss/tok 3.2676 (3.0685)	LR 1.250e-04
0: TRAIN [5][1690/3880]	Time 0.126 (0.164)	Data 1.09e-04 (3.11e-04)	Tok/s 84450 (87347)	Loss/tok 2.8135 (3.0682)	LR 1.250e-04
0: TRAIN [5][1700/3880]	Time 0.183 (0.164)	Data 1.03e-04 (3.10e-04)	Tok/s 92374 (87352)	Loss/tok 3.1221 (3.0684)	LR 1.250e-04
0: TRAIN [5][1710/3880]	Time 0.126 (0.164)	Data 1.04e-04 (3.09e-04)	Tok/s 80632 (87347)	Loss/tok 2.9272 (3.0683)	LR 1.250e-04
0: TRAIN [5][1720/3880]	Time 0.186 (0.164)	Data 1.03e-04 (3.08e-04)	Tok/s 90386 (87338)	Loss/tok 3.1664 (3.0679)	LR 1.250e-04
0: TRAIN [5][1730/3880]	Time 0.070 (0.164)	Data 1.08e-04 (3.07e-04)	Tok/s 77596 (87345)	Loss/tok 2.5072 (3.0686)	LR 1.250e-04
0: TRAIN [5][1740/3880]	Time 0.316 (0.164)	Data 1.04e-04 (3.05e-04)	Tok/s 94329 (87359)	Loss/tok 3.4222 (3.0691)	LR 1.250e-04
0: TRAIN [5][1750/3880]	Time 0.180 (0.164)	Data 1.39e-04 (3.04e-04)	Tok/s 94054 (87362)	Loss/tok 3.0397 (3.0687)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [5][1760/3880]	Time 0.123 (0.164)	Data 1.13e-04 (3.03e-04)	Tok/s 83204 (87344)	Loss/tok 2.9099 (3.0680)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][1770/3880]	Time 0.125 (0.164)	Data 1.13e-04 (3.02e-04)	Tok/s 82770 (87337)	Loss/tok 2.8686 (3.0681)	LR 1.250e-04
0: TRAIN [5][1780/3880]	Time 0.066 (0.164)	Data 1.14e-04 (3.01e-04)	Tok/s 79595 (87341)	Loss/tok 2.5510 (3.0685)	LR 1.250e-04
0: TRAIN [5][1790/3880]	Time 0.125 (0.164)	Data 1.04e-04 (3.00e-04)	Tok/s 82927 (87339)	Loss/tok 2.8982 (3.0686)	LR 1.250e-04
0: TRAIN [5][1800/3880]	Time 0.243 (0.164)	Data 1.15e-04 (2.99e-04)	Tok/s 95033 (87346)	Loss/tok 3.2793 (3.0689)	LR 1.250e-04
0: TRAIN [5][1810/3880]	Time 0.125 (0.164)	Data 9.78e-05 (2.98e-04)	Tok/s 85376 (87339)	Loss/tok 2.8557 (3.0689)	LR 1.250e-04
0: TRAIN [5][1820/3880]	Time 0.183 (0.164)	Data 1.09e-04 (2.97e-04)	Tok/s 92489 (87341)	Loss/tok 3.0982 (3.0686)	LR 1.250e-04
0: TRAIN [5][1830/3880]	Time 0.067 (0.164)	Data 1.10e-04 (2.96e-04)	Tok/s 79227 (87342)	Loss/tok 2.4876 (3.0684)	LR 1.250e-04
0: TRAIN [5][1840/3880]	Time 0.245 (0.164)	Data 1.15e-04 (2.95e-04)	Tok/s 94423 (87352)	Loss/tok 3.2792 (3.0687)	LR 1.250e-04
0: TRAIN [5][1850/3880]	Time 0.248 (0.164)	Data 1.23e-04 (2.94e-04)	Tok/s 93983 (87352)	Loss/tok 3.1673 (3.0687)	LR 1.250e-04
0: TRAIN [5][1860/3880]	Time 0.123 (0.164)	Data 1.12e-04 (2.93e-04)	Tok/s 83337 (87349)	Loss/tok 2.8466 (3.0682)	LR 1.250e-04
0: TRAIN [5][1870/3880]	Time 0.126 (0.164)	Data 1.13e-04 (2.92e-04)	Tok/s 82811 (87347)	Loss/tok 2.9032 (3.0682)	LR 1.250e-04
0: TRAIN [5][1880/3880]	Time 0.067 (0.164)	Data 1.10e-04 (2.91e-04)	Tok/s 79128 (87343)	Loss/tok 2.5522 (3.0679)	LR 1.250e-04
0: TRAIN [5][1890/3880]	Time 0.183 (0.164)	Data 1.10e-04 (2.90e-04)	Tok/s 90785 (87346)	Loss/tok 3.0337 (3.0678)	LR 1.250e-04
0: TRAIN [5][1900/3880]	Time 0.244 (0.164)	Data 1.09e-04 (2.89e-04)	Tok/s 95717 (87355)	Loss/tok 3.1027 (3.0680)	LR 1.250e-04
0: TRAIN [5][1910/3880]	Time 0.181 (0.164)	Data 1.05e-04 (2.88e-04)	Tok/s 93001 (87366)	Loss/tok 2.9990 (3.0680)	LR 1.250e-04
0: TRAIN [5][1920/3880]	Time 0.124 (0.164)	Data 9.97e-05 (2.87e-04)	Tok/s 84375 (87372)	Loss/tok 2.7881 (3.0677)	LR 1.250e-04
0: TRAIN [5][1930/3880]	Time 0.124 (0.164)	Data 1.09e-04 (2.86e-04)	Tok/s 84233 (87365)	Loss/tok 2.8859 (3.0673)	LR 1.250e-04
0: TRAIN [5][1940/3880]	Time 0.123 (0.164)	Data 1.38e-04 (2.86e-04)	Tok/s 84377 (87373)	Loss/tok 2.9294 (3.0674)	LR 1.250e-04
0: TRAIN [5][1950/3880]	Time 0.127 (0.164)	Data 1.28e-04 (2.85e-04)	Tok/s 82215 (87369)	Loss/tok 2.8793 (3.0673)	LR 1.250e-04
0: TRAIN [5][1960/3880]	Time 0.185 (0.164)	Data 1.00e-04 (2.84e-04)	Tok/s 89667 (87373)	Loss/tok 3.1019 (3.0674)	LR 1.250e-04
0: TRAIN [5][1970/3880]	Time 0.124 (0.164)	Data 1.23e-04 (2.83e-04)	Tok/s 82939 (87382)	Loss/tok 2.8733 (3.0681)	LR 1.250e-04
0: TRAIN [5][1980/3880]	Time 0.123 (0.164)	Data 1.31e-04 (2.82e-04)	Tok/s 83943 (87389)	Loss/tok 2.8804 (3.0681)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][1990/3880]	Time 0.317 (0.164)	Data 1.10e-04 (2.81e-04)	Tok/s 94363 (87391)	Loss/tok 3.2680 (3.0681)	LR 1.250e-04
0: TRAIN [5][2000/3880]	Time 0.184 (0.164)	Data 9.92e-05 (2.80e-04)	Tok/s 91190 (87401)	Loss/tok 2.9993 (3.0684)	LR 1.250e-04
0: TRAIN [5][2010/3880]	Time 0.124 (0.164)	Data 1.24e-04 (2.80e-04)	Tok/s 81746 (87400)	Loss/tok 2.9238 (3.0686)	LR 1.250e-04
0: TRAIN [5][2020/3880]	Time 0.184 (0.164)	Data 1.19e-04 (2.79e-04)	Tok/s 92781 (87409)	Loss/tok 3.1740 (3.0686)	LR 1.250e-04
0: TRAIN [5][2030/3880]	Time 0.181 (0.164)	Data 1.07e-04 (2.78e-04)	Tok/s 93028 (87405)	Loss/tok 3.0657 (3.0687)	LR 1.250e-04
0: TRAIN [5][2040/3880]	Time 0.125 (0.164)	Data 1.08e-04 (2.77e-04)	Tok/s 83527 (87412)	Loss/tok 2.8932 (3.0693)	LR 1.250e-04
0: TRAIN [5][2050/3880]	Time 0.184 (0.164)	Data 1.14e-04 (2.76e-04)	Tok/s 91036 (87407)	Loss/tok 3.0717 (3.0689)	LR 1.250e-04
0: TRAIN [5][2060/3880]	Time 0.066 (0.164)	Data 1.04e-04 (2.76e-04)	Tok/s 81529 (87404)	Loss/tok 2.5568 (3.0687)	LR 1.250e-04
0: TRAIN [5][2070/3880]	Time 0.186 (0.164)	Data 1.19e-04 (2.75e-04)	Tok/s 89708 (87398)	Loss/tok 3.1456 (3.0688)	LR 1.250e-04
0: TRAIN [5][2080/3880]	Time 0.184 (0.164)	Data 1.27e-04 (2.74e-04)	Tok/s 91878 (87407)	Loss/tok 3.0102 (3.0694)	LR 1.250e-04
0: TRAIN [5][2090/3880]	Time 0.183 (0.164)	Data 9.97e-05 (2.73e-04)	Tok/s 91337 (87401)	Loss/tok 3.0570 (3.0690)	LR 1.250e-04
0: TRAIN [5][2100/3880]	Time 0.185 (0.164)	Data 1.24e-04 (2.72e-04)	Tok/s 91494 (87396)	Loss/tok 3.0180 (3.0690)	LR 1.250e-04
0: TRAIN [5][2110/3880]	Time 0.124 (0.164)	Data 9.27e-05 (2.72e-04)	Tok/s 84440 (87388)	Loss/tok 2.8969 (3.0687)	LR 1.250e-04
0: TRAIN [5][2120/3880]	Time 0.125 (0.164)	Data 1.24e-04 (2.71e-04)	Tok/s 81881 (87376)	Loss/tok 2.8678 (3.0682)	LR 1.250e-04
0: TRAIN [5][2130/3880]	Time 0.068 (0.164)	Data 1.16e-04 (2.70e-04)	Tok/s 78129 (87356)	Loss/tok 2.4514 (3.0677)	LR 1.250e-04
0: TRAIN [5][2140/3880]	Time 0.183 (0.164)	Data 1.13e-04 (2.69e-04)	Tok/s 91453 (87364)	Loss/tok 3.0724 (3.0678)	LR 1.250e-04
0: TRAIN [5][2150/3880]	Time 0.124 (0.164)	Data 1.22e-04 (2.69e-04)	Tok/s 84534 (87363)	Loss/tok 2.9207 (3.0677)	LR 1.250e-04
0: TRAIN [5][2160/3880]	Time 0.126 (0.164)	Data 1.21e-04 (2.68e-04)	Tok/s 80323 (87363)	Loss/tok 2.8454 (3.0676)	LR 1.250e-04
0: TRAIN [5][2170/3880]	Time 0.067 (0.163)	Data 1.03e-04 (2.67e-04)	Tok/s 77442 (87362)	Loss/tok 2.4521 (3.0673)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][2180/3880]	Time 0.185 (0.164)	Data 1.50e-04 (2.67e-04)	Tok/s 91235 (87364)	Loss/tok 3.0337 (3.0674)	LR 1.250e-04
0: TRAIN [5][2190/3880]	Time 0.184 (0.164)	Data 1.10e-04 (2.66e-04)	Tok/s 90948 (87367)	Loss/tok 3.1091 (3.0677)	LR 1.250e-04
0: TRAIN [5][2200/3880]	Time 0.183 (0.163)	Data 1.10e-04 (2.65e-04)	Tok/s 90462 (87353)	Loss/tok 3.1667 (3.0672)	LR 1.250e-04
0: TRAIN [5][2210/3880]	Time 0.067 (0.164)	Data 1.05e-04 (2.65e-04)	Tok/s 76522 (87358)	Loss/tok 2.4848 (3.0674)	LR 1.250e-04
0: TRAIN [5][2220/3880]	Time 0.128 (0.164)	Data 1.08e-04 (2.64e-04)	Tok/s 78349 (87359)	Loss/tok 2.9202 (3.0676)	LR 1.250e-04
0: TRAIN [5][2230/3880]	Time 0.318 (0.164)	Data 1.01e-04 (2.63e-04)	Tok/s 92780 (87353)	Loss/tok 3.3228 (3.0679)	LR 1.250e-04
0: TRAIN [5][2240/3880]	Time 0.316 (0.164)	Data 1.22e-04 (2.62e-04)	Tok/s 94192 (87351)	Loss/tok 3.4248 (3.0681)	LR 1.250e-04
0: TRAIN [5][2250/3880]	Time 0.186 (0.164)	Data 1.12e-04 (2.62e-04)	Tok/s 90572 (87354)	Loss/tok 3.0986 (3.0682)	LR 1.250e-04
0: TRAIN [5][2260/3880]	Time 0.127 (0.164)	Data 1.18e-04 (2.61e-04)	Tok/s 81422 (87347)	Loss/tok 2.8804 (3.0680)	LR 1.250e-04
0: TRAIN [5][2270/3880]	Time 0.182 (0.164)	Data 1.19e-04 (2.61e-04)	Tok/s 92997 (87348)	Loss/tok 3.0636 (3.0680)	LR 1.250e-04
0: TRAIN [5][2280/3880]	Time 0.185 (0.164)	Data 1.03e-04 (2.60e-04)	Tok/s 91721 (87357)	Loss/tok 3.0816 (3.0683)	LR 1.250e-04
0: TRAIN [5][2290/3880]	Time 0.185 (0.164)	Data 1.14e-04 (2.59e-04)	Tok/s 89943 (87370)	Loss/tok 2.9561 (3.0685)	LR 1.250e-04
0: TRAIN [5][2300/3880]	Time 0.183 (0.164)	Data 1.14e-04 (2.59e-04)	Tok/s 91024 (87377)	Loss/tok 3.0834 (3.0689)	LR 1.250e-04
0: TRAIN [5][2310/3880]	Time 0.067 (0.164)	Data 1.16e-04 (2.58e-04)	Tok/s 78817 (87380)	Loss/tok 2.5131 (3.0691)	LR 1.250e-04
0: TRAIN [5][2320/3880]	Time 0.127 (0.164)	Data 1.03e-04 (2.57e-04)	Tok/s 81700 (87389)	Loss/tok 2.8790 (3.0695)	LR 1.250e-04
0: TRAIN [5][2330/3880]	Time 0.182 (0.164)	Data 1.35e-04 (2.57e-04)	Tok/s 92870 (87378)	Loss/tok 3.0927 (3.0691)	LR 1.250e-04
0: TRAIN [5][2340/3880]	Time 0.125 (0.164)	Data 1.04e-04 (2.56e-04)	Tok/s 82539 (87374)	Loss/tok 2.9001 (3.0689)	LR 1.250e-04
0: TRAIN [5][2350/3880]	Time 0.185 (0.164)	Data 9.80e-05 (2.56e-04)	Tok/s 91034 (87376)	Loss/tok 3.1406 (3.0690)	LR 1.250e-04
0: TRAIN [5][2360/3880]	Time 0.246 (0.164)	Data 1.30e-04 (2.55e-04)	Tok/s 94707 (87381)	Loss/tok 3.1987 (3.0693)	LR 1.250e-04
0: TRAIN [5][2370/3880]	Time 0.125 (0.164)	Data 1.29e-04 (2.54e-04)	Tok/s 81069 (87380)	Loss/tok 2.9360 (3.0695)	LR 1.250e-04
0: TRAIN [5][2380/3880]	Time 0.246 (0.164)	Data 1.42e-04 (2.54e-04)	Tok/s 95338 (87391)	Loss/tok 3.1894 (3.0697)	LR 1.250e-04
0: TRAIN [5][2390/3880]	Time 0.127 (0.164)	Data 1.17e-04 (2.53e-04)	Tok/s 80012 (87384)	Loss/tok 2.8488 (3.0695)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][2400/3880]	Time 0.183 (0.164)	Data 1.20e-04 (2.53e-04)	Tok/s 91221 (87383)	Loss/tok 3.0660 (3.0697)	LR 1.250e-04
0: TRAIN [5][2410/3880]	Time 0.126 (0.164)	Data 1.30e-04 (2.52e-04)	Tok/s 80008 (87382)	Loss/tok 2.8557 (3.0697)	LR 1.250e-04
0: TRAIN [5][2420/3880]	Time 0.124 (0.164)	Data 1.37e-04 (2.52e-04)	Tok/s 83973 (87378)	Loss/tok 2.8766 (3.0697)	LR 1.250e-04
0: TRAIN [5][2430/3880]	Time 0.125 (0.164)	Data 1.23e-04 (2.51e-04)	Tok/s 83394 (87363)	Loss/tok 2.9466 (3.0692)	LR 1.250e-04
0: TRAIN [5][2440/3880]	Time 0.317 (0.164)	Data 1.19e-04 (2.51e-04)	Tok/s 93620 (87362)	Loss/tok 3.4589 (3.0693)	LR 1.250e-04
0: TRAIN [5][2450/3880]	Time 0.124 (0.164)	Data 1.39e-04 (2.50e-04)	Tok/s 83905 (87363)	Loss/tok 2.8433 (3.0692)	LR 1.250e-04
0: TRAIN [5][2460/3880]	Time 0.125 (0.164)	Data 1.37e-04 (2.50e-04)	Tok/s 82350 (87369)	Loss/tok 2.9922 (3.0698)	LR 1.250e-04
0: TRAIN [5][2470/3880]	Time 0.125 (0.164)	Data 1.15e-04 (2.49e-04)	Tok/s 81050 (87376)	Loss/tok 2.8686 (3.0698)	LR 1.250e-04
0: TRAIN [5][2480/3880]	Time 0.126 (0.164)	Data 1.10e-04 (2.48e-04)	Tok/s 82250 (87372)	Loss/tok 2.9546 (3.0697)	LR 1.250e-04
0: TRAIN [5][2490/3880]	Time 0.124 (0.164)	Data 1.19e-04 (2.48e-04)	Tok/s 81997 (87370)	Loss/tok 2.8017 (3.0698)	LR 1.250e-04
0: TRAIN [5][2500/3880]	Time 0.183 (0.164)	Data 1.04e-04 (2.47e-04)	Tok/s 91933 (87366)	Loss/tok 3.1004 (3.0696)	LR 1.250e-04
0: TRAIN [5][2510/3880]	Time 0.183 (0.164)	Data 1.08e-04 (2.47e-04)	Tok/s 91227 (87373)	Loss/tok 3.0769 (3.0701)	LR 1.250e-04
0: TRAIN [5][2520/3880]	Time 0.067 (0.164)	Data 1.13e-04 (2.46e-04)	Tok/s 78552 (87368)	Loss/tok 2.5225 (3.0699)	LR 1.250e-04
0: TRAIN [5][2530/3880]	Time 0.184 (0.164)	Data 1.07e-04 (2.46e-04)	Tok/s 91907 (87367)	Loss/tok 2.9683 (3.0697)	LR 1.250e-04
0: TRAIN [5][2540/3880]	Time 0.126 (0.164)	Data 1.40e-04 (2.45e-04)	Tok/s 82730 (87358)	Loss/tok 2.8670 (3.0695)	LR 1.250e-04
0: TRAIN [5][2550/3880]	Time 0.124 (0.164)	Data 1.35e-04 (2.45e-04)	Tok/s 82569 (87354)	Loss/tok 2.8700 (3.0693)	LR 1.250e-04
0: TRAIN [5][2560/3880]	Time 0.183 (0.164)	Data 1.19e-04 (2.44e-04)	Tok/s 91090 (87363)	Loss/tok 3.0261 (3.0694)	LR 1.250e-04
0: TRAIN [5][2570/3880]	Time 0.184 (0.164)	Data 1.23e-04 (2.44e-04)	Tok/s 92392 (87371)	Loss/tok 3.0950 (3.0696)	LR 1.250e-04
0: TRAIN [5][2580/3880]	Time 0.125 (0.164)	Data 1.06e-04 (2.43e-04)	Tok/s 82735 (87372)	Loss/tok 2.8546 (3.0695)	LR 1.250e-04
0: TRAIN [5][2590/3880]	Time 0.125 (0.164)	Data 9.92e-05 (2.43e-04)	Tok/s 79785 (87364)	Loss/tok 2.7943 (3.0692)	LR 1.250e-04
0: TRAIN [5][2600/3880]	Time 0.183 (0.164)	Data 1.11e-04 (2.42e-04)	Tok/s 92325 (87363)	Loss/tok 3.0944 (3.0692)	LR 1.250e-04
0: TRAIN [5][2610/3880]	Time 0.182 (0.164)	Data 9.20e-05 (2.42e-04)	Tok/s 94064 (87364)	Loss/tok 2.9364 (3.0692)	LR 1.250e-04
0: TRAIN [5][2620/3880]	Time 0.125 (0.164)	Data 1.18e-04 (2.41e-04)	Tok/s 82476 (87361)	Loss/tok 2.7962 (3.0692)	LR 1.250e-04
0: TRAIN [5][2630/3880]	Time 0.126 (0.164)	Data 1.24e-04 (2.41e-04)	Tok/s 82527 (87366)	Loss/tok 2.8630 (3.0692)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][2640/3880]	Time 0.243 (0.164)	Data 1.02e-04 (2.40e-04)	Tok/s 95430 (87356)	Loss/tok 3.1662 (3.0691)	LR 1.250e-04
0: TRAIN [5][2650/3880]	Time 0.125 (0.164)	Data 1.13e-04 (2.40e-04)	Tok/s 81644 (87362)	Loss/tok 2.9403 (3.0694)	LR 1.250e-04
0: TRAIN [5][2660/3880]	Time 0.243 (0.164)	Data 1.32e-04 (2.39e-04)	Tok/s 95653 (87368)	Loss/tok 3.2926 (3.0698)	LR 1.250e-04
0: TRAIN [5][2670/3880]	Time 0.127 (0.164)	Data 1.44e-04 (2.39e-04)	Tok/s 81644 (87369)	Loss/tok 2.8430 (3.0701)	LR 1.250e-04
0: TRAIN [5][2680/3880]	Time 0.315 (0.164)	Data 1.08e-04 (2.39e-04)	Tok/s 95337 (87385)	Loss/tok 3.3666 (3.0705)	LR 1.250e-04
0: TRAIN [5][2690/3880]	Time 0.126 (0.164)	Data 1.03e-04 (2.38e-04)	Tok/s 80927 (87383)	Loss/tok 2.9106 (3.0704)	LR 1.250e-04
0: TRAIN [5][2700/3880]	Time 0.124 (0.164)	Data 1.62e-04 (2.38e-04)	Tok/s 82798 (87376)	Loss/tok 2.8873 (3.0702)	LR 1.250e-04
0: TRAIN [5][2710/3880]	Time 0.247 (0.164)	Data 1.16e-04 (2.37e-04)	Tok/s 94065 (87384)	Loss/tok 3.2826 (3.0705)	LR 1.250e-04
0: TRAIN [5][2720/3880]	Time 0.126 (0.164)	Data 1.09e-04 (2.37e-04)	Tok/s 81359 (87384)	Loss/tok 2.9511 (3.0707)	LR 1.250e-04
0: TRAIN [5][2730/3880]	Time 0.247 (0.164)	Data 1.39e-04 (2.36e-04)	Tok/s 93641 (87383)	Loss/tok 3.2266 (3.0705)	LR 1.250e-04
0: TRAIN [5][2740/3880]	Time 0.124 (0.164)	Data 1.28e-04 (2.36e-04)	Tok/s 83642 (87387)	Loss/tok 3.0640 (3.0707)	LR 1.250e-04
0: TRAIN [5][2750/3880]	Time 0.126 (0.164)	Data 1.13e-04 (2.36e-04)	Tok/s 81146 (87389)	Loss/tok 2.8584 (3.0708)	LR 1.250e-04
0: TRAIN [5][2760/3880]	Time 0.184 (0.165)	Data 1.25e-04 (2.35e-04)	Tok/s 91097 (87393)	Loss/tok 3.0661 (3.0711)	LR 1.250e-04
0: TRAIN [5][2770/3880]	Time 0.124 (0.165)	Data 1.49e-04 (2.35e-04)	Tok/s 84142 (87401)	Loss/tok 2.9130 (3.0714)	LR 1.250e-04
0: TRAIN [5][2780/3880]	Time 0.245 (0.165)	Data 1.36e-04 (2.34e-04)	Tok/s 95878 (87402)	Loss/tok 3.2985 (3.0717)	LR 1.250e-04
0: TRAIN [5][2790/3880]	Time 0.124 (0.165)	Data 1.13e-04 (2.34e-04)	Tok/s 83685 (87403)	Loss/tok 2.9389 (3.0718)	LR 1.250e-04
0: TRAIN [5][2800/3880]	Time 0.124 (0.165)	Data 1.43e-04 (2.34e-04)	Tok/s 84839 (87406)	Loss/tok 2.9198 (3.0722)	LR 1.250e-04
0: TRAIN [5][2810/3880]	Time 0.181 (0.165)	Data 1.04e-04 (2.33e-04)	Tok/s 92034 (87409)	Loss/tok 3.2642 (3.0724)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][2820/3880]	Time 0.124 (0.165)	Data 1.36e-04 (2.33e-04)	Tok/s 82556 (87408)	Loss/tok 2.8220 (3.0723)	LR 1.250e-04
0: TRAIN [5][2830/3880]	Time 0.182 (0.165)	Data 1.25e-04 (2.32e-04)	Tok/s 92519 (87406)	Loss/tok 3.1510 (3.0723)	LR 1.250e-04
0: TRAIN [5][2840/3880]	Time 0.244 (0.165)	Data 1.23e-04 (2.32e-04)	Tok/s 95023 (87415)	Loss/tok 3.2903 (3.0726)	LR 1.250e-04
0: TRAIN [5][2850/3880]	Time 0.124 (0.165)	Data 1.16e-04 (2.32e-04)	Tok/s 84598 (87421)	Loss/tok 2.8843 (3.0726)	LR 1.250e-04
0: TRAIN [5][2860/3880]	Time 0.124 (0.165)	Data 1.15e-04 (2.31e-04)	Tok/s 84385 (87418)	Loss/tok 2.9048 (3.0724)	LR 1.250e-04
0: TRAIN [5][2870/3880]	Time 0.125 (0.165)	Data 1.10e-04 (2.31e-04)	Tok/s 82338 (87410)	Loss/tok 2.8617 (3.0721)	LR 1.250e-04
0: TRAIN [5][2880/3880]	Time 0.245 (0.165)	Data 1.13e-04 (2.31e-04)	Tok/s 96411 (87407)	Loss/tok 3.2928 (3.0721)	LR 1.250e-04
0: TRAIN [5][2890/3880]	Time 0.183 (0.165)	Data 1.09e-04 (2.30e-04)	Tok/s 92522 (87401)	Loss/tok 3.1073 (3.0719)	LR 1.250e-04
0: TRAIN [5][2900/3880]	Time 0.316 (0.165)	Data 1.11e-04 (2.30e-04)	Tok/s 95095 (87399)	Loss/tok 3.3104 (3.0720)	LR 1.250e-04
0: TRAIN [5][2910/3880]	Time 0.066 (0.165)	Data 1.10e-04 (2.29e-04)	Tok/s 80599 (87391)	Loss/tok 2.4924 (3.0721)	LR 1.250e-04
0: TRAIN [5][2920/3880]	Time 0.243 (0.165)	Data 1.14e-04 (2.29e-04)	Tok/s 95593 (87394)	Loss/tok 3.2147 (3.0721)	LR 1.250e-04
0: TRAIN [5][2930/3880]	Time 0.315 (0.165)	Data 9.75e-05 (2.29e-04)	Tok/s 94238 (87387)	Loss/tok 3.3765 (3.0722)	LR 1.250e-04
0: TRAIN [5][2940/3880]	Time 0.125 (0.165)	Data 9.66e-05 (2.28e-04)	Tok/s 82375 (87391)	Loss/tok 2.9378 (3.0722)	LR 1.250e-04
0: TRAIN [5][2950/3880]	Time 0.243 (0.165)	Data 9.78e-05 (2.28e-04)	Tok/s 96201 (87390)	Loss/tok 3.2998 (3.0721)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][2960/3880]	Time 0.124 (0.165)	Data 9.94e-05 (2.27e-04)	Tok/s 84149 (87389)	Loss/tok 2.9355 (3.0720)	LR 1.250e-04
0: TRAIN [5][2970/3880]	Time 0.317 (0.165)	Data 1.33e-04 (2.27e-04)	Tok/s 94418 (87395)	Loss/tok 3.3665 (3.0723)	LR 1.250e-04
0: TRAIN [5][2980/3880]	Time 0.182 (0.165)	Data 1.33e-04 (2.27e-04)	Tok/s 92448 (87397)	Loss/tok 3.0758 (3.0722)	LR 1.250e-04
0: TRAIN [5][2990/3880]	Time 0.124 (0.165)	Data 1.07e-04 (2.26e-04)	Tok/s 83521 (87400)	Loss/tok 2.8385 (3.0723)	LR 1.250e-04
0: TRAIN [5][3000/3880]	Time 0.067 (0.165)	Data 1.06e-04 (2.26e-04)	Tok/s 79031 (87403)	Loss/tok 2.4797 (3.0724)	LR 1.250e-04
0: TRAIN [5][3010/3880]	Time 0.124 (0.165)	Data 1.21e-04 (2.25e-04)	Tok/s 83050 (87409)	Loss/tok 2.9532 (3.0726)	LR 1.250e-04
0: TRAIN [5][3020/3880]	Time 0.125 (0.165)	Data 1.01e-04 (2.25e-04)	Tok/s 81551 (87403)	Loss/tok 2.8610 (3.0722)	LR 1.250e-04
0: TRAIN [5][3030/3880]	Time 0.125 (0.165)	Data 1.35e-04 (2.25e-04)	Tok/s 83221 (87404)	Loss/tok 2.9407 (3.0722)	LR 1.250e-04
0: TRAIN [5][3040/3880]	Time 0.315 (0.165)	Data 1.28e-04 (2.24e-04)	Tok/s 94299 (87396)	Loss/tok 3.4805 (3.0722)	LR 1.250e-04
0: TRAIN [5][3050/3880]	Time 0.124 (0.164)	Data 1.33e-04 (2.24e-04)	Tok/s 81947 (87390)	Loss/tok 2.8570 (3.0720)	LR 1.250e-04
0: TRAIN [5][3060/3880]	Time 0.317 (0.165)	Data 1.09e-04 (2.24e-04)	Tok/s 94369 (87392)	Loss/tok 3.2417 (3.0721)	LR 1.250e-04
0: TRAIN [5][3070/3880]	Time 0.067 (0.164)	Data 1.06e-04 (2.23e-04)	Tok/s 81470 (87388)	Loss/tok 2.4129 (3.0717)	LR 1.250e-04
0: TRAIN [5][3080/3880]	Time 0.315 (0.164)	Data 9.99e-05 (2.23e-04)	Tok/s 93663 (87379)	Loss/tok 3.5282 (3.0716)	LR 1.250e-04
0: TRAIN [5][3090/3880]	Time 0.066 (0.164)	Data 1.17e-04 (2.23e-04)	Tok/s 80021 (87376)	Loss/tok 2.3655 (3.0717)	LR 1.250e-04
0: TRAIN [5][3100/3880]	Time 0.184 (0.164)	Data 1.14e-04 (2.22e-04)	Tok/s 91374 (87380)	Loss/tok 2.9926 (3.0718)	LR 1.250e-04
0: TRAIN [5][3110/3880]	Time 0.246 (0.164)	Data 1.06e-04 (2.22e-04)	Tok/s 95105 (87381)	Loss/tok 3.3216 (3.0718)	LR 1.250e-04
0: TRAIN [5][3120/3880]	Time 0.181 (0.164)	Data 1.03e-04 (2.22e-04)	Tok/s 93237 (87386)	Loss/tok 3.0651 (3.0718)	LR 1.250e-04
0: TRAIN [5][3130/3880]	Time 0.067 (0.164)	Data 1.06e-04 (2.21e-04)	Tok/s 78973 (87384)	Loss/tok 2.5637 (3.0718)	LR 1.250e-04
0: TRAIN [5][3140/3880]	Time 0.181 (0.164)	Data 1.36e-04 (2.21e-04)	Tok/s 92611 (87385)	Loss/tok 3.0136 (3.0716)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][3150/3880]	Time 0.315 (0.164)	Data 1.11e-04 (2.21e-04)	Tok/s 94131 (87388)	Loss/tok 3.4432 (3.0718)	LR 1.250e-04
0: TRAIN [5][3160/3880]	Time 0.243 (0.164)	Data 1.17e-04 (2.20e-04)	Tok/s 94225 (87382)	Loss/tok 3.3437 (3.0717)	LR 1.250e-04
0: TRAIN [5][3170/3880]	Time 0.125 (0.164)	Data 1.14e-04 (2.20e-04)	Tok/s 82535 (87377)	Loss/tok 2.8713 (3.0715)	LR 1.250e-04
0: TRAIN [5][3180/3880]	Time 0.182 (0.164)	Data 9.39e-05 (2.20e-04)	Tok/s 91959 (87378)	Loss/tok 3.0708 (3.0714)	LR 1.250e-04
0: TRAIN [5][3190/3880]	Time 0.068 (0.164)	Data 1.10e-04 (2.19e-04)	Tok/s 75526 (87377)	Loss/tok 2.4935 (3.0714)	LR 1.250e-04
0: TRAIN [5][3200/3880]	Time 0.180 (0.164)	Data 1.13e-04 (2.19e-04)	Tok/s 93388 (87381)	Loss/tok 3.0776 (3.0713)	LR 1.250e-04
0: TRAIN [5][3210/3880]	Time 0.183 (0.164)	Data 1.11e-04 (2.19e-04)	Tok/s 92723 (87395)	Loss/tok 3.1783 (3.0718)	LR 1.250e-04
0: TRAIN [5][3220/3880]	Time 0.246 (0.164)	Data 1.30e-04 (2.18e-04)	Tok/s 94261 (87394)	Loss/tok 3.1977 (3.0718)	LR 1.250e-04
0: TRAIN [5][3230/3880]	Time 0.183 (0.164)	Data 1.01e-04 (2.18e-04)	Tok/s 91108 (87400)	Loss/tok 3.0632 (3.0719)	LR 1.250e-04
0: TRAIN [5][3240/3880]	Time 0.123 (0.164)	Data 1.01e-04 (2.18e-04)	Tok/s 85059 (87398)	Loss/tok 2.7821 (3.0718)	LR 1.250e-04
0: TRAIN [5][3250/3880]	Time 0.244 (0.164)	Data 1.25e-04 (2.17e-04)	Tok/s 95927 (87399)	Loss/tok 3.3231 (3.0719)	LR 1.250e-04
0: TRAIN [5][3260/3880]	Time 0.124 (0.164)	Data 1.17e-04 (2.17e-04)	Tok/s 82286 (87393)	Loss/tok 2.8929 (3.0718)	LR 1.250e-04
0: TRAIN [5][3270/3880]	Time 0.124 (0.164)	Data 1.00e-04 (2.17e-04)	Tok/s 82276 (87391)	Loss/tok 2.8836 (3.0717)	LR 1.250e-04
0: TRAIN [5][3280/3880]	Time 0.244 (0.164)	Data 1.18e-04 (2.16e-04)	Tok/s 96284 (87388)	Loss/tok 3.4072 (3.0717)	LR 1.250e-04
0: TRAIN [5][3290/3880]	Time 0.187 (0.164)	Data 9.66e-05 (2.16e-04)	Tok/s 88846 (87384)	Loss/tok 3.0351 (3.0715)	LR 1.250e-04
0: TRAIN [5][3300/3880]	Time 0.124 (0.164)	Data 9.54e-05 (2.16e-04)	Tok/s 84067 (87378)	Loss/tok 2.9470 (3.0712)	LR 1.250e-04
0: TRAIN [5][3310/3880]	Time 0.127 (0.164)	Data 1.00e-04 (2.15e-04)	Tok/s 79809 (87384)	Loss/tok 2.8774 (3.0713)	LR 1.250e-04
0: TRAIN [5][3320/3880]	Time 0.067 (0.164)	Data 9.25e-05 (2.15e-04)	Tok/s 78891 (87379)	Loss/tok 2.5490 (3.0712)	LR 1.250e-04
0: TRAIN [5][3330/3880]	Time 0.185 (0.164)	Data 1.03e-04 (2.15e-04)	Tok/s 89736 (87378)	Loss/tok 3.1273 (3.0712)	LR 1.250e-04
0: TRAIN [5][3340/3880]	Time 0.126 (0.164)	Data 1.01e-04 (2.14e-04)	Tok/s 82701 (87381)	Loss/tok 2.9122 (3.0711)	LR 1.250e-04
0: TRAIN [5][3350/3880]	Time 0.126 (0.164)	Data 1.06e-04 (2.14e-04)	Tok/s 82906 (87376)	Loss/tok 2.9592 (3.0711)	LR 1.250e-04
0: TRAIN [5][3360/3880]	Time 0.183 (0.164)	Data 1.03e-04 (2.14e-04)	Tok/s 91057 (87379)	Loss/tok 3.0437 (3.0713)	LR 1.250e-04
0: TRAIN [5][3370/3880]	Time 0.246 (0.164)	Data 9.94e-05 (2.14e-04)	Tok/s 94493 (87383)	Loss/tok 3.1414 (3.0712)	LR 1.250e-04
0: TRAIN [5][3380/3880]	Time 0.124 (0.164)	Data 1.04e-04 (2.13e-04)	Tok/s 83425 (87383)	Loss/tok 3.0074 (3.0712)	LR 1.250e-04
0: TRAIN [5][3390/3880]	Time 0.125 (0.164)	Data 1.08e-04 (2.13e-04)	Tok/s 80610 (87374)	Loss/tok 2.8965 (3.0710)	LR 1.250e-04
0: TRAIN [5][3400/3880]	Time 0.126 (0.164)	Data 1.07e-04 (2.13e-04)	Tok/s 80404 (87374)	Loss/tok 2.8342 (3.0713)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [5][3410/3880]	Time 0.245 (0.164)	Data 1.31e-04 (2.12e-04)	Tok/s 94577 (87382)	Loss/tok 3.3188 (3.0714)	LR 1.250e-04
0: TRAIN [5][3420/3880]	Time 0.244 (0.164)	Data 1.11e-04 (2.12e-04)	Tok/s 94781 (87388)	Loss/tok 3.2036 (3.0715)	LR 1.250e-04
0: TRAIN [5][3430/3880]	Time 0.126 (0.164)	Data 1.18e-04 (2.12e-04)	Tok/s 83808 (87383)	Loss/tok 2.7876 (3.0714)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][3440/3880]	Time 0.126 (0.164)	Data 1.32e-04 (2.12e-04)	Tok/s 81683 (87389)	Loss/tok 2.8385 (3.0717)	LR 1.250e-04
0: TRAIN [5][3450/3880]	Time 0.183 (0.164)	Data 1.19e-04 (2.11e-04)	Tok/s 92777 (87394)	Loss/tok 3.1625 (3.0719)	LR 1.250e-04
0: TRAIN [5][3460/3880]	Time 0.183 (0.164)	Data 1.10e-04 (2.11e-04)	Tok/s 91749 (87393)	Loss/tok 3.1415 (3.0718)	LR 1.250e-04
0: TRAIN [5][3470/3880]	Time 0.183 (0.164)	Data 1.16e-04 (2.11e-04)	Tok/s 92026 (87389)	Loss/tok 2.9687 (3.0717)	LR 1.250e-04
0: TRAIN [5][3480/3880]	Time 0.185 (0.164)	Data 1.22e-04 (2.11e-04)	Tok/s 92087 (87394)	Loss/tok 2.9933 (3.0717)	LR 1.250e-04
0: TRAIN [5][3490/3880]	Time 0.183 (0.164)	Data 1.05e-04 (2.10e-04)	Tok/s 92541 (87394)	Loss/tok 3.0144 (3.0716)	LR 1.250e-04
0: TRAIN [5][3500/3880]	Time 0.125 (0.164)	Data 1.06e-04 (2.10e-04)	Tok/s 83460 (87391)	Loss/tok 2.9080 (3.0715)	LR 1.250e-04
0: TRAIN [5][3510/3880]	Time 0.124 (0.164)	Data 1.27e-04 (2.10e-04)	Tok/s 80455 (87388)	Loss/tok 2.8691 (3.0714)	LR 1.250e-04
0: TRAIN [5][3520/3880]	Time 0.312 (0.164)	Data 1.54e-04 (2.10e-04)	Tok/s 95243 (87387)	Loss/tok 3.3108 (3.0715)	LR 1.250e-04
0: TRAIN [5][3530/3880]	Time 0.242 (0.164)	Data 1.29e-04 (2.09e-04)	Tok/s 96193 (87386)	Loss/tok 3.2300 (3.0713)	LR 1.250e-04
0: TRAIN [5][3540/3880]	Time 0.125 (0.164)	Data 1.20e-04 (2.09e-04)	Tok/s 81603 (87385)	Loss/tok 2.9860 (3.0713)	LR 1.250e-04
0: TRAIN [5][3550/3880]	Time 0.123 (0.164)	Data 1.13e-04 (2.09e-04)	Tok/s 85208 (87379)	Loss/tok 2.9112 (3.0709)	LR 1.250e-04
0: TRAIN [5][3560/3880]	Time 0.182 (0.164)	Data 1.21e-04 (2.09e-04)	Tok/s 91885 (87382)	Loss/tok 3.0774 (3.0708)	LR 1.250e-04
0: TRAIN [5][3570/3880]	Time 0.127 (0.164)	Data 1.18e-04 (2.08e-04)	Tok/s 82933 (87379)	Loss/tok 2.8745 (3.0707)	LR 1.250e-04
0: TRAIN [5][3580/3880]	Time 0.184 (0.164)	Data 1.19e-04 (2.08e-04)	Tok/s 90479 (87381)	Loss/tok 3.1253 (3.0710)	LR 1.250e-04
0: TRAIN [5][3590/3880]	Time 0.123 (0.164)	Data 1.20e-04 (2.08e-04)	Tok/s 85242 (87379)	Loss/tok 2.8553 (3.0708)	LR 1.250e-04
0: TRAIN [5][3600/3880]	Time 0.067 (0.164)	Data 1.19e-04 (2.08e-04)	Tok/s 78829 (87370)	Loss/tok 2.5120 (3.0704)	LR 1.250e-04
0: TRAIN [5][3610/3880]	Time 0.067 (0.164)	Data 1.23e-04 (2.07e-04)	Tok/s 78419 (87366)	Loss/tok 2.4611 (3.0703)	LR 1.250e-04
0: TRAIN [5][3620/3880]	Time 0.185 (0.164)	Data 1.18e-04 (2.07e-04)	Tok/s 91163 (87370)	Loss/tok 3.1613 (3.0706)	LR 1.250e-04
0: TRAIN [5][3630/3880]	Time 0.245 (0.164)	Data 9.94e-05 (2.07e-04)	Tok/s 95864 (87366)	Loss/tok 3.1610 (3.0706)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][3640/3880]	Time 0.185 (0.164)	Data 2.05e-04 (2.07e-04)	Tok/s 90306 (87370)	Loss/tok 3.1159 (3.0707)	LR 1.250e-04
0: TRAIN [5][3650/3880]	Time 0.183 (0.164)	Data 1.12e-04 (2.06e-04)	Tok/s 90621 (87372)	Loss/tok 3.0641 (3.0706)	LR 1.250e-04
0: TRAIN [5][3660/3880]	Time 0.124 (0.164)	Data 9.80e-05 (2.06e-04)	Tok/s 84294 (87374)	Loss/tok 2.9319 (3.0709)	LR 1.250e-04
0: TRAIN [5][3670/3880]	Time 0.123 (0.164)	Data 9.63e-05 (2.06e-04)	Tok/s 85098 (87373)	Loss/tok 2.9278 (3.0710)	LR 1.250e-04
0: TRAIN [5][3680/3880]	Time 0.245 (0.164)	Data 9.27e-05 (2.05e-04)	Tok/s 95994 (87375)	Loss/tok 3.1432 (3.0710)	LR 1.250e-04
0: TRAIN [5][3690/3880]	Time 0.125 (0.164)	Data 9.80e-05 (2.05e-04)	Tok/s 83666 (87376)	Loss/tok 2.8684 (3.0710)	LR 1.250e-04
0: TRAIN [5][3700/3880]	Time 0.123 (0.164)	Data 9.54e-05 (2.05e-04)	Tok/s 83626 (87377)	Loss/tok 2.9690 (3.0711)	LR 1.250e-04
0: TRAIN [5][3710/3880]	Time 0.124 (0.164)	Data 9.89e-05 (2.05e-04)	Tok/s 83022 (87383)	Loss/tok 2.8455 (3.0710)	LR 1.250e-04
0: TRAIN [5][3720/3880]	Time 0.315 (0.164)	Data 1.11e-04 (2.04e-04)	Tok/s 95425 (87390)	Loss/tok 3.3898 (3.0711)	LR 1.250e-04
0: TRAIN [5][3730/3880]	Time 0.313 (0.164)	Data 1.14e-04 (2.04e-04)	Tok/s 95082 (87387)	Loss/tok 3.3571 (3.0713)	LR 1.250e-04
0: TRAIN [5][3740/3880]	Time 0.181 (0.164)	Data 9.37e-05 (2.04e-04)	Tok/s 93539 (87382)	Loss/tok 2.9989 (3.0711)	LR 1.250e-04
0: TRAIN [5][3750/3880]	Time 0.245 (0.164)	Data 1.01e-04 (2.04e-04)	Tok/s 95610 (87381)	Loss/tok 3.1915 (3.0712)	LR 1.250e-04
0: TRAIN [5][3760/3880]	Time 0.123 (0.164)	Data 1.20e-04 (2.03e-04)	Tok/s 84845 (87376)	Loss/tok 2.9569 (3.0710)	LR 1.250e-04
0: TRAIN [5][3770/3880]	Time 0.067 (0.164)	Data 9.61e-05 (2.03e-04)	Tok/s 77266 (87370)	Loss/tok 2.5310 (3.0708)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [5][3780/3880]	Time 0.126 (0.164)	Data 1.06e-04 (2.03e-04)	Tok/s 80937 (87368)	Loss/tok 2.8903 (3.0711)	LR 1.250e-04
0: TRAIN [5][3790/3880]	Time 0.184 (0.164)	Data 1.09e-04 (2.03e-04)	Tok/s 90386 (87371)	Loss/tok 3.0849 (3.0711)	LR 1.250e-04
0: TRAIN [5][3800/3880]	Time 0.123 (0.164)	Data 1.09e-04 (2.02e-04)	Tok/s 83842 (87369)	Loss/tok 2.9379 (3.0709)	LR 1.250e-04
0: TRAIN [5][3810/3880]	Time 0.067 (0.164)	Data 1.15e-04 (2.02e-04)	Tok/s 79607 (87362)	Loss/tok 2.5020 (3.0706)	LR 1.250e-04
0: TRAIN [5][3820/3880]	Time 0.184 (0.164)	Data 1.10e-04 (2.02e-04)	Tok/s 92509 (87367)	Loss/tok 3.0943 (3.0705)	LR 1.250e-04
0: TRAIN [5][3830/3880]	Time 0.127 (0.164)	Data 1.09e-04 (2.02e-04)	Tok/s 80483 (87366)	Loss/tok 2.9085 (3.0705)	LR 1.250e-04
0: TRAIN [5][3840/3880]	Time 0.069 (0.164)	Data 1.14e-04 (2.01e-04)	Tok/s 78009 (87364)	Loss/tok 2.5639 (3.0705)	LR 1.250e-04
0: TRAIN [5][3850/3880]	Time 0.123 (0.164)	Data 1.27e-04 (2.01e-04)	Tok/s 83074 (87365)	Loss/tok 2.8651 (3.0704)	LR 1.250e-04
0: TRAIN [5][3860/3880]	Time 0.184 (0.164)	Data 1.11e-04 (2.01e-04)	Tok/s 91858 (87366)	Loss/tok 3.0479 (3.0702)	LR 1.250e-04
0: TRAIN [5][3870/3880]	Time 0.182 (0.164)	Data 1.11e-04 (2.01e-04)	Tok/s 91773 (87372)	Loss/tok 3.0991 (3.0704)	LR 1.250e-04
:::MLL 1586327132.940 epoch_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 524}}
:::MLL 1586327132.941 eval_start: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [5][0/6]	Time 0.718 (0.718)	Decoder iters 149.0 (149.0)	Tok/s 22883 (22883)
0: Running moses detokenizer
0: BLEU(score=23.861168375896472, counts=[37101, 18535, 10528, 6259], totals=[65741, 62738, 59735, 56737], precisions=[56.435101382698775, 29.543498358251778, 17.624508244747634, 11.031601952870261], bp=1.0, sys_len=65741, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586327135.984 eval_accuracy: {"value": 23.86, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 535}}
:::MLL 1586327135.985 eval_stop: {"value": null, "metadata": {"epoch_num": 6, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 5	Training Loss: 3.0700	Test BLEU: 23.86
0: Performance: Epoch: 5	Training: 349426 Tok/s
0: Finished epoch 5
:::MLL 1586327135.985 block_stop: {"value": null, "metadata": {"first_epoch_num": 6, "file": "train.py", "lineno": 557}}
:::MLL 1586327135.985 block_start: {"value": null, "metadata": {"first_epoch_num": 7, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1586327135.986 epoch_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 514}}
0: Starting epoch 6
0: Executing preallocation
0: Sampler for epoch 6 uses seed 1596212798
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [6][0/3880]	Time 0.454 (0.454)	Data 3.28e-01 (3.28e-01)	Tok/s 23002 (23002)	Loss/tok 2.9019 (2.9019)	LR 1.250e-04
0: TRAIN [6][10/3880]	Time 0.181 (0.192)	Data 1.15e-04 (2.99e-02)	Tok/s 92042 (82487)	Loss/tok 3.0393 (3.0313)	LR 1.250e-04
0: TRAIN [6][20/3880]	Time 0.182 (0.179)	Data 1.13e-04 (1.57e-02)	Tok/s 92362 (85556)	Loss/tok 3.0643 (3.0162)	LR 1.250e-04
0: TRAIN [6][30/3880]	Time 0.315 (0.172)	Data 1.15e-04 (1.07e-02)	Tok/s 95977 (85568)	Loss/tok 3.2587 (3.0134)	LR 1.250e-04
0: TRAIN [6][40/3880]	Time 0.180 (0.170)	Data 1.15e-04 (8.11e-03)	Tok/s 91367 (86126)	Loss/tok 3.1146 (3.0178)	LR 1.250e-04
0: TRAIN [6][50/3880]	Time 0.181 (0.166)	Data 1.13e-04 (6.54e-03)	Tok/s 92219 (86202)	Loss/tok 2.9607 (3.0178)	LR 1.250e-04
0: TRAIN [6][60/3880]	Time 0.181 (0.167)	Data 1.15e-04 (5.49e-03)	Tok/s 92422 (86501)	Loss/tok 3.0683 (3.0373)	LR 1.250e-04
0: TRAIN [6][70/3880]	Time 0.124 (0.164)	Data 1.16e-04 (4.73e-03)	Tok/s 80908 (86459)	Loss/tok 2.7922 (3.0233)	LR 1.250e-04
0: TRAIN [6][80/3880]	Time 0.125 (0.165)	Data 1.19e-04 (4.17e-03)	Tok/s 83140 (86589)	Loss/tok 2.8669 (3.0265)	LR 1.250e-04
0: TRAIN [6][90/3880]	Time 0.317 (0.166)	Data 1.15e-04 (3.72e-03)	Tok/s 93718 (86688)	Loss/tok 3.3015 (3.0394)	LR 1.250e-04
0: TRAIN [6][100/3880]	Time 0.125 (0.166)	Data 1.12e-04 (3.36e-03)	Tok/s 82389 (86742)	Loss/tok 2.8673 (3.0454)	LR 1.250e-04
0: TRAIN [6][110/3880]	Time 0.242 (0.167)	Data 1.34e-04 (3.07e-03)	Tok/s 96939 (86698)	Loss/tok 3.2788 (3.0492)	LR 1.250e-04
0: TRAIN [6][120/3880]	Time 0.182 (0.166)	Data 1.44e-04 (2.83e-03)	Tok/s 93052 (86783)	Loss/tok 2.9584 (3.0461)	LR 1.250e-04
0: TRAIN [6][130/3880]	Time 0.245 (0.166)	Data 1.34e-04 (2.62e-03)	Tok/s 93433 (86889)	Loss/tok 3.3087 (3.0457)	LR 1.250e-04
0: TRAIN [6][140/3880]	Time 0.183 (0.164)	Data 1.13e-04 (2.45e-03)	Tok/s 91660 (86830)	Loss/tok 2.9310 (3.0372)	LR 1.250e-04
0: TRAIN [6][150/3880]	Time 0.316 (0.166)	Data 1.15e-04 (2.29e-03)	Tok/s 94500 (87009)	Loss/tok 3.4579 (3.0425)	LR 1.250e-04
0: TRAIN [6][160/3880]	Time 0.184 (0.166)	Data 1.15e-04 (2.16e-03)	Tok/s 92455 (87094)	Loss/tok 3.0806 (3.0445)	LR 1.250e-04
0: TRAIN [6][170/3880]	Time 0.182 (0.165)	Data 1.34e-04 (2.04e-03)	Tok/s 91590 (87023)	Loss/tok 2.9993 (3.0428)	LR 1.250e-04
0: TRAIN [6][180/3880]	Time 0.127 (0.166)	Data 1.16e-04 (1.93e-03)	Tok/s 81656 (87163)	Loss/tok 2.9314 (3.0430)	LR 1.250e-04
0: TRAIN [6][190/3880]	Time 0.069 (0.166)	Data 1.33e-04 (1.84e-03)	Tok/s 77866 (87201)	Loss/tok 2.5407 (3.0455)	LR 1.250e-04
0: TRAIN [6][200/3880]	Time 0.182 (0.168)	Data 1.09e-04 (1.75e-03)	Tok/s 94414 (87429)	Loss/tok 2.9449 (3.0523)	LR 1.250e-04
0: TRAIN [6][210/3880]	Time 0.125 (0.168)	Data 1.41e-04 (1.68e-03)	Tok/s 82225 (87366)	Loss/tok 2.8212 (3.0519)	LR 1.250e-04
0: TRAIN [6][220/3880]	Time 0.187 (0.168)	Data 1.25e-04 (1.61e-03)	Tok/s 90329 (87400)	Loss/tok 3.1045 (3.0514)	LR 1.250e-04
0: TRAIN [6][230/3880]	Time 0.183 (0.168)	Data 1.21e-04 (1.54e-03)	Tok/s 90968 (87451)	Loss/tok 3.2198 (3.0536)	LR 1.250e-04
0: TRAIN [6][240/3880]	Time 0.184 (0.168)	Data 1.37e-04 (1.48e-03)	Tok/s 90929 (87460)	Loss/tok 3.1477 (3.0532)	LR 1.250e-04
0: TRAIN [6][250/3880]	Time 0.123 (0.168)	Data 1.20e-04 (1.43e-03)	Tok/s 85148 (87453)	Loss/tok 2.8258 (3.0537)	LR 1.250e-04
0: TRAIN [6][260/3880]	Time 0.124 (0.167)	Data 1.19e-04 (1.38e-03)	Tok/s 83281 (87432)	Loss/tok 2.9037 (3.0545)	LR 1.250e-04
0: TRAIN [6][270/3880]	Time 0.245 (0.167)	Data 1.19e-04 (1.33e-03)	Tok/s 95460 (87420)	Loss/tok 3.1924 (3.0546)	LR 1.250e-04
0: TRAIN [6][280/3880]	Time 0.182 (0.166)	Data 1.18e-04 (1.29e-03)	Tok/s 92439 (87411)	Loss/tok 3.0519 (3.0545)	LR 1.250e-04
0: TRAIN [6][290/3880]	Time 0.183 (0.165)	Data 1.56e-04 (1.25e-03)	Tok/s 91782 (87299)	Loss/tok 2.9995 (3.0502)	LR 1.250e-04
0: TRAIN [6][300/3880]	Time 0.246 (0.165)	Data 1.38e-04 (1.21e-03)	Tok/s 95592 (87297)	Loss/tok 3.1909 (3.0528)	LR 1.250e-04
0: TRAIN [6][310/3880]	Time 0.126 (0.165)	Data 1.44e-04 (1.18e-03)	Tok/s 83125 (87300)	Loss/tok 2.8688 (3.0505)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][320/3880]	Time 0.068 (0.164)	Data 1.23e-04 (1.14e-03)	Tok/s 78772 (87232)	Loss/tok 2.4879 (3.0495)	LR 1.250e-04
0: TRAIN [6][330/3880]	Time 0.242 (0.165)	Data 1.38e-04 (1.11e-03)	Tok/s 96914 (87336)	Loss/tok 3.1913 (3.0536)	LR 1.250e-04
0: TRAIN [6][340/3880]	Time 0.124 (0.166)	Data 1.32e-04 (1.08e-03)	Tok/s 82539 (87397)	Loss/tok 2.8967 (3.0549)	LR 1.250e-04
0: TRAIN [6][350/3880]	Time 0.181 (0.166)	Data 1.19e-04 (1.06e-03)	Tok/s 92247 (87403)	Loss/tok 3.0166 (3.0540)	LR 1.250e-04
0: TRAIN [6][360/3880]	Time 0.182 (0.166)	Data 1.25e-04 (1.03e-03)	Tok/s 91002 (87456)	Loss/tok 3.0148 (3.0553)	LR 1.250e-04
0: TRAIN [6][370/3880]	Time 0.186 (0.167)	Data 1.22e-04 (1.01e-03)	Tok/s 90746 (87484)	Loss/tok 3.0448 (3.0574)	LR 1.250e-04
0: TRAIN [6][380/3880]	Time 0.185 (0.166)	Data 1.28e-04 (9.84e-04)	Tok/s 91482 (87449)	Loss/tok 3.0987 (3.0567)	LR 1.250e-04
0: TRAIN [6][390/3880]	Time 0.124 (0.166)	Data 1.11e-04 (9.62e-04)	Tok/s 85686 (87494)	Loss/tok 2.9067 (3.0564)	LR 1.250e-04
0: TRAIN [6][400/3880]	Time 0.319 (0.167)	Data 1.09e-04 (9.41e-04)	Tok/s 93437 (87564)	Loss/tok 3.3750 (3.0583)	LR 1.250e-04
0: TRAIN [6][410/3880]	Time 0.181 (0.166)	Data 1.46e-04 (9.21e-04)	Tok/s 92080 (87497)	Loss/tok 3.1463 (3.0561)	LR 1.250e-04
0: TRAIN [6][420/3880]	Time 0.125 (0.165)	Data 1.32e-04 (9.02e-04)	Tok/s 83108 (87459)	Loss/tok 2.8570 (3.0543)	LR 1.250e-04
0: TRAIN [6][430/3880]	Time 0.123 (0.164)	Data 1.06e-04 (8.84e-04)	Tok/s 83656 (87353)	Loss/tok 2.9341 (3.0534)	LR 1.250e-04
0: TRAIN [6][440/3880]	Time 0.124 (0.164)	Data 1.09e-04 (8.66e-04)	Tok/s 84049 (87339)	Loss/tok 2.8753 (3.0520)	LR 1.250e-04
0: TRAIN [6][450/3880]	Time 0.067 (0.164)	Data 1.36e-04 (8.50e-04)	Tok/s 79164 (87341)	Loss/tok 2.5348 (3.0534)	LR 1.250e-04
0: TRAIN [6][460/3880]	Time 0.125 (0.164)	Data 1.30e-04 (8.35e-04)	Tok/s 82917 (87351)	Loss/tok 2.8978 (3.0531)	LR 1.250e-04
0: TRAIN [6][470/3880]	Time 0.125 (0.164)	Data 1.12e-04 (8.19e-04)	Tok/s 80709 (87319)	Loss/tok 2.8668 (3.0522)	LR 1.250e-04
0: TRAIN [6][480/3880]	Time 0.123 (0.163)	Data 1.13e-04 (8.05e-04)	Tok/s 82810 (87241)	Loss/tok 2.9500 (3.0520)	LR 1.250e-04
0: TRAIN [6][490/3880]	Time 0.184 (0.164)	Data 1.45e-04 (7.91e-04)	Tok/s 92220 (87287)	Loss/tok 3.0461 (3.0539)	LR 1.250e-04
0: TRAIN [6][500/3880]	Time 0.244 (0.164)	Data 1.07e-04 (7.77e-04)	Tok/s 95264 (87307)	Loss/tok 3.2367 (3.0540)	LR 1.250e-04
0: TRAIN [6][510/3880]	Time 0.067 (0.164)	Data 1.27e-04 (7.65e-04)	Tok/s 76507 (87286)	Loss/tok 2.4375 (3.0546)	LR 1.250e-04
0: TRAIN [6][520/3880]	Time 0.186 (0.163)	Data 1.15e-04 (7.52e-04)	Tok/s 89590 (87242)	Loss/tok 3.0709 (3.0532)	LR 1.250e-04
0: TRAIN [6][530/3880]	Time 0.183 (0.163)	Data 1.06e-04 (7.40e-04)	Tok/s 92410 (87197)	Loss/tok 3.1546 (3.0515)	LR 1.250e-04
0: TRAIN [6][540/3880]	Time 0.183 (0.163)	Data 1.52e-04 (7.29e-04)	Tok/s 91282 (87188)	Loss/tok 2.9989 (3.0526)	LR 1.250e-04
0: TRAIN [6][550/3880]	Time 0.183 (0.162)	Data 1.14e-04 (7.18e-04)	Tok/s 91692 (87171)	Loss/tok 3.0766 (3.0512)	LR 1.250e-04
0: TRAIN [6][560/3880]	Time 0.245 (0.162)	Data 1.06e-04 (7.07e-04)	Tok/s 94791 (87163)	Loss/tok 3.1577 (3.0507)	LR 1.250e-04
0: TRAIN [6][570/3880]	Time 0.126 (0.162)	Data 1.11e-04 (6.97e-04)	Tok/s 81541 (87181)	Loss/tok 2.8826 (3.0502)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [6][580/3880]	Time 0.123 (0.162)	Data 1.44e-04 (6.87e-04)	Tok/s 83555 (87195)	Loss/tok 2.9128 (3.0498)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][590/3880]	Time 0.319 (0.163)	Data 1.07e-04 (6.77e-04)	Tok/s 93581 (87201)	Loss/tok 3.3902 (3.0504)	LR 1.250e-04
0: TRAIN [6][600/3880]	Time 0.185 (0.163)	Data 1.05e-04 (6.67e-04)	Tok/s 92281 (87219)	Loss/tok 3.0385 (3.0504)	LR 1.250e-04
0: TRAIN [6][610/3880]	Time 0.245 (0.163)	Data 1.09e-04 (6.58e-04)	Tok/s 94476 (87255)	Loss/tok 3.3347 (3.0519)	LR 1.250e-04
0: TRAIN [6][620/3880]	Time 0.125 (0.163)	Data 1.06e-04 (6.49e-04)	Tok/s 81900 (87219)	Loss/tok 2.8273 (3.0504)	LR 1.250e-04
0: TRAIN [6][630/3880]	Time 0.069 (0.163)	Data 1.29e-04 (6.41e-04)	Tok/s 75614 (87245)	Loss/tok 2.4689 (3.0543)	LR 1.250e-04
0: TRAIN [6][640/3880]	Time 0.181 (0.164)	Data 1.08e-04 (6.33e-04)	Tok/s 94044 (87258)	Loss/tok 3.0943 (3.0556)	LR 1.250e-04
0: TRAIN [6][650/3880]	Time 0.184 (0.164)	Data 1.07e-04 (6.25e-04)	Tok/s 90683 (87281)	Loss/tok 3.1308 (3.0560)	LR 1.250e-04
0: TRAIN [6][660/3880]	Time 0.124 (0.164)	Data 1.07e-04 (6.17e-04)	Tok/s 82326 (87287)	Loss/tok 2.8439 (3.0560)	LR 1.250e-04
0: TRAIN [6][670/3880]	Time 0.247 (0.164)	Data 1.08e-04 (6.10e-04)	Tok/s 93174 (87294)	Loss/tok 3.3002 (3.0555)	LR 1.250e-04
0: TRAIN [6][680/3880]	Time 0.246 (0.164)	Data 1.11e-04 (6.02e-04)	Tok/s 95623 (87275)	Loss/tok 3.1045 (3.0549)	LR 1.250e-04
0: TRAIN [6][690/3880]	Time 0.317 (0.165)	Data 1.24e-04 (5.95e-04)	Tok/s 93195 (87330)	Loss/tok 3.4191 (3.0574)	LR 1.250e-04
0: TRAIN [6][700/3880]	Time 0.183 (0.164)	Data 1.15e-04 (5.88e-04)	Tok/s 92387 (87321)	Loss/tok 3.0843 (3.0566)	LR 1.250e-04
0: TRAIN [6][710/3880]	Time 0.123 (0.165)	Data 1.26e-04 (5.82e-04)	Tok/s 82590 (87334)	Loss/tok 2.8022 (3.0575)	LR 1.250e-04
0: TRAIN [6][720/3880]	Time 0.123 (0.164)	Data 1.30e-04 (5.76e-04)	Tok/s 84708 (87306)	Loss/tok 2.9047 (3.0563)	LR 1.250e-04
0: TRAIN [6][730/3880]	Time 0.241 (0.164)	Data 1.02e-04 (5.69e-04)	Tok/s 96589 (87315)	Loss/tok 3.2553 (3.0561)	LR 1.250e-04
0: TRAIN [6][740/3880]	Time 0.185 (0.164)	Data 1.21e-04 (5.63e-04)	Tok/s 90397 (87337)	Loss/tok 3.0727 (3.0564)	LR 1.250e-04
0: TRAIN [6][750/3880]	Time 0.123 (0.164)	Data 1.41e-04 (5.57e-04)	Tok/s 85056 (87346)	Loss/tok 2.8632 (3.0563)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][760/3880]	Time 0.125 (0.164)	Data 1.17e-04 (5.52e-04)	Tok/s 83505 (87333)	Loss/tok 2.9244 (3.0556)	LR 1.250e-04
0: TRAIN [6][770/3880]	Time 0.125 (0.164)	Data 1.06e-04 (5.46e-04)	Tok/s 81662 (87309)	Loss/tok 2.9614 (3.0549)	LR 1.250e-04
0: TRAIN [6][780/3880]	Time 0.185 (0.164)	Data 1.10e-04 (5.40e-04)	Tok/s 89660 (87335)	Loss/tok 2.9942 (3.0555)	LR 1.250e-04
0: TRAIN [6][790/3880]	Time 0.243 (0.164)	Data 1.05e-04 (5.35e-04)	Tok/s 96879 (87334)	Loss/tok 3.0964 (3.0551)	LR 1.250e-04
0: TRAIN [6][800/3880]	Time 0.068 (0.164)	Data 1.50e-04 (5.30e-04)	Tok/s 79771 (87351)	Loss/tok 2.4105 (3.0556)	LR 1.250e-04
0: TRAIN [6][810/3880]	Time 0.066 (0.163)	Data 1.37e-04 (5.25e-04)	Tok/s 78840 (87300)	Loss/tok 2.4729 (3.0541)	LR 1.250e-04
0: TRAIN [6][820/3880]	Time 0.185 (0.164)	Data 1.10e-04 (5.20e-04)	Tok/s 92119 (87315)	Loss/tok 3.0479 (3.0548)	LR 1.250e-04
0: TRAIN [6][830/3880]	Time 0.068 (0.164)	Data 1.21e-04 (5.15e-04)	Tok/s 77479 (87351)	Loss/tok 2.5373 (3.0562)	LR 1.250e-04
0: TRAIN [6][840/3880]	Time 0.126 (0.164)	Data 1.57e-04 (5.11e-04)	Tok/s 82987 (87373)	Loss/tok 2.9136 (3.0575)	LR 1.250e-04
0: TRAIN [6][850/3880]	Time 0.126 (0.164)	Data 1.06e-04 (5.06e-04)	Tok/s 81801 (87371)	Loss/tok 2.8746 (3.0570)	LR 1.250e-04
0: TRAIN [6][860/3880]	Time 0.068 (0.165)	Data 1.15e-04 (5.02e-04)	Tok/s 77564 (87395)	Loss/tok 2.5075 (3.0575)	LR 1.250e-04
0: TRAIN [6][870/3880]	Time 0.124 (0.165)	Data 1.21e-04 (4.97e-04)	Tok/s 81451 (87404)	Loss/tok 2.8173 (3.0572)	LR 1.250e-04
0: TRAIN [6][880/3880]	Time 0.124 (0.164)	Data 1.03e-04 (4.93e-04)	Tok/s 83315 (87391)	Loss/tok 2.7758 (3.0561)	LR 1.250e-04
0: TRAIN [6][890/3880]	Time 0.124 (0.164)	Data 1.03e-04 (4.89e-04)	Tok/s 82679 (87364)	Loss/tok 2.8400 (3.0551)	LR 1.250e-04
0: TRAIN [6][900/3880]	Time 0.125 (0.164)	Data 1.17e-04 (4.85e-04)	Tok/s 84203 (87351)	Loss/tok 2.9126 (3.0543)	LR 1.250e-04
0: TRAIN [6][910/3880]	Time 0.184 (0.164)	Data 1.30e-04 (4.81e-04)	Tok/s 90560 (87345)	Loss/tok 3.1030 (3.0539)	LR 1.250e-04
0: TRAIN [6][920/3880]	Time 0.124 (0.163)	Data 1.35e-04 (4.77e-04)	Tok/s 83258 (87303)	Loss/tok 2.7939 (3.0532)	LR 1.250e-04
0: TRAIN [6][930/3880]	Time 0.184 (0.164)	Data 1.20e-04 (4.73e-04)	Tok/s 92293 (87303)	Loss/tok 3.0075 (3.0532)	LR 1.250e-04
0: TRAIN [6][940/3880]	Time 0.183 (0.164)	Data 1.26e-04 (4.69e-04)	Tok/s 92656 (87345)	Loss/tok 3.0695 (3.0544)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][950/3880]	Time 0.185 (0.164)	Data 1.24e-04 (4.66e-04)	Tok/s 90114 (87347)	Loss/tok 2.9993 (3.0542)	LR 1.250e-04
0: TRAIN [6][960/3880]	Time 0.184 (0.164)	Data 1.34e-04 (4.62e-04)	Tok/s 91171 (87329)	Loss/tok 3.0410 (3.0533)	LR 1.250e-04
0: TRAIN [6][970/3880]	Time 0.125 (0.164)	Data 1.32e-04 (4.59e-04)	Tok/s 84087 (87322)	Loss/tok 2.8634 (3.0527)	LR 1.250e-04
0: TRAIN [6][980/3880]	Time 0.124 (0.163)	Data 1.35e-04 (4.55e-04)	Tok/s 83302 (87296)	Loss/tok 2.8386 (3.0516)	LR 1.250e-04
0: TRAIN [6][990/3880]	Time 0.123 (0.163)	Data 1.39e-04 (4.52e-04)	Tok/s 83506 (87273)	Loss/tok 2.8827 (3.0506)	LR 1.250e-04
0: TRAIN [6][1000/3880]	Time 0.182 (0.163)	Data 1.33e-04 (4.49e-04)	Tok/s 91725 (87253)	Loss/tok 3.1026 (3.0498)	LR 1.250e-04
0: TRAIN [6][1010/3880]	Time 0.127 (0.163)	Data 1.13e-04 (4.46e-04)	Tok/s 82821 (87255)	Loss/tok 2.8214 (3.0499)	LR 1.250e-04
0: TRAIN [6][1020/3880]	Time 0.126 (0.163)	Data 1.25e-04 (4.43e-04)	Tok/s 81596 (87257)	Loss/tok 2.9494 (3.0501)	LR 1.250e-04
0: TRAIN [6][1030/3880]	Time 0.182 (0.163)	Data 1.12e-04 (4.39e-04)	Tok/s 91527 (87256)	Loss/tok 3.0561 (3.0503)	LR 1.250e-04
0: TRAIN [6][1040/3880]	Time 0.318 (0.163)	Data 1.23e-04 (4.36e-04)	Tok/s 93335 (87280)	Loss/tok 3.3264 (3.0509)	LR 1.250e-04
0: TRAIN [6][1050/3880]	Time 0.246 (0.163)	Data 1.22e-04 (4.33e-04)	Tok/s 93733 (87317)	Loss/tok 3.3333 (3.0521)	LR 1.250e-04
0: TRAIN [6][1060/3880]	Time 0.314 (0.164)	Data 1.22e-04 (4.30e-04)	Tok/s 94840 (87348)	Loss/tok 3.3452 (3.0540)	LR 1.250e-04
0: TRAIN [6][1070/3880]	Time 0.246 (0.164)	Data 1.14e-04 (4.28e-04)	Tok/s 94418 (87338)	Loss/tok 3.1339 (3.0534)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][1080/3880]	Time 0.184 (0.164)	Data 1.08e-04 (4.25e-04)	Tok/s 91166 (87324)	Loss/tok 3.0089 (3.0530)	LR 1.250e-04
0: TRAIN [6][1090/3880]	Time 0.125 (0.164)	Data 1.15e-04 (4.22e-04)	Tok/s 83335 (87305)	Loss/tok 2.8324 (3.0532)	LR 1.250e-04
0: TRAIN [6][1100/3880]	Time 0.245 (0.164)	Data 1.19e-04 (4.19e-04)	Tok/s 95536 (87309)	Loss/tok 3.0763 (3.0530)	LR 1.250e-04
0: TRAIN [6][1110/3880]	Time 0.125 (0.164)	Data 1.20e-04 (4.16e-04)	Tok/s 83599 (87326)	Loss/tok 2.8787 (3.0536)	LR 1.250e-04
0: TRAIN [6][1120/3880]	Time 0.124 (0.164)	Data 1.12e-04 (4.14e-04)	Tok/s 82509 (87299)	Loss/tok 2.8513 (3.0525)	LR 1.250e-04
0: TRAIN [6][1130/3880]	Time 0.125 (0.163)	Data 1.13e-04 (4.11e-04)	Tok/s 81639 (87298)	Loss/tok 2.8543 (3.0522)	LR 1.250e-04
0: TRAIN [6][1140/3880]	Time 0.184 (0.163)	Data 1.19e-04 (4.09e-04)	Tok/s 89890 (87275)	Loss/tok 3.1444 (3.0515)	LR 1.250e-04
0: TRAIN [6][1150/3880]	Time 0.124 (0.163)	Data 1.18e-04 (4.06e-04)	Tok/s 83099 (87295)	Loss/tok 2.9394 (3.0512)	LR 1.250e-04
0: TRAIN [6][1160/3880]	Time 0.183 (0.164)	Data 1.60e-04 (4.04e-04)	Tok/s 90792 (87331)	Loss/tok 3.0035 (3.0524)	LR 1.250e-04
0: TRAIN [6][1170/3880]	Time 0.182 (0.164)	Data 1.34e-04 (4.01e-04)	Tok/s 91892 (87343)	Loss/tok 3.1011 (3.0528)	LR 1.250e-04
0: TRAIN [6][1180/3880]	Time 0.067 (0.163)	Data 1.14e-04 (3.99e-04)	Tok/s 79264 (87310)	Loss/tok 2.4667 (3.0518)	LR 1.250e-04
0: TRAIN [6][1190/3880]	Time 0.125 (0.163)	Data 1.18e-04 (3.97e-04)	Tok/s 82337 (87292)	Loss/tok 2.9249 (3.0517)	LR 1.250e-04
0: TRAIN [6][1200/3880]	Time 0.244 (0.163)	Data 1.13e-04 (3.94e-04)	Tok/s 95256 (87316)	Loss/tok 3.2117 (3.0523)	LR 1.250e-04
0: TRAIN [6][1210/3880]	Time 0.186 (0.163)	Data 1.34e-04 (3.92e-04)	Tok/s 90116 (87335)	Loss/tok 2.9473 (3.0522)	LR 1.250e-04
0: TRAIN [6][1220/3880]	Time 0.124 (0.164)	Data 1.17e-04 (3.90e-04)	Tok/s 83368 (87357)	Loss/tok 2.9267 (3.0520)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][1230/3880]	Time 0.123 (0.164)	Data 1.24e-04 (3.88e-04)	Tok/s 86352 (87356)	Loss/tok 2.8817 (3.0523)	LR 1.250e-04
0: TRAIN [6][1240/3880]	Time 0.181 (0.164)	Data 1.14e-04 (3.85e-04)	Tok/s 91974 (87366)	Loss/tok 3.1004 (3.0526)	LR 1.250e-04
0: TRAIN [6][1250/3880]	Time 0.245 (0.164)	Data 1.04e-04 (3.83e-04)	Tok/s 94480 (87382)	Loss/tok 3.2998 (3.0533)	LR 1.250e-04
0: TRAIN [6][1260/3880]	Time 0.124 (0.164)	Data 1.08e-04 (3.81e-04)	Tok/s 83872 (87370)	Loss/tok 2.9073 (3.0529)	LR 1.250e-04
0: TRAIN [6][1270/3880]	Time 0.244 (0.164)	Data 1.24e-04 (3.79e-04)	Tok/s 96774 (87369)	Loss/tok 3.2358 (3.0530)	LR 1.250e-04
0: TRAIN [6][1280/3880]	Time 0.182 (0.164)	Data 1.12e-04 (3.77e-04)	Tok/s 93121 (87381)	Loss/tok 3.0585 (3.0536)	LR 1.250e-04
0: TRAIN [6][1290/3880]	Time 0.068 (0.164)	Data 1.35e-04 (3.75e-04)	Tok/s 78206 (87381)	Loss/tok 2.5354 (3.0535)	LR 1.250e-04
0: TRAIN [6][1300/3880]	Time 0.182 (0.164)	Data 1.42e-04 (3.73e-04)	Tok/s 91716 (87372)	Loss/tok 3.1300 (3.0534)	LR 1.250e-04
0: TRAIN [6][1310/3880]	Time 0.124 (0.163)	Data 1.09e-04 (3.71e-04)	Tok/s 82834 (87363)	Loss/tok 2.8940 (3.0530)	LR 1.250e-04
0: TRAIN [6][1320/3880]	Time 0.124 (0.163)	Data 1.34e-04 (3.69e-04)	Tok/s 82841 (87355)	Loss/tok 2.9237 (3.0523)	LR 1.250e-04
0: TRAIN [6][1330/3880]	Time 0.185 (0.163)	Data 1.24e-04 (3.67e-04)	Tok/s 90568 (87363)	Loss/tok 3.0979 (3.0528)	LR 1.250e-04
0: TRAIN [6][1340/3880]	Time 0.242 (0.163)	Data 1.25e-04 (3.66e-04)	Tok/s 97696 (87362)	Loss/tok 3.1431 (3.0530)	LR 1.250e-04
0: TRAIN [6][1350/3880]	Time 0.183 (0.163)	Data 1.24e-04 (3.64e-04)	Tok/s 91138 (87369)	Loss/tok 3.0286 (3.0529)	LR 1.250e-04
0: TRAIN [6][1360/3880]	Time 0.067 (0.163)	Data 1.23e-04 (3.62e-04)	Tok/s 79411 (87360)	Loss/tok 2.4250 (3.0525)	LR 1.250e-04
0: TRAIN [6][1370/3880]	Time 0.068 (0.163)	Data 1.08e-04 (3.60e-04)	Tok/s 78447 (87353)	Loss/tok 2.5103 (3.0521)	LR 1.250e-04
0: TRAIN [6][1380/3880]	Time 0.181 (0.163)	Data 1.65e-04 (3.59e-04)	Tok/s 93051 (87360)	Loss/tok 3.0425 (3.0522)	LR 1.250e-04
0: TRAIN [6][1390/3880]	Time 0.125 (0.163)	Data 1.06e-04 (3.57e-04)	Tok/s 84486 (87361)	Loss/tok 2.8631 (3.0522)	LR 1.250e-04
0: TRAIN [6][1400/3880]	Time 0.316 (0.163)	Data 1.08e-04 (3.55e-04)	Tok/s 93377 (87365)	Loss/tok 3.3607 (3.0524)	LR 1.250e-04
0: TRAIN [6][1410/3880]	Time 0.183 (0.163)	Data 1.40e-04 (3.53e-04)	Tok/s 91533 (87368)	Loss/tok 3.1472 (3.0524)	LR 1.250e-04
0: TRAIN [6][1420/3880]	Time 0.124 (0.163)	Data 1.04e-04 (3.52e-04)	Tok/s 83328 (87363)	Loss/tok 2.8003 (3.0528)	LR 1.250e-04
0: TRAIN [6][1430/3880]	Time 0.125 (0.163)	Data 1.38e-04 (3.50e-04)	Tok/s 83722 (87342)	Loss/tok 2.8349 (3.0520)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][1440/3880]	Time 0.126 (0.163)	Data 1.05e-04 (3.49e-04)	Tok/s 83841 (87355)	Loss/tok 2.8662 (3.0522)	LR 1.250e-04
0: TRAIN [6][1450/3880]	Time 0.069 (0.163)	Data 1.04e-04 (3.47e-04)	Tok/s 76886 (87358)	Loss/tok 2.4898 (3.0523)	LR 1.250e-04
0: TRAIN [6][1460/3880]	Time 0.184 (0.163)	Data 1.29e-04 (3.45e-04)	Tok/s 90888 (87360)	Loss/tok 3.1220 (3.0526)	LR 1.250e-04
0: TRAIN [6][1470/3880]	Time 0.125 (0.163)	Data 1.21e-04 (3.44e-04)	Tok/s 82621 (87363)	Loss/tok 2.8635 (3.0525)	LR 1.250e-04
0: TRAIN [6][1480/3880]	Time 0.125 (0.163)	Data 1.11e-04 (3.42e-04)	Tok/s 82517 (87360)	Loss/tok 3.0100 (3.0522)	LR 1.250e-04
0: TRAIN [6][1490/3880]	Time 0.184 (0.163)	Data 1.25e-04 (3.41e-04)	Tok/s 90913 (87368)	Loss/tok 3.0136 (3.0519)	LR 1.250e-04
0: TRAIN [6][1500/3880]	Time 0.124 (0.163)	Data 1.26e-04 (3.39e-04)	Tok/s 84761 (87366)	Loss/tok 2.8380 (3.0517)	LR 1.250e-04
0: TRAIN [6][1510/3880]	Time 0.069 (0.163)	Data 1.32e-04 (3.38e-04)	Tok/s 75398 (87357)	Loss/tok 2.5596 (3.0514)	LR 1.250e-04
0: TRAIN [6][1520/3880]	Time 0.124 (0.163)	Data 1.34e-04 (3.37e-04)	Tok/s 82834 (87362)	Loss/tok 2.9248 (3.0518)	LR 1.250e-04
0: TRAIN [6][1530/3880]	Time 0.126 (0.163)	Data 1.24e-04 (3.35e-04)	Tok/s 83494 (87349)	Loss/tok 2.8732 (3.0515)	LR 1.250e-04
0: TRAIN [6][1540/3880]	Time 0.183 (0.163)	Data 1.06e-04 (3.34e-04)	Tok/s 92309 (87339)	Loss/tok 2.9316 (3.0509)	LR 1.250e-04
0: TRAIN [6][1550/3880]	Time 0.184 (0.163)	Data 1.24e-04 (3.32e-04)	Tok/s 90665 (87350)	Loss/tok 3.1046 (3.0515)	LR 1.250e-04
0: TRAIN [6][1560/3880]	Time 0.183 (0.163)	Data 1.13e-04 (3.31e-04)	Tok/s 92928 (87365)	Loss/tok 3.1139 (3.0519)	LR 1.250e-04
0: TRAIN [6][1570/3880]	Time 0.124 (0.163)	Data 1.25e-04 (3.30e-04)	Tok/s 81249 (87349)	Loss/tok 2.8950 (3.0518)	LR 1.250e-04
0: TRAIN [6][1580/3880]	Time 0.125 (0.163)	Data 1.22e-04 (3.28e-04)	Tok/s 82473 (87332)	Loss/tok 2.8403 (3.0513)	LR 1.250e-04
0: TRAIN [6][1590/3880]	Time 0.123 (0.163)	Data 1.06e-04 (3.27e-04)	Tok/s 82423 (87333)	Loss/tok 2.8658 (3.0513)	LR 1.250e-04
0: TRAIN [6][1600/3880]	Time 0.183 (0.163)	Data 1.17e-04 (3.26e-04)	Tok/s 91898 (87316)	Loss/tok 3.0359 (3.0509)	LR 1.250e-04
0: TRAIN [6][1610/3880]	Time 0.185 (0.163)	Data 1.27e-04 (3.25e-04)	Tok/s 91113 (87326)	Loss/tok 3.0605 (3.0512)	LR 1.250e-04
0: TRAIN [6][1620/3880]	Time 0.068 (0.163)	Data 1.04e-04 (3.23e-04)	Tok/s 77480 (87322)	Loss/tok 2.3792 (3.0514)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][1630/3880]	Time 0.184 (0.163)	Data 1.33e-04 (3.22e-04)	Tok/s 91755 (87323)	Loss/tok 3.1437 (3.0520)	LR 1.250e-04
0: TRAIN [6][1640/3880]	Time 0.067 (0.163)	Data 1.25e-04 (3.21e-04)	Tok/s 79130 (87316)	Loss/tok 2.4818 (3.0520)	LR 1.250e-04
0: TRAIN [6][1650/3880]	Time 0.125 (0.163)	Data 1.33e-04 (3.20e-04)	Tok/s 82450 (87302)	Loss/tok 2.9452 (3.0517)	LR 1.250e-04
0: TRAIN [6][1660/3880]	Time 0.067 (0.163)	Data 1.12e-04 (3.18e-04)	Tok/s 78833 (87290)	Loss/tok 2.5371 (3.0517)	LR 1.250e-04
0: TRAIN [6][1670/3880]	Time 0.126 (0.163)	Data 1.19e-04 (3.17e-04)	Tok/s 80441 (87294)	Loss/tok 2.9252 (3.0521)	LR 1.250e-04
0: TRAIN [6][1680/3880]	Time 0.181 (0.163)	Data 1.31e-04 (3.16e-04)	Tok/s 92013 (87317)	Loss/tok 3.0865 (3.0523)	LR 1.250e-04
0: TRAIN [6][1690/3880]	Time 0.123 (0.163)	Data 1.49e-04 (3.15e-04)	Tok/s 83077 (87311)	Loss/tok 2.9000 (3.0520)	LR 1.250e-04
0: TRAIN [6][1700/3880]	Time 0.315 (0.163)	Data 1.22e-04 (3.14e-04)	Tok/s 93708 (87320)	Loss/tok 3.3420 (3.0522)	LR 1.250e-04
0: TRAIN [6][1710/3880]	Time 0.067 (0.163)	Data 1.17e-04 (3.13e-04)	Tok/s 76393 (87309)	Loss/tok 2.4575 (3.0520)	LR 1.250e-04
0: TRAIN [6][1720/3880]	Time 0.125 (0.163)	Data 1.57e-04 (3.12e-04)	Tok/s 82712 (87293)	Loss/tok 2.7473 (3.0522)	LR 1.250e-04
0: TRAIN [6][1730/3880]	Time 0.243 (0.163)	Data 1.28e-04 (3.11e-04)	Tok/s 95199 (87292)	Loss/tok 3.3568 (3.0521)	LR 1.250e-04
0: TRAIN [6][1740/3880]	Time 0.125 (0.163)	Data 1.18e-04 (3.10e-04)	Tok/s 82128 (87303)	Loss/tok 2.9354 (3.0523)	LR 1.250e-04
0: TRAIN [6][1750/3880]	Time 0.182 (0.163)	Data 1.44e-04 (3.08e-04)	Tok/s 91368 (87305)	Loss/tok 3.0602 (3.0524)	LR 1.250e-04
0: TRAIN [6][1760/3880]	Time 0.124 (0.163)	Data 1.01e-04 (3.07e-04)	Tok/s 81196 (87294)	Loss/tok 2.9521 (3.0524)	LR 1.250e-04
0: TRAIN [6][1770/3880]	Time 0.183 (0.163)	Data 1.51e-04 (3.06e-04)	Tok/s 90459 (87281)	Loss/tok 3.0962 (3.0519)	LR 1.250e-04
0: TRAIN [6][1780/3880]	Time 0.187 (0.163)	Data 1.31e-04 (3.05e-04)	Tok/s 90789 (87284)	Loss/tok 2.9292 (3.0517)	LR 1.250e-04
0: TRAIN [6][1790/3880]	Time 0.186 (0.163)	Data 1.21e-04 (3.04e-04)	Tok/s 90882 (87300)	Loss/tok 2.9917 (3.0524)	LR 1.250e-04
0: TRAIN [6][1800/3880]	Time 0.317 (0.163)	Data 9.97e-05 (3.03e-04)	Tok/s 93938 (87303)	Loss/tok 3.3302 (3.0533)	LR 1.250e-04
0: TRAIN [6][1810/3880]	Time 0.181 (0.163)	Data 1.09e-04 (3.02e-04)	Tok/s 93064 (87296)	Loss/tok 2.9999 (3.0530)	LR 1.250e-04
0: TRAIN [6][1820/3880]	Time 0.182 (0.163)	Data 1.18e-04 (3.01e-04)	Tok/s 93128 (87303)	Loss/tok 2.9520 (3.0532)	LR 1.250e-04
0: TRAIN [6][1830/3880]	Time 0.128 (0.163)	Data 1.11e-04 (3.00e-04)	Tok/s 80528 (87308)	Loss/tok 2.8657 (3.0537)	LR 1.250e-04
0: TRAIN [6][1840/3880]	Time 0.068 (0.163)	Data 1.06e-04 (2.99e-04)	Tok/s 76806 (87292)	Loss/tok 2.5323 (3.0532)	LR 1.250e-04
0: TRAIN [6][1850/3880]	Time 0.127 (0.163)	Data 1.16e-04 (2.98e-04)	Tok/s 81145 (87288)	Loss/tok 2.8084 (3.0528)	LR 1.250e-04
0: TRAIN [6][1860/3880]	Time 0.129 (0.163)	Data 1.28e-04 (2.97e-04)	Tok/s 80475 (87294)	Loss/tok 2.7686 (3.0530)	LR 1.250e-04
0: TRAIN [6][1870/3880]	Time 0.125 (0.163)	Data 1.42e-04 (2.96e-04)	Tok/s 83506 (87290)	Loss/tok 2.7842 (3.0529)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [6][1880/3880]	Time 0.181 (0.163)	Data 1.22e-04 (2.95e-04)	Tok/s 94480 (87284)	Loss/tok 3.0798 (3.0528)	LR 1.250e-04
0: TRAIN [6][1890/3880]	Time 0.245 (0.163)	Data 1.04e-04 (2.94e-04)	Tok/s 95248 (87292)	Loss/tok 3.3031 (3.0527)	LR 1.250e-04
0: TRAIN [6][1900/3880]	Time 0.183 (0.163)	Data 1.18e-04 (2.93e-04)	Tok/s 92436 (87301)	Loss/tok 3.0852 (3.0528)	LR 1.250e-04
0: TRAIN [6][1910/3880]	Time 0.067 (0.163)	Data 1.06e-04 (2.92e-04)	Tok/s 77574 (87292)	Loss/tok 2.3817 (3.0526)	LR 1.250e-04
0: TRAIN [6][1920/3880]	Time 0.248 (0.163)	Data 1.09e-04 (2.91e-04)	Tok/s 92997 (87296)	Loss/tok 3.2584 (3.0527)	LR 1.250e-04
0: TRAIN [6][1930/3880]	Time 0.067 (0.163)	Data 1.24e-04 (2.90e-04)	Tok/s 78960 (87299)	Loss/tok 2.4977 (3.0529)	LR 1.250e-04
0: TRAIN [6][1940/3880]	Time 0.124 (0.163)	Data 1.28e-04 (2.89e-04)	Tok/s 83335 (87300)	Loss/tok 2.8543 (3.0530)	LR 1.250e-04
0: TRAIN [6][1950/3880]	Time 0.125 (0.163)	Data 1.03e-04 (2.88e-04)	Tok/s 82644 (87298)	Loss/tok 2.9049 (3.0530)	LR 1.250e-04
0: TRAIN [6][1960/3880]	Time 0.181 (0.163)	Data 9.73e-05 (2.88e-04)	Tok/s 91621 (87296)	Loss/tok 3.1097 (3.0530)	LR 1.250e-04
0: TRAIN [6][1970/3880]	Time 0.126 (0.163)	Data 1.20e-04 (2.87e-04)	Tok/s 79680 (87296)	Loss/tok 2.8958 (3.0528)	LR 1.250e-04
0: TRAIN [6][1980/3880]	Time 0.184 (0.163)	Data 1.33e-04 (2.86e-04)	Tok/s 91873 (87285)	Loss/tok 3.0955 (3.0524)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][1990/3880]	Time 0.244 (0.163)	Data 1.06e-04 (2.85e-04)	Tok/s 94108 (87285)	Loss/tok 3.3181 (3.0528)	LR 1.250e-04
0: TRAIN [6][2000/3880]	Time 0.181 (0.163)	Data 1.01e-04 (2.84e-04)	Tok/s 92977 (87293)	Loss/tok 3.0849 (3.0532)	LR 1.250e-04
0: TRAIN [6][2010/3880]	Time 0.181 (0.163)	Data 1.02e-04 (2.83e-04)	Tok/s 91931 (87297)	Loss/tok 3.0602 (3.0535)	LR 1.250e-04
0: TRAIN [6][2020/3880]	Time 0.126 (0.163)	Data 1.05e-04 (2.82e-04)	Tok/s 81596 (87307)	Loss/tok 2.8642 (3.0537)	LR 1.250e-04
0: TRAIN [6][2030/3880]	Time 0.184 (0.163)	Data 1.06e-04 (2.82e-04)	Tok/s 91905 (87306)	Loss/tok 3.0010 (3.0536)	LR 1.250e-04
0: TRAIN [6][2040/3880]	Time 0.182 (0.163)	Data 1.04e-04 (2.81e-04)	Tok/s 92200 (87310)	Loss/tok 3.0385 (3.0534)	LR 1.250e-04
0: TRAIN [6][2050/3880]	Time 0.068 (0.163)	Data 9.89e-05 (2.80e-04)	Tok/s 77897 (87305)	Loss/tok 2.4890 (3.0530)	LR 1.250e-04
0: TRAIN [6][2060/3880]	Time 0.125 (0.163)	Data 1.09e-04 (2.79e-04)	Tok/s 84315 (87319)	Loss/tok 2.7947 (3.0535)	LR 1.250e-04
0: TRAIN [6][2070/3880]	Time 0.182 (0.163)	Data 1.08e-04 (2.78e-04)	Tok/s 93182 (87316)	Loss/tok 3.1272 (3.0531)	LR 1.250e-04
0: TRAIN [6][2080/3880]	Time 0.182 (0.163)	Data 1.02e-04 (2.77e-04)	Tok/s 92608 (87320)	Loss/tok 2.9900 (3.0530)	LR 1.250e-04
0: TRAIN [6][2090/3880]	Time 0.183 (0.163)	Data 1.13e-04 (2.77e-04)	Tok/s 91357 (87319)	Loss/tok 3.0525 (3.0531)	LR 1.250e-04
0: TRAIN [6][2100/3880]	Time 0.246 (0.163)	Data 1.24e-04 (2.76e-04)	Tok/s 95773 (87336)	Loss/tok 3.1844 (3.0538)	LR 1.250e-04
0: TRAIN [6][2110/3880]	Time 0.124 (0.163)	Data 1.41e-04 (2.75e-04)	Tok/s 83001 (87327)	Loss/tok 2.8666 (3.0536)	LR 1.250e-04
0: TRAIN [6][2120/3880]	Time 0.124 (0.163)	Data 1.26e-04 (2.75e-04)	Tok/s 82493 (87335)	Loss/tok 2.8422 (3.0541)	LR 1.250e-04
0: TRAIN [6][2130/3880]	Time 0.184 (0.164)	Data 1.52e-04 (2.74e-04)	Tok/s 91178 (87353)	Loss/tok 3.1113 (3.0549)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][2140/3880]	Time 0.121 (0.164)	Data 1.22e-04 (2.73e-04)	Tok/s 85678 (87349)	Loss/tok 2.8217 (3.0549)	LR 1.250e-04
0: TRAIN [6][2150/3880]	Time 0.124 (0.164)	Data 1.08e-04 (2.72e-04)	Tok/s 83525 (87345)	Loss/tok 2.8711 (3.0548)	LR 1.250e-04
0: TRAIN [6][2160/3880]	Time 0.124 (0.164)	Data 1.14e-04 (2.72e-04)	Tok/s 83756 (87340)	Loss/tok 2.9116 (3.0548)	LR 1.250e-04
0: TRAIN [6][2170/3880]	Time 0.185 (0.164)	Data 1.33e-04 (2.71e-04)	Tok/s 90029 (87347)	Loss/tok 3.0281 (3.0547)	LR 1.250e-04
0: TRAIN [6][2180/3880]	Time 0.247 (0.164)	Data 1.10e-04 (2.70e-04)	Tok/s 96267 (87369)	Loss/tok 3.2361 (3.0552)	LR 1.250e-04
0: TRAIN [6][2190/3880]	Time 0.183 (0.164)	Data 1.15e-04 (2.70e-04)	Tok/s 92522 (87373)	Loss/tok 3.1583 (3.0551)	LR 1.250e-04
0: TRAIN [6][2200/3880]	Time 0.067 (0.164)	Data 1.08e-04 (2.69e-04)	Tok/s 78306 (87377)	Loss/tok 2.4894 (3.0556)	LR 1.250e-04
0: TRAIN [6][2210/3880]	Time 0.125 (0.164)	Data 1.12e-04 (2.68e-04)	Tok/s 82391 (87373)	Loss/tok 2.8381 (3.0553)	LR 1.250e-04
0: TRAIN [6][2220/3880]	Time 0.320 (0.164)	Data 1.26e-04 (2.68e-04)	Tok/s 94146 (87366)	Loss/tok 3.4079 (3.0553)	LR 1.250e-04
0: TRAIN [6][2230/3880]	Time 0.126 (0.164)	Data 1.15e-04 (2.67e-04)	Tok/s 83356 (87354)	Loss/tok 2.8205 (3.0548)	LR 1.250e-04
0: TRAIN [6][2240/3880]	Time 0.125 (0.164)	Data 1.12e-04 (2.66e-04)	Tok/s 83832 (87368)	Loss/tok 2.8668 (3.0549)	LR 1.250e-04
0: TRAIN [6][2250/3880]	Time 0.127 (0.164)	Data 1.28e-04 (2.66e-04)	Tok/s 82865 (87362)	Loss/tok 2.8422 (3.0546)	LR 1.250e-04
0: TRAIN [6][2260/3880]	Time 0.187 (0.164)	Data 1.29e-04 (2.65e-04)	Tok/s 89330 (87372)	Loss/tok 3.0997 (3.0549)	LR 1.250e-04
0: TRAIN [6][2270/3880]	Time 0.068 (0.164)	Data 1.10e-04 (2.64e-04)	Tok/s 77140 (87360)	Loss/tok 2.4613 (3.0545)	LR 1.250e-04
0: TRAIN [6][2280/3880]	Time 0.123 (0.164)	Data 1.10e-04 (2.64e-04)	Tok/s 82004 (87360)	Loss/tok 2.8779 (3.0543)	LR 1.250e-04
0: TRAIN [6][2290/3880]	Time 0.125 (0.164)	Data 1.16e-04 (2.63e-04)	Tok/s 82886 (87368)	Loss/tok 2.7648 (3.0545)	LR 1.250e-04
0: TRAIN [6][2300/3880]	Time 0.181 (0.164)	Data 1.11e-04 (2.63e-04)	Tok/s 91934 (87382)	Loss/tok 3.1707 (3.0549)	LR 1.250e-04
0: TRAIN [6][2310/3880]	Time 0.244 (0.164)	Data 1.12e-04 (2.62e-04)	Tok/s 94366 (87380)	Loss/tok 3.1870 (3.0548)	LR 1.250e-04
0: TRAIN [6][2320/3880]	Time 0.183 (0.164)	Data 1.07e-04 (2.61e-04)	Tok/s 93146 (87370)	Loss/tok 3.1301 (3.0545)	LR 1.250e-04
0: TRAIN [6][2330/3880]	Time 0.124 (0.164)	Data 1.26e-04 (2.61e-04)	Tok/s 85194 (87375)	Loss/tok 2.9109 (3.0545)	LR 1.250e-04
0: TRAIN [6][2340/3880]	Time 0.124 (0.164)	Data 1.39e-04 (2.60e-04)	Tok/s 83656 (87373)	Loss/tok 2.8918 (3.0544)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][2350/3880]	Time 0.185 (0.164)	Data 1.34e-04 (2.59e-04)	Tok/s 89456 (87377)	Loss/tok 3.1031 (3.0549)	LR 1.250e-04
0: TRAIN [6][2360/3880]	Time 0.125 (0.164)	Data 1.27e-04 (2.59e-04)	Tok/s 82238 (87363)	Loss/tok 2.8843 (3.0548)	LR 1.250e-04
0: TRAIN [6][2370/3880]	Time 0.128 (0.164)	Data 1.34e-04 (2.58e-04)	Tok/s 81004 (87353)	Loss/tok 2.9324 (3.0546)	LR 1.250e-04
0: TRAIN [6][2380/3880]	Time 0.184 (0.164)	Data 1.34e-04 (2.58e-04)	Tok/s 91601 (87347)	Loss/tok 2.9350 (3.0543)	LR 1.250e-04
0: TRAIN [6][2390/3880]	Time 0.244 (0.163)	Data 2.08e-04 (2.57e-04)	Tok/s 95363 (87339)	Loss/tok 3.1658 (3.0540)	LR 1.250e-04
0: TRAIN [6][2400/3880]	Time 0.184 (0.164)	Data 1.31e-04 (2.57e-04)	Tok/s 91858 (87351)	Loss/tok 3.0744 (3.0542)	LR 1.250e-04
0: TRAIN [6][2410/3880]	Time 0.128 (0.164)	Data 1.39e-04 (2.56e-04)	Tok/s 80590 (87355)	Loss/tok 2.8257 (3.0547)	LR 1.250e-04
0: TRAIN [6][2420/3880]	Time 0.126 (0.164)	Data 1.16e-04 (2.56e-04)	Tok/s 82230 (87346)	Loss/tok 2.9167 (3.0546)	LR 1.250e-04
0: TRAIN [6][2430/3880]	Time 0.125 (0.163)	Data 1.09e-04 (2.55e-04)	Tok/s 81723 (87335)	Loss/tok 2.8381 (3.0542)	LR 1.250e-04
0: TRAIN [6][2440/3880]	Time 0.124 (0.163)	Data 1.33e-04 (2.55e-04)	Tok/s 84719 (87334)	Loss/tok 2.9442 (3.0545)	LR 1.250e-04
0: TRAIN [6][2450/3880]	Time 0.125 (0.163)	Data 1.44e-04 (2.54e-04)	Tok/s 84629 (87328)	Loss/tok 2.8773 (3.0543)	LR 1.250e-04
0: TRAIN [6][2460/3880]	Time 0.183 (0.163)	Data 1.31e-04 (2.53e-04)	Tok/s 91545 (87324)	Loss/tok 3.0337 (3.0540)	LR 1.250e-04
0: TRAIN [6][2470/3880]	Time 0.182 (0.163)	Data 1.10e-04 (2.53e-04)	Tok/s 91227 (87322)	Loss/tok 2.9729 (3.0539)	LR 1.250e-04
0: TRAIN [6][2480/3880]	Time 0.243 (0.163)	Data 1.22e-04 (2.52e-04)	Tok/s 95740 (87318)	Loss/tok 3.2632 (3.0539)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][2490/3880]	Time 0.245 (0.163)	Data 1.18e-04 (2.52e-04)	Tok/s 96281 (87322)	Loss/tok 3.0690 (3.0541)	LR 1.250e-04
0: TRAIN [6][2500/3880]	Time 0.126 (0.163)	Data 1.29e-04 (2.51e-04)	Tok/s 82328 (87324)	Loss/tok 2.8904 (3.0542)	LR 1.250e-04
0: TRAIN [6][2510/3880]	Time 0.186 (0.163)	Data 1.25e-04 (2.51e-04)	Tok/s 89799 (87325)	Loss/tok 3.0734 (3.0543)	LR 1.250e-04
0: TRAIN [6][2520/3880]	Time 0.126 (0.163)	Data 1.23e-04 (2.50e-04)	Tok/s 82302 (87322)	Loss/tok 2.9257 (3.0540)	LR 1.250e-04
0: TRAIN [6][2530/3880]	Time 0.124 (0.163)	Data 1.23e-04 (2.50e-04)	Tok/s 82855 (87300)	Loss/tok 2.9149 (3.0535)	LR 1.250e-04
0: TRAIN [6][2540/3880]	Time 0.185 (0.163)	Data 1.35e-04 (2.49e-04)	Tok/s 91308 (87299)	Loss/tok 2.9594 (3.0533)	LR 1.250e-04
0: TRAIN [6][2550/3880]	Time 0.184 (0.163)	Data 1.20e-04 (2.49e-04)	Tok/s 91901 (87313)	Loss/tok 2.9596 (3.0537)	LR 1.250e-04
0: TRAIN [6][2560/3880]	Time 0.182 (0.163)	Data 1.22e-04 (2.48e-04)	Tok/s 93247 (87318)	Loss/tok 3.0506 (3.0539)	LR 1.250e-04
0: TRAIN [6][2570/3880]	Time 0.124 (0.163)	Data 1.16e-04 (2.48e-04)	Tok/s 83092 (87320)	Loss/tok 2.8222 (3.0541)	LR 1.250e-04
0: TRAIN [6][2580/3880]	Time 0.184 (0.163)	Data 1.26e-04 (2.47e-04)	Tok/s 91820 (87315)	Loss/tok 2.8999 (3.0544)	LR 1.250e-04
0: TRAIN [6][2590/3880]	Time 0.316 (0.163)	Data 1.35e-04 (2.47e-04)	Tok/s 93890 (87316)	Loss/tok 3.5555 (3.0547)	LR 1.250e-04
0: TRAIN [6][2600/3880]	Time 0.183 (0.163)	Data 1.23e-04 (2.46e-04)	Tok/s 92137 (87315)	Loss/tok 3.0278 (3.0549)	LR 1.250e-04
0: TRAIN [6][2610/3880]	Time 0.182 (0.163)	Data 1.38e-04 (2.46e-04)	Tok/s 90661 (87314)	Loss/tok 3.1022 (3.0549)	LR 1.250e-04
0: TRAIN [6][2620/3880]	Time 0.182 (0.163)	Data 1.13e-04 (2.46e-04)	Tok/s 91358 (87320)	Loss/tok 3.1005 (3.0551)	LR 1.250e-04
0: TRAIN [6][2630/3880]	Time 0.315 (0.164)	Data 1.34e-04 (2.45e-04)	Tok/s 92760 (87327)	Loss/tok 3.4400 (3.0555)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][2640/3880]	Time 0.244 (0.164)	Data 1.30e-04 (2.45e-04)	Tok/s 97087 (87338)	Loss/tok 3.2658 (3.0559)	LR 1.250e-04
0: TRAIN [6][2650/3880]	Time 0.243 (0.164)	Data 1.11e-04 (2.44e-04)	Tok/s 95682 (87344)	Loss/tok 3.2609 (3.0564)	LR 1.250e-04
0: TRAIN [6][2660/3880]	Time 0.124 (0.164)	Data 1.26e-04 (2.44e-04)	Tok/s 86384 (87339)	Loss/tok 2.8416 (3.0562)	LR 1.250e-04
0: TRAIN [6][2670/3880]	Time 0.183 (0.164)	Data 1.34e-04 (2.43e-04)	Tok/s 93304 (87345)	Loss/tok 2.9572 (3.0568)	LR 1.250e-04
0: TRAIN [6][2680/3880]	Time 0.126 (0.164)	Data 1.35e-04 (2.43e-04)	Tok/s 82203 (87344)	Loss/tok 2.8450 (3.0568)	LR 1.250e-04
0: TRAIN [6][2690/3880]	Time 0.124 (0.164)	Data 1.33e-04 (2.42e-04)	Tok/s 84639 (87345)	Loss/tok 2.8979 (3.0566)	LR 1.250e-04
0: TRAIN [6][2700/3880]	Time 0.124 (0.164)	Data 1.34e-04 (2.42e-04)	Tok/s 83222 (87347)	Loss/tok 2.8796 (3.0567)	LR 1.250e-04
0: TRAIN [6][2710/3880]	Time 0.183 (0.164)	Data 1.29e-04 (2.42e-04)	Tok/s 90928 (87347)	Loss/tok 3.2263 (3.0566)	LR 1.250e-04
0: TRAIN [6][2720/3880]	Time 0.067 (0.164)	Data 1.34e-04 (2.41e-04)	Tok/s 79645 (87342)	Loss/tok 2.4210 (3.0564)	LR 1.250e-04
0: TRAIN [6][2730/3880]	Time 0.183 (0.164)	Data 1.23e-04 (2.41e-04)	Tok/s 90901 (87339)	Loss/tok 3.0528 (3.0563)	LR 1.250e-04
0: TRAIN [6][2740/3880]	Time 0.123 (0.164)	Data 1.22e-04 (2.40e-04)	Tok/s 84119 (87337)	Loss/tok 2.7511 (3.0561)	LR 1.250e-04
0: TRAIN [6][2750/3880]	Time 0.124 (0.164)	Data 1.25e-04 (2.40e-04)	Tok/s 82833 (87347)	Loss/tok 2.8638 (3.0563)	LR 1.250e-04
0: TRAIN [6][2760/3880]	Time 0.182 (0.164)	Data 1.45e-04 (2.40e-04)	Tok/s 93744 (87343)	Loss/tok 3.0071 (3.0562)	LR 1.250e-04
0: TRAIN [6][2770/3880]	Time 0.246 (0.164)	Data 1.25e-04 (2.39e-04)	Tok/s 93790 (87342)	Loss/tok 3.2252 (3.0561)	LR 1.250e-04
0: TRAIN [6][2780/3880]	Time 0.184 (0.164)	Data 1.31e-04 (2.39e-04)	Tok/s 92778 (87337)	Loss/tok 2.9715 (3.0558)	LR 1.250e-04
0: TRAIN [6][2790/3880]	Time 0.125 (0.164)	Data 1.21e-04 (2.38e-04)	Tok/s 81737 (87329)	Loss/tok 2.9011 (3.0558)	LR 1.250e-04
0: TRAIN [6][2800/3880]	Time 0.184 (0.164)	Data 1.43e-04 (2.38e-04)	Tok/s 92240 (87327)	Loss/tok 2.9624 (3.0558)	LR 1.250e-04
0: TRAIN [6][2810/3880]	Time 0.125 (0.164)	Data 1.20e-04 (2.38e-04)	Tok/s 82797 (87335)	Loss/tok 2.8189 (3.0560)	LR 1.250e-04
0: TRAIN [6][2820/3880]	Time 0.243 (0.164)	Data 1.30e-04 (2.37e-04)	Tok/s 96006 (87338)	Loss/tok 3.1881 (3.0559)	LR 1.250e-04
0: TRAIN [6][2830/3880]	Time 0.124 (0.164)	Data 1.14e-04 (2.37e-04)	Tok/s 81677 (87335)	Loss/tok 2.9321 (3.0559)	LR 1.250e-04
0: TRAIN [6][2840/3880]	Time 0.126 (0.164)	Data 1.20e-04 (2.36e-04)	Tok/s 82720 (87339)	Loss/tok 2.8952 (3.0560)	LR 1.250e-04
0: TRAIN [6][2850/3880]	Time 0.123 (0.164)	Data 1.16e-04 (2.36e-04)	Tok/s 85402 (87343)	Loss/tok 2.8668 (3.0562)	LR 1.250e-04
0: TRAIN [6][2860/3880]	Time 0.125 (0.164)	Data 1.22e-04 (2.36e-04)	Tok/s 83176 (87341)	Loss/tok 2.8308 (3.0560)	LR 1.250e-04
0: TRAIN [6][2870/3880]	Time 0.182 (0.164)	Data 1.16e-04 (2.35e-04)	Tok/s 93317 (87340)	Loss/tok 3.0482 (3.0557)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][2880/3880]	Time 0.066 (0.164)	Data 1.21e-04 (2.35e-04)	Tok/s 79446 (87350)	Loss/tok 2.3644 (3.0562)	LR 1.250e-04
0: TRAIN [6][2890/3880]	Time 0.123 (0.164)	Data 1.31e-04 (2.34e-04)	Tok/s 83923 (87337)	Loss/tok 2.8843 (3.0558)	LR 1.250e-04
0: TRAIN [6][2900/3880]	Time 0.127 (0.164)	Data 1.38e-04 (2.34e-04)	Tok/s 82556 (87341)	Loss/tok 2.8800 (3.0563)	LR 1.250e-04
0: TRAIN [6][2910/3880]	Time 0.124 (0.164)	Data 1.13e-04 (2.34e-04)	Tok/s 83739 (87344)	Loss/tok 2.9397 (3.0562)	LR 1.250e-04
0: TRAIN [6][2920/3880]	Time 0.184 (0.164)	Data 1.15e-04 (2.33e-04)	Tok/s 91689 (87355)	Loss/tok 3.0210 (3.0567)	LR 1.250e-04
0: TRAIN [6][2930/3880]	Time 0.125 (0.164)	Data 1.33e-04 (2.33e-04)	Tok/s 82601 (87353)	Loss/tok 2.8631 (3.0565)	LR 1.250e-04
0: TRAIN [6][2940/3880]	Time 0.316 (0.164)	Data 1.13e-04 (2.33e-04)	Tok/s 94653 (87356)	Loss/tok 3.3486 (3.0566)	LR 1.250e-04
0: TRAIN [6][2950/3880]	Time 0.182 (0.164)	Data 1.13e-04 (2.32e-04)	Tok/s 92761 (87358)	Loss/tok 3.0535 (3.0566)	LR 1.250e-04
0: TRAIN [6][2960/3880]	Time 0.124 (0.164)	Data 1.42e-04 (2.32e-04)	Tok/s 83837 (87356)	Loss/tok 2.8442 (3.0565)	LR 1.250e-04
0: TRAIN [6][2970/3880]	Time 0.125 (0.164)	Data 1.39e-04 (2.32e-04)	Tok/s 81594 (87357)	Loss/tok 2.8718 (3.0566)	LR 1.250e-04
0: TRAIN [6][2980/3880]	Time 0.125 (0.164)	Data 1.39e-04 (2.31e-04)	Tok/s 82862 (87354)	Loss/tok 2.8635 (3.0566)	LR 1.250e-04
0: TRAIN [6][2990/3880]	Time 0.126 (0.164)	Data 1.34e-04 (2.31e-04)	Tok/s 82639 (87353)	Loss/tok 2.9084 (3.0565)	LR 1.250e-04
0: TRAIN [6][3000/3880]	Time 0.125 (0.164)	Data 1.39e-04 (2.31e-04)	Tok/s 82121 (87358)	Loss/tok 2.8718 (3.0565)	LR 1.250e-04
0: TRAIN [6][3010/3880]	Time 0.185 (0.164)	Data 1.24e-04 (2.30e-04)	Tok/s 90854 (87362)	Loss/tok 3.0039 (3.0565)	LR 1.250e-04
0: TRAIN [6][3020/3880]	Time 0.244 (0.164)	Data 1.27e-04 (2.30e-04)	Tok/s 96423 (87366)	Loss/tok 3.2111 (3.0564)	LR 1.250e-04
0: TRAIN [6][3030/3880]	Time 0.066 (0.164)	Data 1.25e-04 (2.30e-04)	Tok/s 79237 (87352)	Loss/tok 2.5300 (3.0560)	LR 1.250e-04
0: TRAIN [6][3040/3880]	Time 0.127 (0.164)	Data 1.47e-04 (2.29e-04)	Tok/s 81834 (87358)	Loss/tok 2.8516 (3.0561)	LR 1.250e-04
0: TRAIN [6][3050/3880]	Time 0.123 (0.164)	Data 1.37e-04 (2.29e-04)	Tok/s 81666 (87355)	Loss/tok 2.8760 (3.0563)	LR 1.250e-04
0: TRAIN [6][3060/3880]	Time 0.127 (0.164)	Data 1.41e-04 (2.29e-04)	Tok/s 80730 (87353)	Loss/tok 2.9840 (3.0563)	LR 1.250e-04
0: TRAIN [6][3070/3880]	Time 0.123 (0.164)	Data 1.63e-04 (2.28e-04)	Tok/s 82561 (87348)	Loss/tok 2.9068 (3.0561)	LR 1.250e-04
0: TRAIN [6][3080/3880]	Time 0.246 (0.164)	Data 1.40e-04 (2.28e-04)	Tok/s 94509 (87343)	Loss/tok 3.2087 (3.0559)	LR 1.250e-04
0: TRAIN [6][3090/3880]	Time 0.317 (0.164)	Data 1.24e-04 (2.28e-04)	Tok/s 95421 (87354)	Loss/tok 3.2838 (3.0564)	LR 1.250e-04
0: TRAIN [6][3100/3880]	Time 0.241 (0.164)	Data 1.21e-04 (2.27e-04)	Tok/s 97096 (87360)	Loss/tok 3.1873 (3.0565)	LR 1.250e-04
0: TRAIN [6][3110/3880]	Time 0.183 (0.164)	Data 1.25e-04 (2.27e-04)	Tok/s 90435 (87369)	Loss/tok 3.0775 (3.0564)	LR 1.250e-04
0: TRAIN [6][3120/3880]	Time 0.184 (0.164)	Data 1.34e-04 (2.27e-04)	Tok/s 92725 (87375)	Loss/tok 3.0712 (3.0565)	LR 1.250e-04
0: TRAIN [6][3130/3880]	Time 0.181 (0.164)	Data 1.39e-04 (2.26e-04)	Tok/s 91991 (87374)	Loss/tok 3.0474 (3.0563)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [6][3140/3880]	Time 0.244 (0.164)	Data 1.21e-04 (2.26e-04)	Tok/s 94737 (87382)	Loss/tok 3.2734 (3.0565)	LR 1.250e-04
0: TRAIN [6][3150/3880]	Time 0.245 (0.164)	Data 1.22e-04 (2.26e-04)	Tok/s 95753 (87386)	Loss/tok 3.3137 (3.0567)	LR 1.250e-04
0: TRAIN [6][3160/3880]	Time 0.068 (0.164)	Data 1.38e-04 (2.26e-04)	Tok/s 79621 (87380)	Loss/tok 2.4925 (3.0566)	LR 1.250e-04
0: TRAIN [6][3170/3880]	Time 0.244 (0.164)	Data 1.31e-04 (2.25e-04)	Tok/s 95215 (87385)	Loss/tok 3.3407 (3.0567)	LR 1.250e-04
0: TRAIN [6][3180/3880]	Time 0.182 (0.164)	Data 1.47e-04 (2.25e-04)	Tok/s 93483 (87387)	Loss/tok 2.9956 (3.0565)	LR 1.250e-04
0: TRAIN [6][3190/3880]	Time 0.181 (0.164)	Data 1.32e-04 (2.25e-04)	Tok/s 93233 (87393)	Loss/tok 3.0516 (3.0565)	LR 1.250e-04
0: TRAIN [6][3200/3880]	Time 0.181 (0.164)	Data 1.27e-04 (2.24e-04)	Tok/s 93755 (87394)	Loss/tok 2.9579 (3.0564)	LR 1.250e-04
0: TRAIN [6][3210/3880]	Time 0.184 (0.164)	Data 1.31e-04 (2.24e-04)	Tok/s 92178 (87398)	Loss/tok 3.0834 (3.0563)	LR 1.250e-04
0: TRAIN [6][3220/3880]	Time 0.123 (0.164)	Data 1.29e-04 (2.24e-04)	Tok/s 82641 (87394)	Loss/tok 2.7998 (3.0560)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][3230/3880]	Time 0.243 (0.164)	Data 1.45e-04 (2.24e-04)	Tok/s 96264 (87396)	Loss/tok 3.2279 (3.0561)	LR 1.250e-04
0: TRAIN [6][3240/3880]	Time 0.316 (0.164)	Data 1.34e-04 (2.23e-04)	Tok/s 93003 (87392)	Loss/tok 3.4131 (3.0564)	LR 1.250e-04
0: TRAIN [6][3250/3880]	Time 0.126 (0.164)	Data 1.22e-04 (2.23e-04)	Tok/s 81373 (87395)	Loss/tok 2.7559 (3.0568)	LR 1.250e-04
0: TRAIN [6][3260/3880]	Time 0.127 (0.164)	Data 1.42e-04 (2.23e-04)	Tok/s 82348 (87399)	Loss/tok 2.8949 (3.0567)	LR 1.250e-04
0: TRAIN [6][3270/3880]	Time 0.184 (0.164)	Data 1.43e-04 (2.23e-04)	Tok/s 90473 (87400)	Loss/tok 3.0212 (3.0565)	LR 1.250e-04
0: TRAIN [6][3280/3880]	Time 0.242 (0.164)	Data 1.20e-04 (2.22e-04)	Tok/s 96372 (87394)	Loss/tok 3.2382 (3.0564)	LR 1.250e-04
0: TRAIN [6][3290/3880]	Time 0.067 (0.164)	Data 1.25e-04 (2.22e-04)	Tok/s 78271 (87391)	Loss/tok 2.4993 (3.0563)	LR 1.250e-04
0: TRAIN [6][3300/3880]	Time 0.122 (0.164)	Data 1.40e-04 (2.22e-04)	Tok/s 85388 (87388)	Loss/tok 2.8602 (3.0563)	LR 1.250e-04
0: TRAIN [6][3310/3880]	Time 0.124 (0.164)	Data 1.20e-04 (2.21e-04)	Tok/s 83804 (87386)	Loss/tok 2.8174 (3.0562)	LR 1.250e-04
0: TRAIN [6][3320/3880]	Time 0.124 (0.164)	Data 1.36e-04 (2.21e-04)	Tok/s 83120 (87381)	Loss/tok 2.7853 (3.0560)	LR 1.250e-04
0: TRAIN [6][3330/3880]	Time 0.183 (0.164)	Data 1.43e-04 (2.21e-04)	Tok/s 91757 (87395)	Loss/tok 2.9868 (3.0564)	LR 1.250e-04
0: TRAIN [6][3340/3880]	Time 0.124 (0.164)	Data 1.41e-04 (2.21e-04)	Tok/s 81926 (87387)	Loss/tok 2.9209 (3.0562)	LR 1.250e-04
0: TRAIN [6][3350/3880]	Time 0.183 (0.164)	Data 1.14e-04 (2.20e-04)	Tok/s 91677 (87394)	Loss/tok 3.0065 (3.0562)	LR 1.250e-04
0: TRAIN [6][3360/3880]	Time 0.124 (0.164)	Data 1.19e-04 (2.20e-04)	Tok/s 83888 (87395)	Loss/tok 2.9434 (3.0562)	LR 1.250e-04
0: TRAIN [6][3370/3880]	Time 0.122 (0.164)	Data 1.16e-04 (2.20e-04)	Tok/s 84447 (87390)	Loss/tok 2.9552 (3.0561)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][3380/3880]	Time 0.123 (0.164)	Data 1.37e-04 (2.19e-04)	Tok/s 82389 (87391)	Loss/tok 2.7995 (3.0561)	LR 1.250e-04
0: TRAIN [6][3390/3880]	Time 0.243 (0.164)	Data 1.11e-04 (2.19e-04)	Tok/s 96542 (87393)	Loss/tok 3.1883 (3.0560)	LR 1.250e-04
0: TRAIN [6][3400/3880]	Time 0.123 (0.164)	Data 1.32e-04 (2.19e-04)	Tok/s 83589 (87390)	Loss/tok 2.8735 (3.0560)	LR 1.250e-04
0: TRAIN [6][3410/3880]	Time 0.124 (0.164)	Data 1.18e-04 (2.19e-04)	Tok/s 82274 (87389)	Loss/tok 2.8969 (3.0559)	LR 1.250e-04
0: TRAIN [6][3420/3880]	Time 0.314 (0.164)	Data 1.28e-04 (2.18e-04)	Tok/s 95516 (87386)	Loss/tok 3.4445 (3.0562)	LR 1.250e-04
0: TRAIN [6][3430/3880]	Time 0.123 (0.164)	Data 1.46e-04 (2.18e-04)	Tok/s 82825 (87384)	Loss/tok 2.9067 (3.0560)	LR 1.250e-04
0: TRAIN [6][3440/3880]	Time 0.186 (0.164)	Data 1.23e-04 (2.18e-04)	Tok/s 89643 (87394)	Loss/tok 3.0161 (3.0563)	LR 1.250e-04
0: TRAIN [6][3450/3880]	Time 0.125 (0.164)	Data 1.11e-04 (2.18e-04)	Tok/s 82838 (87392)	Loss/tok 2.8385 (3.0562)	LR 1.250e-04
0: TRAIN [6][3460/3880]	Time 0.317 (0.164)	Data 1.15e-04 (2.17e-04)	Tok/s 92908 (87393)	Loss/tok 3.4118 (3.0563)	LR 1.250e-04
0: TRAIN [6][3470/3880]	Time 0.184 (0.164)	Data 1.23e-04 (2.17e-04)	Tok/s 91805 (87395)	Loss/tok 3.1012 (3.0562)	LR 1.250e-04
0: TRAIN [6][3480/3880]	Time 0.245 (0.164)	Data 1.12e-04 (2.17e-04)	Tok/s 95528 (87401)	Loss/tok 3.1388 (3.0560)	LR 1.250e-04
0: TRAIN [6][3490/3880]	Time 0.186 (0.164)	Data 1.18e-04 (2.16e-04)	Tok/s 89520 (87398)	Loss/tok 3.1812 (3.0559)	LR 1.250e-04
0: TRAIN [6][3500/3880]	Time 0.124 (0.164)	Data 1.42e-04 (2.16e-04)	Tok/s 83661 (87393)	Loss/tok 2.8442 (3.0557)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][3510/3880]	Time 0.126 (0.164)	Data 1.39e-04 (2.16e-04)	Tok/s 82020 (87395)	Loss/tok 2.8424 (3.0561)	LR 1.250e-04
0: TRAIN [6][3520/3880]	Time 0.128 (0.164)	Data 1.26e-04 (2.16e-04)	Tok/s 79205 (87398)	Loss/tok 2.8747 (3.0565)	LR 1.250e-04
0: TRAIN [6][3530/3880]	Time 0.184 (0.164)	Data 1.43e-04 (2.15e-04)	Tok/s 89737 (87394)	Loss/tok 3.0396 (3.0563)	LR 1.250e-04
0: TRAIN [6][3540/3880]	Time 0.124 (0.164)	Data 1.30e-04 (2.15e-04)	Tok/s 82403 (87397)	Loss/tok 2.8117 (3.0561)	LR 1.250e-04
0: TRAIN [6][3550/3880]	Time 0.247 (0.164)	Data 1.41e-04 (2.15e-04)	Tok/s 95199 (87400)	Loss/tok 3.2215 (3.0562)	LR 1.250e-04
0: TRAIN [6][3560/3880]	Time 0.313 (0.164)	Data 1.37e-04 (2.15e-04)	Tok/s 95461 (87404)	Loss/tok 3.3959 (3.0564)	LR 1.250e-04
0: TRAIN [6][3570/3880]	Time 0.124 (0.164)	Data 1.14e-04 (2.14e-04)	Tok/s 83276 (87401)	Loss/tok 2.8986 (3.0563)	LR 1.250e-04
0: TRAIN [6][3580/3880]	Time 0.125 (0.164)	Data 1.38e-04 (2.14e-04)	Tok/s 82107 (87405)	Loss/tok 2.8758 (3.0564)	LR 1.250e-04
0: TRAIN [6][3590/3880]	Time 0.124 (0.164)	Data 1.18e-04 (2.14e-04)	Tok/s 83321 (87412)	Loss/tok 2.8637 (3.0566)	LR 1.250e-04
0: TRAIN [6][3600/3880]	Time 0.125 (0.164)	Data 1.08e-04 (2.14e-04)	Tok/s 83542 (87407)	Loss/tok 2.9125 (3.0565)	LR 1.250e-04
0: TRAIN [6][3610/3880]	Time 0.181 (0.164)	Data 1.20e-04 (2.13e-04)	Tok/s 92184 (87404)	Loss/tok 2.9473 (3.0562)	LR 1.250e-04
0: TRAIN [6][3620/3880]	Time 0.126 (0.164)	Data 1.49e-04 (2.13e-04)	Tok/s 80416 (87403)	Loss/tok 2.8661 (3.0563)	LR 1.250e-04
0: TRAIN [6][3630/3880]	Time 0.243 (0.164)	Data 1.25e-04 (2.13e-04)	Tok/s 96334 (87403)	Loss/tok 3.2226 (3.0562)	LR 1.250e-04
0: TRAIN [6][3640/3880]	Time 0.126 (0.164)	Data 1.24e-04 (2.13e-04)	Tok/s 81572 (87396)	Loss/tok 2.8959 (3.0562)	LR 1.250e-04
0: TRAIN [6][3650/3880]	Time 0.183 (0.164)	Data 1.14e-04 (2.12e-04)	Tok/s 93454 (87399)	Loss/tok 3.1629 (3.0562)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][3660/3880]	Time 0.181 (0.164)	Data 1.09e-04 (2.12e-04)	Tok/s 93295 (87399)	Loss/tok 3.0481 (3.0563)	LR 1.250e-04
0: TRAIN [6][3670/3880]	Time 0.183 (0.164)	Data 1.32e-04 (2.12e-04)	Tok/s 92313 (87404)	Loss/tok 3.1093 (3.0566)	LR 1.250e-04
0: TRAIN [6][3680/3880]	Time 0.242 (0.164)	Data 1.32e-04 (2.12e-04)	Tok/s 95701 (87406)	Loss/tok 3.2802 (3.0565)	LR 1.250e-04
0: TRAIN [6][3690/3880]	Time 0.244 (0.164)	Data 1.25e-04 (2.11e-04)	Tok/s 95001 (87407)	Loss/tok 3.2675 (3.0565)	LR 1.250e-04
0: TRAIN [6][3700/3880]	Time 0.181 (0.164)	Data 1.36e-04 (2.11e-04)	Tok/s 93166 (87408)	Loss/tok 3.1517 (3.0565)	LR 1.250e-04
0: TRAIN [6][3710/3880]	Time 0.183 (0.164)	Data 1.08e-04 (2.11e-04)	Tok/s 93043 (87412)	Loss/tok 3.1119 (3.0563)	LR 1.250e-04
0: TRAIN [6][3720/3880]	Time 0.185 (0.164)	Data 1.13e-04 (2.11e-04)	Tok/s 90675 (87407)	Loss/tok 3.1051 (3.0561)	LR 1.250e-04
0: TRAIN [6][3730/3880]	Time 0.126 (0.164)	Data 1.34e-04 (2.10e-04)	Tok/s 82193 (87414)	Loss/tok 2.9580 (3.0564)	LR 1.250e-04
0: TRAIN [6][3740/3880]	Time 0.124 (0.164)	Data 1.25e-04 (2.10e-04)	Tok/s 82767 (87418)	Loss/tok 2.9395 (3.0565)	LR 1.250e-04
0: TRAIN [6][3750/3880]	Time 0.182 (0.164)	Data 1.28e-04 (2.10e-04)	Tok/s 91783 (87415)	Loss/tok 3.1069 (3.0563)	LR 1.250e-04
0: TRAIN [6][3760/3880]	Time 0.124 (0.164)	Data 1.38e-04 (2.10e-04)	Tok/s 84022 (87411)	Loss/tok 2.8661 (3.0563)	LR 1.250e-04
0: TRAIN [6][3770/3880]	Time 0.124 (0.164)	Data 1.35e-04 (2.10e-04)	Tok/s 84918 (87413)	Loss/tok 2.8693 (3.0564)	LR 1.250e-04
0: TRAIN [6][3780/3880]	Time 0.125 (0.164)	Data 1.33e-04 (2.09e-04)	Tok/s 82183 (87405)	Loss/tok 2.9018 (3.0561)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [6][3790/3880]	Time 0.124 (0.164)	Data 1.08e-04 (2.09e-04)	Tok/s 82958 (87401)	Loss/tok 2.9663 (3.0561)	LR 1.250e-04
0: TRAIN [6][3800/3880]	Time 0.126 (0.164)	Data 1.15e-04 (2.09e-04)	Tok/s 81608 (87401)	Loss/tok 2.8753 (3.0560)	LR 1.250e-04
0: TRAIN [6][3810/3880]	Time 0.125 (0.163)	Data 1.30e-04 (2.09e-04)	Tok/s 81480 (87395)	Loss/tok 2.8700 (3.0557)	LR 1.250e-04
0: TRAIN [6][3820/3880]	Time 0.123 (0.163)	Data 1.08e-04 (2.08e-04)	Tok/s 85678 (87393)	Loss/tok 2.7755 (3.0557)	LR 1.250e-04
0: TRAIN [6][3830/3880]	Time 0.184 (0.164)	Data 1.26e-04 (2.08e-04)	Tok/s 90702 (87400)	Loss/tok 2.9983 (3.0558)	LR 1.250e-04
0: TRAIN [6][3840/3880]	Time 0.068 (0.164)	Data 1.35e-04 (2.08e-04)	Tok/s 79195 (87402)	Loss/tok 2.4507 (3.0559)	LR 1.250e-04
0: TRAIN [6][3850/3880]	Time 0.185 (0.163)	Data 1.28e-04 (2.08e-04)	Tok/s 89792 (87396)	Loss/tok 2.9736 (3.0557)	LR 1.250e-04
0: TRAIN [6][3860/3880]	Time 0.124 (0.164)	Data 1.13e-04 (2.07e-04)	Tok/s 84413 (87402)	Loss/tok 2.7655 (3.0556)	LR 1.250e-04
0: TRAIN [6][3870/3880]	Time 0.184 (0.164)	Data 1.24e-04 (2.07e-04)	Tok/s 90417 (87402)	Loss/tok 3.1945 (3.0558)	LR 1.250e-04
:::MLL 1586327771.015 epoch_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 524}}
:::MLL 1586327771.015 eval_start: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [6][0/6]	Time 0.648 (0.648)	Decoder iters 100.0 (100.0)	Tok/s 25309 (25309)
0: Running moses detokenizer
0: BLEU(score=23.83332239467201, counts=[37230, 18596, 10522, 6228], totals=[65831, 62828, 59825, 56827], precisions=[56.55390317631511, 29.598268288024446, 17.587964897618054, 10.95957907332782], bp=1.0, sys_len=65831, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586327774.020 eval_accuracy: {"value": 23.83, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 535}}
:::MLL 1586327774.020 eval_stop: {"value": null, "metadata": {"epoch_num": 7, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 6	Training Loss: 3.0566	Test BLEU: 23.83
0: Performance: Epoch: 6	Training: 349605 Tok/s
0: Finished epoch 6
:::MLL 1586327774.020 block_stop: {"value": null, "metadata": {"first_epoch_num": 7, "file": "train.py", "lineno": 557}}
:::MLL 1586327774.021 block_start: {"value": null, "metadata": {"first_epoch_num": 8, "epoch_count": 1, "file": "train.py", "lineno": 511}}
:::MLL 1586327774.021 epoch_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 514}}
0: Starting epoch 7
0: Executing preallocation
0: Sampler for epoch 7 uses seed 2354751958
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [7][0/3880]	Time 0.690 (0.690)	Data 3.45e-01 (3.45e-01)	Tok/s 23970 (23970)	Loss/tok 3.0683 (3.0683)	LR 1.250e-04
0: TRAIN [7][10/3880]	Time 0.181 (0.230)	Data 1.42e-04 (3.14e-02)	Tok/s 90549 (83349)	Loss/tok 3.2305 (3.1255)	LR 1.250e-04
0: TRAIN [7][20/3880]	Time 0.127 (0.209)	Data 1.49e-04 (1.65e-02)	Tok/s 83190 (85852)	Loss/tok 2.8920 (3.1195)	LR 1.250e-04
0: TRAIN [7][30/3880]	Time 0.124 (0.187)	Data 1.23e-04 (1.12e-02)	Tok/s 83777 (86082)	Loss/tok 2.8876 (3.0769)	LR 1.250e-04
0: TRAIN [7][40/3880]	Time 0.123 (0.181)	Data 1.45e-04 (8.53e-03)	Tok/s 82293 (86355)	Loss/tok 2.8531 (3.0575)	LR 1.250e-04
0: TRAIN [7][50/3880]	Time 0.248 (0.181)	Data 1.26e-04 (6.88e-03)	Tok/s 96157 (87150)	Loss/tok 3.0241 (3.0571)	LR 1.250e-04
0: TRAIN [7][60/3880]	Time 0.182 (0.178)	Data 1.22e-04 (5.78e-03)	Tok/s 90889 (87067)	Loss/tok 3.1682 (3.0621)	LR 1.250e-04
0: TRAIN [7][70/3880]	Time 0.124 (0.177)	Data 1.36e-04 (4.98e-03)	Tok/s 83081 (87221)	Loss/tok 2.8759 (3.0576)	LR 1.250e-04
0: TRAIN [7][80/3880]	Time 0.183 (0.174)	Data 1.48e-04 (4.38e-03)	Tok/s 91662 (87343)	Loss/tok 3.0149 (3.0498)	LR 1.250e-04
0: TRAIN [7][90/3880]	Time 0.125 (0.172)	Data 1.13e-04 (3.91e-03)	Tok/s 81926 (87134)	Loss/tok 2.8234 (3.0480)	LR 1.250e-04
0: TRAIN [7][100/3880]	Time 0.125 (0.168)	Data 1.37e-04 (3.54e-03)	Tok/s 83781 (87013)	Loss/tok 2.9392 (3.0420)	LR 1.250e-04
0: TRAIN [7][110/3880]	Time 0.125 (0.167)	Data 1.37e-04 (3.23e-03)	Tok/s 83122 (86904)	Loss/tok 2.8947 (3.0357)	LR 1.250e-04
0: TRAIN [7][120/3880]	Time 0.242 (0.165)	Data 1.13e-04 (2.97e-03)	Tok/s 96999 (86778)	Loss/tok 3.1475 (3.0319)	LR 1.250e-04
0: TRAIN [7][130/3880]	Time 0.243 (0.165)	Data 1.10e-04 (2.76e-03)	Tok/s 95113 (86801)	Loss/tok 3.2490 (3.0344)	LR 1.250e-04
0: TRAIN [7][140/3880]	Time 0.067 (0.164)	Data 1.15e-04 (2.57e-03)	Tok/s 79481 (86777)	Loss/tok 2.5224 (3.0359)	LR 1.250e-04
0: TRAIN [7][150/3880]	Time 0.124 (0.163)	Data 1.23e-04 (2.41e-03)	Tok/s 83474 (86677)	Loss/tok 2.8488 (3.0328)	LR 1.250e-04
0: TRAIN [7][160/3880]	Time 0.126 (0.164)	Data 1.10e-04 (2.26e-03)	Tok/s 80798 (86749)	Loss/tok 2.8510 (3.0361)	LR 1.250e-04
0: TRAIN [7][170/3880]	Time 0.124 (0.165)	Data 1.27e-04 (2.14e-03)	Tok/s 82379 (86860)	Loss/tok 2.9328 (3.0395)	LR 1.250e-04
0: TRAIN [7][180/3880]	Time 0.126 (0.167)	Data 1.27e-04 (2.03e-03)	Tok/s 80933 (86977)	Loss/tok 2.8424 (3.0450)	LR 1.250e-04
0: TRAIN [7][190/3880]	Time 0.186 (0.168)	Data 1.07e-04 (1.93e-03)	Tok/s 89986 (87088)	Loss/tok 2.9857 (3.0497)	LR 1.250e-04
0: TRAIN [7][200/3880]	Time 0.124 (0.166)	Data 1.12e-04 (1.84e-03)	Tok/s 80953 (86933)	Loss/tok 2.8408 (3.0461)	LR 1.250e-04
0: TRAIN [7][210/3880]	Time 0.245 (0.167)	Data 1.14e-04 (1.76e-03)	Tok/s 94207 (86984)	Loss/tok 3.2651 (3.0493)	LR 1.250e-04
0: TRAIN [7][220/3880]	Time 0.186 (0.167)	Data 1.07e-04 (1.68e-03)	Tok/s 90827 (87013)	Loss/tok 3.0934 (3.0475)	LR 1.250e-04
0: TRAIN [7][230/3880]	Time 0.126 (0.166)	Data 1.21e-04 (1.62e-03)	Tok/s 82024 (86979)	Loss/tok 2.8583 (3.0464)	LR 1.250e-04
0: TRAIN [7][240/3880]	Time 0.124 (0.166)	Data 1.08e-04 (1.55e-03)	Tok/s 82640 (86901)	Loss/tok 2.7927 (3.0486)	LR 1.250e-04
0: TRAIN [7][250/3880]	Time 0.124 (0.167)	Data 1.08e-04 (1.50e-03)	Tok/s 82143 (86928)	Loss/tok 2.9005 (3.0510)	LR 1.250e-04
0: TRAIN [7][260/3880]	Time 0.184 (0.167)	Data 1.26e-04 (1.44e-03)	Tok/s 91231 (86999)	Loss/tok 3.0239 (3.0492)	LR 1.250e-04
0: TRAIN [7][270/3880]	Time 0.184 (0.166)	Data 1.43e-04 (1.39e-03)	Tok/s 90758 (86942)	Loss/tok 3.0515 (3.0464)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][280/3880]	Time 0.068 (0.165)	Data 1.47e-04 (1.35e-03)	Tok/s 76704 (86892)	Loss/tok 2.5015 (3.0470)	LR 1.250e-04
0: TRAIN [7][290/3880]	Time 0.183 (0.165)	Data 1.18e-04 (1.31e-03)	Tok/s 91336 (86901)	Loss/tok 2.9950 (3.0463)	LR 1.250e-04
0: TRAIN [7][300/3880]	Time 0.126 (0.165)	Data 1.62e-04 (1.27e-03)	Tok/s 81238 (86869)	Loss/tok 2.9188 (3.0448)	LR 1.250e-04
0: TRAIN [7][310/3880]	Time 0.183 (0.165)	Data 1.16e-04 (1.23e-03)	Tok/s 92830 (86863)	Loss/tok 3.0857 (3.0439)	LR 1.250e-04
0: TRAIN [7][320/3880]	Time 0.245 (0.165)	Data 1.25e-04 (1.20e-03)	Tok/s 94921 (86915)	Loss/tok 3.2254 (3.0454)	LR 1.250e-04
0: TRAIN [7][330/3880]	Time 0.127 (0.165)	Data 1.25e-04 (1.17e-03)	Tok/s 80423 (86878)	Loss/tok 2.8274 (3.0448)	LR 1.250e-04
0: TRAIN [7][340/3880]	Time 0.124 (0.165)	Data 1.10e-04 (1.13e-03)	Tok/s 84473 (86887)	Loss/tok 2.9746 (3.0442)	LR 1.250e-04
0: TRAIN [7][350/3880]	Time 0.126 (0.164)	Data 1.64e-04 (1.11e-03)	Tok/s 80879 (86807)	Loss/tok 2.9025 (3.0410)	LR 1.250e-04
0: TRAIN [7][360/3880]	Time 0.182 (0.165)	Data 1.35e-04 (1.08e-03)	Tok/s 91220 (86901)	Loss/tok 3.0048 (3.0452)	LR 1.250e-04
0: TRAIN [7][370/3880]	Time 0.124 (0.164)	Data 1.14e-04 (1.05e-03)	Tok/s 83284 (86817)	Loss/tok 2.9575 (3.0432)	LR 1.250e-04
0: TRAIN [7][380/3880]	Time 0.125 (0.163)	Data 1.08e-04 (1.03e-03)	Tok/s 82987 (86760)	Loss/tok 2.9065 (3.0419)	LR 1.250e-04
0: TRAIN [7][390/3880]	Time 0.245 (0.164)	Data 1.28e-04 (1.01e-03)	Tok/s 94936 (86841)	Loss/tok 3.1916 (3.0439)	LR 1.250e-04
0: TRAIN [7][400/3880]	Time 0.182 (0.163)	Data 1.25e-04 (9.83e-04)	Tok/s 91930 (86765)	Loss/tok 2.9929 (3.0413)	LR 1.250e-04
0: TRAIN [7][410/3880]	Time 0.125 (0.163)	Data 1.09e-04 (9.62e-04)	Tok/s 80749 (86695)	Loss/tok 2.8282 (3.0392)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][420/3880]	Time 0.124 (0.163)	Data 1.08e-04 (9.42e-04)	Tok/s 81969 (86737)	Loss/tok 2.8565 (3.0400)	LR 1.250e-04
0: TRAIN [7][430/3880]	Time 0.125 (0.163)	Data 1.44e-04 (9.23e-04)	Tok/s 82363 (86719)	Loss/tok 2.9008 (3.0388)	LR 1.250e-04
0: TRAIN [7][440/3880]	Time 0.186 (0.162)	Data 1.44e-04 (9.05e-04)	Tok/s 89739 (86702)	Loss/tok 3.0300 (3.0373)	LR 1.250e-04
0: TRAIN [7][450/3880]	Time 0.126 (0.163)	Data 1.54e-04 (8.88e-04)	Tok/s 80336 (86751)	Loss/tok 2.8414 (3.0394)	LR 1.250e-04
0: TRAIN [7][460/3880]	Time 0.125 (0.163)	Data 1.06e-04 (8.72e-04)	Tok/s 82014 (86773)	Loss/tok 2.8139 (3.0396)	LR 1.250e-04
0: TRAIN [7][470/3880]	Time 0.184 (0.163)	Data 1.23e-04 (8.56e-04)	Tok/s 91046 (86782)	Loss/tok 3.1510 (3.0389)	LR 1.250e-04
0: TRAIN [7][480/3880]	Time 0.124 (0.163)	Data 1.29e-04 (8.41e-04)	Tok/s 81191 (86759)	Loss/tok 2.7700 (3.0413)	LR 1.250e-04
0: TRAIN [7][490/3880]	Time 0.182 (0.163)	Data 1.31e-04 (8.26e-04)	Tok/s 92989 (86789)	Loss/tok 3.0008 (3.0408)	LR 1.250e-04
0: TRAIN [7][500/3880]	Time 0.125 (0.164)	Data 1.42e-04 (8.12e-04)	Tok/s 82002 (86850)	Loss/tok 2.9366 (3.0410)	LR 1.250e-04
0: TRAIN [7][510/3880]	Time 0.124 (0.164)	Data 1.22e-04 (7.99e-04)	Tok/s 84471 (86830)	Loss/tok 2.9478 (3.0424)	LR 1.250e-04
0: TRAIN [7][520/3880]	Time 0.126 (0.163)	Data 1.30e-04 (7.86e-04)	Tok/s 83262 (86826)	Loss/tok 2.7985 (3.0413)	LR 1.250e-04
0: TRAIN [7][530/3880]	Time 0.123 (0.164)	Data 1.24e-04 (7.73e-04)	Tok/s 84213 (86866)	Loss/tok 2.9119 (3.0427)	LR 1.250e-04
0: TRAIN [7][540/3880]	Time 0.183 (0.164)	Data 1.32e-04 (7.61e-04)	Tok/s 90941 (86889)	Loss/tok 3.1087 (3.0439)	LR 1.250e-04
0: TRAIN [7][550/3880]	Time 0.125 (0.164)	Data 1.48e-04 (7.50e-04)	Tok/s 83103 (86860)	Loss/tok 2.8413 (3.0430)	LR 1.250e-04
0: TRAIN [7][560/3880]	Time 0.182 (0.164)	Data 1.30e-04 (7.39e-04)	Tok/s 92387 (86883)	Loss/tok 3.0355 (3.0442)	LR 1.250e-04
0: TRAIN [7][570/3880]	Time 0.123 (0.164)	Data 1.15e-04 (7.28e-04)	Tok/s 83044 (86911)	Loss/tok 2.8273 (3.0440)	LR 1.250e-04
0: TRAIN [7][580/3880]	Time 0.125 (0.164)	Data 1.12e-04 (7.17e-04)	Tok/s 82416 (86906)	Loss/tok 2.7536 (3.0430)	LR 1.250e-04
0: TRAIN [7][590/3880]	Time 0.124 (0.164)	Data 1.29e-04 (7.07e-04)	Tok/s 82592 (86965)	Loss/tok 2.8713 (3.0438)	LR 1.250e-04
0: TRAIN [7][600/3880]	Time 0.125 (0.164)	Data 1.33e-04 (6.98e-04)	Tok/s 83320 (86926)	Loss/tok 2.8439 (3.0430)	LR 1.250e-04
0: TRAIN [7][610/3880]	Time 0.124 (0.164)	Data 1.25e-04 (6.88e-04)	Tok/s 83319 (86891)	Loss/tok 2.9340 (3.0424)	LR 1.250e-04
0: TRAIN [7][620/3880]	Time 0.245 (0.164)	Data 1.09e-04 (6.79e-04)	Tok/s 94508 (86871)	Loss/tok 3.2258 (3.0415)	LR 1.250e-04
0: TRAIN [7][630/3880]	Time 0.184 (0.164)	Data 1.54e-04 (6.71e-04)	Tok/s 91564 (86939)	Loss/tok 2.9611 (3.0422)	LR 1.250e-04
0: TRAIN [7][640/3880]	Time 0.243 (0.164)	Data 1.41e-04 (6.62e-04)	Tok/s 95594 (86925)	Loss/tok 3.0836 (3.0411)	LR 1.250e-04
0: TRAIN [7][650/3880]	Time 0.067 (0.163)	Data 1.19e-04 (6.54e-04)	Tok/s 79374 (86942)	Loss/tok 2.4676 (3.0405)	LR 1.250e-04
0: TRAIN [7][660/3880]	Time 0.181 (0.163)	Data 1.39e-04 (6.46e-04)	Tok/s 90731 (86894)	Loss/tok 3.0824 (3.0390)	LR 1.250e-04
0: TRAIN [7][670/3880]	Time 0.123 (0.163)	Data 1.33e-04 (6.38e-04)	Tok/s 84601 (86895)	Loss/tok 2.8668 (3.0383)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [7][680/3880]	Time 0.128 (0.163)	Data 1.31e-04 (6.30e-04)	Tok/s 81456 (86933)	Loss/tok 2.8441 (3.0386)	LR 1.250e-04
0: TRAIN [7][690/3880]	Time 0.067 (0.163)	Data 1.41e-04 (6.23e-04)	Tok/s 78808 (86943)	Loss/tok 2.5352 (3.0398)	LR 1.250e-04
0: TRAIN [7][700/3880]	Time 0.123 (0.163)	Data 1.30e-04 (6.16e-04)	Tok/s 82775 (86957)	Loss/tok 2.7888 (3.0404)	LR 1.250e-04
0: TRAIN [7][710/3880]	Time 0.245 (0.163)	Data 1.08e-04 (6.09e-04)	Tok/s 94603 (86978)	Loss/tok 3.2723 (3.0399)	LR 1.250e-04
0: TRAIN [7][720/3880]	Time 0.123 (0.163)	Data 1.36e-04 (6.02e-04)	Tok/s 83166 (86975)	Loss/tok 2.9738 (3.0412)	LR 1.250e-04
0: TRAIN [7][730/3880]	Time 0.182 (0.163)	Data 1.32e-04 (5.96e-04)	Tok/s 90191 (86992)	Loss/tok 3.0795 (3.0422)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][740/3880]	Time 0.313 (0.163)	Data 1.16e-04 (5.89e-04)	Tok/s 94287 (86994)	Loss/tok 3.3974 (3.0429)	LR 1.250e-04
0: TRAIN [7][750/3880]	Time 0.123 (0.163)	Data 1.42e-04 (5.83e-04)	Tok/s 83893 (87027)	Loss/tok 2.8218 (3.0437)	LR 1.250e-04
0: TRAIN [7][760/3880]	Time 0.125 (0.164)	Data 1.37e-04 (5.77e-04)	Tok/s 83253 (87055)	Loss/tok 2.7768 (3.0451)	LR 1.250e-04
0: TRAIN [7][770/3880]	Time 0.182 (0.164)	Data 1.24e-04 (5.72e-04)	Tok/s 91139 (87074)	Loss/tok 3.0950 (3.0456)	LR 1.250e-04
0: TRAIN [7][780/3880]	Time 0.243 (0.164)	Data 1.13e-04 (5.66e-04)	Tok/s 95405 (87079)	Loss/tok 3.1534 (3.0450)	LR 1.250e-04
0: TRAIN [7][790/3880]	Time 0.244 (0.164)	Data 1.27e-04 (5.60e-04)	Tok/s 96187 (87090)	Loss/tok 3.2134 (3.0451)	LR 1.250e-04
0: TRAIN [7][800/3880]	Time 0.068 (0.164)	Data 1.37e-04 (5.55e-04)	Tok/s 79531 (87109)	Loss/tok 2.4615 (3.0465)	LR 1.250e-04
0: TRAIN [7][810/3880]	Time 0.068 (0.164)	Data 1.21e-04 (5.50e-04)	Tok/s 77582 (87100)	Loss/tok 2.5094 (3.0454)	LR 1.250e-04
0: TRAIN [7][820/3880]	Time 0.184 (0.164)	Data 1.45e-04 (5.45e-04)	Tok/s 91319 (87101)	Loss/tok 3.0592 (3.0447)	LR 1.250e-04
0: TRAIN [7][830/3880]	Time 0.122 (0.164)	Data 1.38e-04 (5.40e-04)	Tok/s 84580 (87110)	Loss/tok 2.8511 (3.0443)	LR 1.250e-04
0: TRAIN [7][840/3880]	Time 0.124 (0.164)	Data 1.66e-04 (5.35e-04)	Tok/s 83671 (87104)	Loss/tok 2.8181 (3.0443)	LR 1.250e-04
0: TRAIN [7][850/3880]	Time 0.125 (0.163)	Data 1.46e-04 (5.30e-04)	Tok/s 81569 (87092)	Loss/tok 2.9029 (3.0437)	LR 1.250e-04
0: TRAIN [7][860/3880]	Time 0.185 (0.163)	Data 1.31e-04 (5.26e-04)	Tok/s 90366 (87106)	Loss/tok 3.1305 (3.0440)	LR 1.250e-04
0: TRAIN [7][870/3880]	Time 0.316 (0.164)	Data 1.29e-04 (5.21e-04)	Tok/s 93717 (87133)	Loss/tok 3.3390 (3.0449)	LR 1.250e-04
0: TRAIN [7][880/3880]	Time 0.066 (0.163)	Data 1.16e-04 (5.16e-04)	Tok/s 80641 (87105)	Loss/tok 2.5728 (3.0441)	LR 1.250e-04
0: TRAIN [7][890/3880]	Time 0.124 (0.164)	Data 1.49e-04 (5.12e-04)	Tok/s 84250 (87120)	Loss/tok 2.9540 (3.0442)	LR 1.250e-04
0: TRAIN [7][900/3880]	Time 0.184 (0.164)	Data 1.19e-04 (5.08e-04)	Tok/s 91673 (87182)	Loss/tok 2.9742 (3.0449)	LR 1.250e-04
0: TRAIN [7][910/3880]	Time 0.124 (0.164)	Data 1.34e-04 (5.04e-04)	Tok/s 85046 (87180)	Loss/tok 2.8649 (3.0442)	LR 1.250e-04
0: TRAIN [7][920/3880]	Time 0.246 (0.164)	Data 1.33e-04 (5.00e-04)	Tok/s 93988 (87224)	Loss/tok 3.2026 (3.0447)	LR 1.250e-04
0: TRAIN [7][930/3880]	Time 0.183 (0.164)	Data 1.27e-04 (4.96e-04)	Tok/s 90930 (87202)	Loss/tok 2.9926 (3.0439)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][940/3880]	Time 0.126 (0.164)	Data 1.16e-04 (4.92e-04)	Tok/s 82582 (87212)	Loss/tok 2.8088 (3.0443)	LR 1.250e-04
0: TRAIN [7][950/3880]	Time 0.185 (0.164)	Data 1.24e-04 (4.88e-04)	Tok/s 89516 (87216)	Loss/tok 3.0744 (3.0436)	LR 1.250e-04
0: TRAIN [7][960/3880]	Time 0.126 (0.164)	Data 1.29e-04 (4.84e-04)	Tok/s 80306 (87199)	Loss/tok 2.9466 (3.0443)	LR 1.250e-04
0: TRAIN [7][970/3880]	Time 0.123 (0.164)	Data 1.25e-04 (4.80e-04)	Tok/s 83915 (87188)	Loss/tok 2.8828 (3.0434)	LR 1.250e-04
0: TRAIN [7][980/3880]	Time 0.182 (0.163)	Data 1.16e-04 (4.77e-04)	Tok/s 92241 (87192)	Loss/tok 2.9813 (3.0428)	LR 1.250e-04
0: TRAIN [7][990/3880]	Time 0.186 (0.163)	Data 1.43e-04 (4.73e-04)	Tok/s 90546 (87179)	Loss/tok 3.0475 (3.0418)	LR 1.250e-04
0: TRAIN [7][1000/3880]	Time 0.124 (0.163)	Data 1.20e-04 (4.70e-04)	Tok/s 81266 (87186)	Loss/tok 2.9378 (3.0422)	LR 1.250e-04
0: TRAIN [7][1010/3880]	Time 0.182 (0.163)	Data 1.18e-04 (4.66e-04)	Tok/s 92287 (87198)	Loss/tok 3.1154 (3.0422)	LR 1.250e-04
0: TRAIN [7][1020/3880]	Time 0.126 (0.163)	Data 1.19e-04 (4.63e-04)	Tok/s 81983 (87185)	Loss/tok 2.8036 (3.0417)	LR 1.250e-04
0: TRAIN [7][1030/3880]	Time 0.066 (0.163)	Data 1.33e-04 (4.60e-04)	Tok/s 80865 (87191)	Loss/tok 2.4185 (3.0418)	LR 1.250e-04
0: TRAIN [7][1040/3880]	Time 0.124 (0.163)	Data 1.44e-04 (4.57e-04)	Tok/s 84459 (87219)	Loss/tok 2.8472 (3.0419)	LR 1.250e-04
0: TRAIN [7][1050/3880]	Time 0.124 (0.163)	Data 1.19e-04 (4.53e-04)	Tok/s 84587 (87189)	Loss/tok 2.9987 (3.0409)	LR 1.250e-04
0: TRAIN [7][1060/3880]	Time 0.123 (0.163)	Data 1.34e-04 (4.50e-04)	Tok/s 84936 (87189)	Loss/tok 2.8607 (3.0408)	LR 1.250e-04
0: TRAIN [7][1070/3880]	Time 0.123 (0.163)	Data 1.30e-04 (4.48e-04)	Tok/s 82969 (87184)	Loss/tok 2.9461 (3.0405)	LR 1.250e-04
0: TRAIN [7][1080/3880]	Time 0.182 (0.163)	Data 1.27e-04 (4.45e-04)	Tok/s 91411 (87182)	Loss/tok 3.1967 (3.0402)	LR 1.250e-04
0: TRAIN [7][1090/3880]	Time 0.066 (0.162)	Data 1.19e-04 (4.42e-04)	Tok/s 80993 (87169)	Loss/tok 2.5140 (3.0397)	LR 1.250e-04
0: TRAIN [7][1100/3880]	Time 0.068 (0.163)	Data 1.46e-04 (4.39e-04)	Tok/s 80787 (87187)	Loss/tok 2.5100 (3.0403)	LR 1.250e-04
0: TRAIN [7][1110/3880]	Time 0.123 (0.163)	Data 1.47e-04 (4.36e-04)	Tok/s 84584 (87183)	Loss/tok 2.8762 (3.0402)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][1120/3880]	Time 0.123 (0.163)	Data 1.36e-04 (4.34e-04)	Tok/s 84704 (87193)	Loss/tok 2.7198 (3.0413)	LR 1.250e-04
0: TRAIN [7][1130/3880]	Time 0.068 (0.163)	Data 1.13e-04 (4.31e-04)	Tok/s 77041 (87169)	Loss/tok 2.4785 (3.0404)	LR 1.250e-04
0: TRAIN [7][1140/3880]	Time 0.182 (0.163)	Data 1.14e-04 (4.28e-04)	Tok/s 91719 (87183)	Loss/tok 3.0664 (3.0405)	LR 1.250e-04
0: TRAIN [7][1150/3880]	Time 0.247 (0.163)	Data 1.44e-04 (4.26e-04)	Tok/s 94460 (87225)	Loss/tok 3.1819 (3.0435)	LR 1.250e-04
0: TRAIN [7][1160/3880]	Time 0.184 (0.163)	Data 1.31e-04 (4.23e-04)	Tok/s 91039 (87214)	Loss/tok 3.1215 (3.0431)	LR 1.250e-04
0: TRAIN [7][1170/3880]	Time 0.182 (0.163)	Data 1.16e-04 (4.20e-04)	Tok/s 93536 (87248)	Loss/tok 3.0583 (3.0443)	LR 1.250e-04
0: TRAIN [7][1180/3880]	Time 0.123 (0.163)	Data 1.29e-04 (4.18e-04)	Tok/s 84962 (87223)	Loss/tok 2.8117 (3.0437)	LR 1.250e-04
0: TRAIN [7][1190/3880]	Time 0.124 (0.163)	Data 1.31e-04 (4.15e-04)	Tok/s 83854 (87239)	Loss/tok 2.7796 (3.0437)	LR 1.250e-04
0: TRAIN [7][1200/3880]	Time 0.246 (0.163)	Data 1.29e-04 (4.13e-04)	Tok/s 93979 (87234)	Loss/tok 3.2213 (3.0433)	LR 1.250e-04
0: TRAIN [7][1210/3880]	Time 0.183 (0.163)	Data 1.16e-04 (4.11e-04)	Tok/s 91357 (87217)	Loss/tok 2.9840 (3.0426)	LR 1.250e-04
0: TRAIN [7][1220/3880]	Time 0.317 (0.163)	Data 1.21e-04 (4.08e-04)	Tok/s 92454 (87188)	Loss/tok 3.4580 (3.0422)	LR 1.250e-04
0: TRAIN [7][1230/3880]	Time 0.124 (0.163)	Data 1.30e-04 (4.06e-04)	Tok/s 82536 (87194)	Loss/tok 2.8620 (3.0426)	LR 1.250e-04
0: TRAIN [7][1240/3880]	Time 0.126 (0.163)	Data 1.17e-04 (4.04e-04)	Tok/s 82825 (87197)	Loss/tok 2.9039 (3.0425)	LR 1.250e-04
0: TRAIN [7][1250/3880]	Time 0.126 (0.163)	Data 1.35e-04 (4.01e-04)	Tok/s 80311 (87203)	Loss/tok 2.7855 (3.0436)	LR 1.250e-04
0: TRAIN [7][1260/3880]	Time 0.125 (0.163)	Data 1.12e-04 (3.99e-04)	Tok/s 81850 (87199)	Loss/tok 2.8109 (3.0437)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][1270/3880]	Time 0.127 (0.163)	Data 1.35e-04 (3.97e-04)	Tok/s 83508 (87207)	Loss/tok 2.8751 (3.0437)	LR 1.250e-04
0: TRAIN [7][1280/3880]	Time 0.123 (0.163)	Data 1.34e-04 (3.95e-04)	Tok/s 83185 (87212)	Loss/tok 2.7688 (3.0437)	LR 1.250e-04
0: TRAIN [7][1290/3880]	Time 0.125 (0.163)	Data 1.33e-04 (3.93e-04)	Tok/s 84444 (87188)	Loss/tok 2.9223 (3.0429)	LR 1.250e-04
0: TRAIN [7][1300/3880]	Time 0.318 (0.163)	Data 1.32e-04 (3.91e-04)	Tok/s 94560 (87190)	Loss/tok 3.3464 (3.0432)	LR 1.250e-04
0: TRAIN [7][1310/3880]	Time 0.125 (0.163)	Data 1.44e-04 (3.89e-04)	Tok/s 82644 (87188)	Loss/tok 2.8389 (3.0425)	LR 1.250e-04
0: TRAIN [7][1320/3880]	Time 0.244 (0.163)	Data 1.29e-04 (3.87e-04)	Tok/s 96529 (87196)	Loss/tok 3.0928 (3.0429)	LR 1.250e-04
0: TRAIN [7][1330/3880]	Time 0.184 (0.163)	Data 1.38e-04 (3.85e-04)	Tok/s 91500 (87188)	Loss/tok 3.0210 (3.0430)	LR 1.250e-04
0: TRAIN [7][1340/3880]	Time 0.316 (0.163)	Data 1.22e-04 (3.83e-04)	Tok/s 93325 (87207)	Loss/tok 3.4141 (3.0437)	LR 1.250e-04
0: TRAIN [7][1350/3880]	Time 0.123 (0.163)	Data 1.12e-04 (3.81e-04)	Tok/s 85137 (87201)	Loss/tok 2.8910 (3.0431)	LR 1.250e-04
0: TRAIN [7][1360/3880]	Time 0.244 (0.163)	Data 1.25e-04 (3.79e-04)	Tok/s 95754 (87191)	Loss/tok 3.1816 (3.0429)	LR 1.250e-04
0: TRAIN [7][1370/3880]	Time 0.317 (0.163)	Data 1.44e-04 (3.77e-04)	Tok/s 92525 (87188)	Loss/tok 3.4898 (3.0434)	LR 1.250e-04
0: TRAIN [7][1380/3880]	Time 0.125 (0.163)	Data 1.24e-04 (3.75e-04)	Tok/s 82917 (87181)	Loss/tok 2.9239 (3.0439)	LR 1.250e-04
0: TRAIN [7][1390/3880]	Time 0.124 (0.163)	Data 1.20e-04 (3.74e-04)	Tok/s 83290 (87184)	Loss/tok 2.8803 (3.0446)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][1400/3880]	Time 0.316 (0.163)	Data 1.33e-04 (3.72e-04)	Tok/s 93408 (87218)	Loss/tok 3.3659 (3.0456)	LR 1.250e-04
0: TRAIN [7][1410/3880]	Time 0.125 (0.163)	Data 1.33e-04 (3.70e-04)	Tok/s 81802 (87215)	Loss/tok 2.8686 (3.0451)	LR 1.250e-04
0: TRAIN [7][1420/3880]	Time 0.125 (0.164)	Data 1.46e-04 (3.68e-04)	Tok/s 84172 (87239)	Loss/tok 2.8335 (3.0455)	LR 1.250e-04
0: TRAIN [7][1430/3880]	Time 0.315 (0.164)	Data 1.41e-04 (3.67e-04)	Tok/s 93982 (87241)	Loss/tok 3.3248 (3.0463)	LR 1.250e-04
0: TRAIN [7][1440/3880]	Time 0.123 (0.164)	Data 1.34e-04 (3.65e-04)	Tok/s 84805 (87243)	Loss/tok 2.8555 (3.0467)	LR 1.250e-04
0: TRAIN [7][1450/3880]	Time 0.181 (0.164)	Data 1.44e-04 (3.63e-04)	Tok/s 93473 (87240)	Loss/tok 3.1016 (3.0470)	LR 1.250e-04
0: TRAIN [7][1460/3880]	Time 0.184 (0.164)	Data 1.17e-04 (3.62e-04)	Tok/s 91139 (87233)	Loss/tok 3.0007 (3.0463)	LR 1.250e-04
0: TRAIN [7][1470/3880]	Time 0.124 (0.163)	Data 1.33e-04 (3.60e-04)	Tok/s 83206 (87222)	Loss/tok 2.8775 (3.0460)	LR 1.250e-04
0: TRAIN [7][1480/3880]	Time 0.186 (0.164)	Data 1.54e-04 (3.59e-04)	Tok/s 90281 (87233)	Loss/tok 3.1446 (3.0467)	LR 1.250e-04
0: TRAIN [7][1490/3880]	Time 0.124 (0.164)	Data 1.14e-04 (3.57e-04)	Tok/s 82182 (87231)	Loss/tok 2.9952 (3.0469)	LR 1.250e-04
0: TRAIN [7][1500/3880]	Time 0.127 (0.164)	Data 1.35e-04 (3.56e-04)	Tok/s 81245 (87222)	Loss/tok 2.9164 (3.0470)	LR 1.250e-04
0: TRAIN [7][1510/3880]	Time 0.319 (0.164)	Data 1.39e-04 (3.54e-04)	Tok/s 93336 (87218)	Loss/tok 3.3993 (3.0474)	LR 1.250e-04
0: TRAIN [7][1520/3880]	Time 0.125 (0.164)	Data 1.18e-04 (3.53e-04)	Tok/s 81885 (87196)	Loss/tok 2.8515 (3.0467)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][1530/3880]	Time 0.124 (0.164)	Data 1.38e-04 (3.51e-04)	Tok/s 84789 (87196)	Loss/tok 2.8819 (3.0469)	LR 1.250e-04
0: TRAIN [7][1540/3880]	Time 0.128 (0.164)	Data 1.52e-04 (3.50e-04)	Tok/s 82009 (87214)	Loss/tok 2.8255 (3.0470)	LR 1.250e-04
0: TRAIN [7][1550/3880]	Time 0.241 (0.164)	Data 1.13e-04 (3.48e-04)	Tok/s 96213 (87213)	Loss/tok 3.1947 (3.0467)	LR 1.250e-04
0: TRAIN [7][1560/3880]	Time 0.124 (0.164)	Data 1.20e-04 (3.47e-04)	Tok/s 82931 (87212)	Loss/tok 2.8061 (3.0462)	LR 1.250e-04
0: TRAIN [7][1570/3880]	Time 0.244 (0.163)	Data 1.24e-04 (3.45e-04)	Tok/s 95864 (87208)	Loss/tok 3.1803 (3.0458)	LR 1.250e-04
0: TRAIN [7][1580/3880]	Time 0.125 (0.163)	Data 1.14e-04 (3.44e-04)	Tok/s 83800 (87198)	Loss/tok 2.9208 (3.0457)	LR 1.250e-04
0: TRAIN [7][1590/3880]	Time 0.066 (0.163)	Data 1.16e-04 (3.43e-04)	Tok/s 79913 (87174)	Loss/tok 2.6036 (3.0451)	LR 1.250e-04
0: TRAIN [7][1600/3880]	Time 0.183 (0.163)	Data 1.39e-04 (3.41e-04)	Tok/s 91261 (87172)	Loss/tok 3.0487 (3.0455)	LR 1.250e-04
0: TRAIN [7][1610/3880]	Time 0.244 (0.163)	Data 1.35e-04 (3.40e-04)	Tok/s 96103 (87192)	Loss/tok 3.2813 (3.0459)	LR 1.250e-04
0: TRAIN [7][1620/3880]	Time 0.126 (0.163)	Data 1.32e-04 (3.39e-04)	Tok/s 81775 (87186)	Loss/tok 2.8632 (3.0456)	LR 1.250e-04
0: TRAIN [7][1630/3880]	Time 0.125 (0.163)	Data 1.16e-04 (3.37e-04)	Tok/s 83751 (87193)	Loss/tok 2.8439 (3.0456)	LR 1.250e-04
0: TRAIN [7][1640/3880]	Time 0.129 (0.163)	Data 1.26e-04 (3.36e-04)	Tok/s 79294 (87205)	Loss/tok 2.8920 (3.0461)	LR 1.250e-04
0: TRAIN [7][1650/3880]	Time 0.069 (0.164)	Data 1.20e-04 (3.35e-04)	Tok/s 76141 (87214)	Loss/tok 2.4694 (3.0466)	LR 1.250e-04
0: TRAIN [7][1660/3880]	Time 0.184 (0.164)	Data 1.07e-04 (3.33e-04)	Tok/s 91918 (87211)	Loss/tok 3.0151 (3.0463)	LR 1.250e-04
0: TRAIN [7][1670/3880]	Time 0.125 (0.164)	Data 1.12e-04 (3.32e-04)	Tok/s 81764 (87221)	Loss/tok 2.9262 (3.0464)	LR 1.250e-04
0: TRAIN [7][1680/3880]	Time 0.124 (0.164)	Data 1.33e-04 (3.31e-04)	Tok/s 83536 (87212)	Loss/tok 2.9273 (3.0462)	LR 1.250e-04
0: TRAIN [7][1690/3880]	Time 0.243 (0.164)	Data 1.38e-04 (3.30e-04)	Tok/s 95118 (87214)	Loss/tok 3.2897 (3.0470)	LR 1.250e-04
0: TRAIN [7][1700/3880]	Time 0.070 (0.164)	Data 1.28e-04 (3.29e-04)	Tok/s 75144 (87211)	Loss/tok 2.4347 (3.0467)	LR 1.250e-04
0: TRAIN [7][1710/3880]	Time 0.124 (0.163)	Data 1.19e-04 (3.27e-04)	Tok/s 82418 (87207)	Loss/tok 2.8675 (3.0461)	LR 1.250e-04
0: TRAIN [7][1720/3880]	Time 0.125 (0.163)	Data 1.08e-04 (3.26e-04)	Tok/s 82036 (87196)	Loss/tok 2.8635 (3.0457)	LR 1.250e-04
0: TRAIN [7][1730/3880]	Time 0.186 (0.163)	Data 1.23e-04 (3.25e-04)	Tok/s 89012 (87201)	Loss/tok 3.0611 (3.0455)	LR 1.250e-04
0: TRAIN [7][1740/3880]	Time 0.183 (0.163)	Data 1.08e-04 (3.24e-04)	Tok/s 92037 (87192)	Loss/tok 3.0063 (3.0450)	LR 1.250e-04
0: TRAIN [7][1750/3880]	Time 0.183 (0.163)	Data 1.15e-04 (3.23e-04)	Tok/s 92219 (87209)	Loss/tok 2.9825 (3.0448)	LR 1.250e-04
0: TRAIN [7][1760/3880]	Time 0.242 (0.163)	Data 1.16e-04 (3.21e-04)	Tok/s 96706 (87211)	Loss/tok 3.2309 (3.0445)	LR 1.250e-04
0: TRAIN [7][1770/3880]	Time 0.126 (0.163)	Data 1.27e-04 (3.20e-04)	Tok/s 81832 (87213)	Loss/tok 2.7677 (3.0445)	LR 1.250e-04
0: TRAIN [7][1780/3880]	Time 0.183 (0.163)	Data 1.30e-04 (3.19e-04)	Tok/s 90876 (87222)	Loss/tok 3.1039 (3.0452)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [7][1790/3880]	Time 0.124 (0.163)	Data 1.06e-04 (3.18e-04)	Tok/s 83905 (87231)	Loss/tok 2.8779 (3.0455)	LR 1.250e-04
0: TRAIN [7][1800/3880]	Time 0.183 (0.163)	Data 1.13e-04 (3.17e-04)	Tok/s 91841 (87231)	Loss/tok 2.9592 (3.0455)	LR 1.250e-04
0: TRAIN [7][1810/3880]	Time 0.185 (0.163)	Data 1.15e-04 (3.16e-04)	Tok/s 92825 (87252)	Loss/tok 3.0591 (3.0457)	LR 1.250e-04
0: TRAIN [7][1820/3880]	Time 0.183 (0.164)	Data 1.35e-04 (3.15e-04)	Tok/s 91578 (87268)	Loss/tok 2.9944 (3.0457)	LR 1.250e-04
0: TRAIN [7][1830/3880]	Time 0.244 (0.164)	Data 1.35e-04 (3.14e-04)	Tok/s 97824 (87276)	Loss/tok 3.2155 (3.0459)	LR 1.250e-04
0: TRAIN [7][1840/3880]	Time 0.067 (0.164)	Data 1.30e-04 (3.13e-04)	Tok/s 80299 (87277)	Loss/tok 2.4566 (3.0458)	LR 1.250e-04
0: TRAIN [7][1850/3880]	Time 0.123 (0.164)	Data 1.33e-04 (3.12e-04)	Tok/s 82731 (87276)	Loss/tok 2.9368 (3.0456)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][1860/3880]	Time 0.124 (0.164)	Data 1.48e-04 (3.11e-04)	Tok/s 83272 (87298)	Loss/tok 2.8039 (3.0459)	LR 1.250e-04
0: TRAIN [7][1870/3880]	Time 0.067 (0.164)	Data 1.25e-04 (3.10e-04)	Tok/s 78861 (87297)	Loss/tok 2.5539 (3.0460)	LR 1.250e-04
0: TRAIN [7][1880/3880]	Time 0.124 (0.164)	Data 1.24e-04 (3.09e-04)	Tok/s 84460 (87317)	Loss/tok 2.8951 (3.0470)	LR 1.250e-04
0: TRAIN [7][1890/3880]	Time 0.126 (0.164)	Data 1.31e-04 (3.08e-04)	Tok/s 84277 (87315)	Loss/tok 2.8938 (3.0467)	LR 1.250e-04
0: TRAIN [7][1900/3880]	Time 0.182 (0.164)	Data 1.16e-04 (3.07e-04)	Tok/s 91944 (87316)	Loss/tok 3.0995 (3.0465)	LR 1.250e-04
0: TRAIN [7][1910/3880]	Time 0.126 (0.164)	Data 1.26e-04 (3.06e-04)	Tok/s 82723 (87321)	Loss/tok 2.8419 (3.0464)	LR 1.250e-04
0: TRAIN [7][1920/3880]	Time 0.125 (0.164)	Data 1.45e-04 (3.05e-04)	Tok/s 81274 (87319)	Loss/tok 2.9223 (3.0467)	LR 1.250e-04
0: TRAIN [7][1930/3880]	Time 0.124 (0.164)	Data 1.20e-04 (3.04e-04)	Tok/s 85186 (87320)	Loss/tok 2.8460 (3.0464)	LR 1.250e-04
0: TRAIN [7][1940/3880]	Time 0.123 (0.164)	Data 1.30e-04 (3.03e-04)	Tok/s 83139 (87321)	Loss/tok 2.8639 (3.0465)	LR 1.250e-04
0: TRAIN [7][1950/3880]	Time 0.126 (0.164)	Data 1.31e-04 (3.02e-04)	Tok/s 83052 (87315)	Loss/tok 2.8088 (3.0465)	LR 1.250e-04
0: TRAIN [7][1960/3880]	Time 0.126 (0.164)	Data 1.29e-04 (3.01e-04)	Tok/s 79523 (87327)	Loss/tok 2.8327 (3.0469)	LR 1.250e-04
0: TRAIN [7][1970/3880]	Time 0.183 (0.164)	Data 1.15e-04 (3.00e-04)	Tok/s 92110 (87334)	Loss/tok 3.0989 (3.0469)	LR 1.250e-04
0: TRAIN [7][1980/3880]	Time 0.183 (0.164)	Data 1.40e-04 (3.00e-04)	Tok/s 92050 (87342)	Loss/tok 3.0869 (3.0467)	LR 1.250e-04
0: TRAIN [7][1990/3880]	Time 0.243 (0.164)	Data 1.17e-04 (2.99e-04)	Tok/s 94587 (87349)	Loss/tok 3.3227 (3.0470)	LR 1.250e-04
0: TRAIN [7][2000/3880]	Time 0.124 (0.164)	Data 1.15e-04 (2.98e-04)	Tok/s 83115 (87348)	Loss/tok 2.9725 (3.0466)	LR 1.250e-04
0: TRAIN [7][2010/3880]	Time 0.243 (0.164)	Data 1.37e-04 (2.97e-04)	Tok/s 95850 (87352)	Loss/tok 3.2740 (3.0466)	LR 1.250e-04
0: TRAIN [7][2020/3880]	Time 0.181 (0.164)	Data 1.30e-04 (2.96e-04)	Tok/s 94825 (87367)	Loss/tok 2.9432 (3.0469)	LR 1.250e-04
0: TRAIN [7][2030/3880]	Time 0.127 (0.164)	Data 1.40e-04 (2.95e-04)	Tok/s 80812 (87371)	Loss/tok 2.7724 (3.0468)	LR 1.250e-04
0: TRAIN [7][2040/3880]	Time 0.182 (0.164)	Data 1.14e-04 (2.94e-04)	Tok/s 92863 (87361)	Loss/tok 2.9999 (3.0462)	LR 1.250e-04
0: TRAIN [7][2050/3880]	Time 0.068 (0.164)	Data 1.45e-04 (2.94e-04)	Tok/s 76341 (87357)	Loss/tok 2.5152 (3.0464)	LR 1.250e-04
0: TRAIN [7][2060/3880]	Time 0.125 (0.164)	Data 1.18e-04 (2.93e-04)	Tok/s 81994 (87344)	Loss/tok 2.9391 (3.0461)	LR 1.250e-04
0: TRAIN [7][2070/3880]	Time 0.124 (0.164)	Data 1.39e-04 (2.92e-04)	Tok/s 82151 (87343)	Loss/tok 2.8605 (3.0460)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][2080/3880]	Time 0.127 (0.164)	Data 1.28e-04 (2.91e-04)	Tok/s 79942 (87356)	Loss/tok 2.8063 (3.0464)	LR 1.250e-04
0: TRAIN [7][2090/3880]	Time 0.181 (0.164)	Data 1.29e-04 (2.90e-04)	Tok/s 93532 (87351)	Loss/tok 3.1224 (3.0465)	LR 1.250e-04
0: TRAIN [7][2100/3880]	Time 0.183 (0.164)	Data 1.15e-04 (2.90e-04)	Tok/s 91024 (87349)	Loss/tok 3.0663 (3.0461)	LR 1.250e-04
0: TRAIN [7][2110/3880]	Time 0.125 (0.164)	Data 1.14e-04 (2.89e-04)	Tok/s 83393 (87345)	Loss/tok 2.7961 (3.0459)	LR 1.250e-04
0: TRAIN [7][2120/3880]	Time 0.183 (0.164)	Data 1.28e-04 (2.88e-04)	Tok/s 92001 (87340)	Loss/tok 3.0205 (3.0455)	LR 1.250e-04
0: TRAIN [7][2130/3880]	Time 0.126 (0.164)	Data 1.25e-04 (2.87e-04)	Tok/s 81388 (87337)	Loss/tok 2.8733 (3.0456)	LR 1.250e-04
0: TRAIN [7][2140/3880]	Time 0.125 (0.164)	Data 1.14e-04 (2.86e-04)	Tok/s 83894 (87331)	Loss/tok 2.8504 (3.0452)	LR 1.250e-04
0: TRAIN [7][2150/3880]	Time 0.067 (0.163)	Data 1.33e-04 (2.86e-04)	Tok/s 78132 (87321)	Loss/tok 2.5585 (3.0449)	LR 1.250e-04
0: TRAIN [7][2160/3880]	Time 0.244 (0.163)	Data 1.29e-04 (2.85e-04)	Tok/s 95996 (87317)	Loss/tok 3.2029 (3.0448)	LR 1.250e-04
0: TRAIN [7][2170/3880]	Time 0.243 (0.163)	Data 1.34e-04 (2.84e-04)	Tok/s 95727 (87306)	Loss/tok 3.1691 (3.0448)	LR 1.250e-04
0: TRAIN [7][2180/3880]	Time 0.068 (0.163)	Data 1.31e-04 (2.83e-04)	Tok/s 76788 (87297)	Loss/tok 2.3860 (3.0443)	LR 1.250e-04
0: TRAIN [7][2190/3880]	Time 0.126 (0.163)	Data 1.16e-04 (2.83e-04)	Tok/s 81990 (87283)	Loss/tok 2.8681 (3.0441)	LR 1.250e-04
0: TRAIN [7][2200/3880]	Time 0.186 (0.163)	Data 1.29e-04 (2.82e-04)	Tok/s 91833 (87292)	Loss/tok 3.0023 (3.0441)	LR 1.250e-04
0: TRAIN [7][2210/3880]	Time 0.126 (0.163)	Data 1.20e-04 (2.81e-04)	Tok/s 82744 (87300)	Loss/tok 2.8031 (3.0440)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][2220/3880]	Time 0.125 (0.163)	Data 1.39e-04 (2.81e-04)	Tok/s 82032 (87293)	Loss/tok 2.9058 (3.0439)	LR 1.250e-04
0: TRAIN [7][2230/3880]	Time 0.123 (0.163)	Data 1.18e-04 (2.80e-04)	Tok/s 82155 (87280)	Loss/tok 2.9679 (3.0435)	LR 1.250e-04
0: TRAIN [7][2240/3880]	Time 0.313 (0.163)	Data 1.11e-04 (2.79e-04)	Tok/s 95108 (87276)	Loss/tok 3.3619 (3.0433)	LR 1.250e-04
0: TRAIN [7][2250/3880]	Time 0.181 (0.163)	Data 1.14e-04 (2.79e-04)	Tok/s 93919 (87290)	Loss/tok 3.0337 (3.0440)	LR 1.250e-04
0: TRAIN [7][2260/3880]	Time 0.181 (0.163)	Data 1.30e-04 (2.78e-04)	Tok/s 93112 (87286)	Loss/tok 3.0068 (3.0439)	LR 1.250e-04
0: TRAIN [7][2270/3880]	Time 0.185 (0.163)	Data 1.28e-04 (2.77e-04)	Tok/s 89651 (87282)	Loss/tok 2.9206 (3.0436)	LR 1.250e-04
0: TRAIN [7][2280/3880]	Time 0.126 (0.163)	Data 1.28e-04 (2.76e-04)	Tok/s 81924 (87281)	Loss/tok 2.8764 (3.0435)	LR 1.250e-04
0: TRAIN [7][2290/3880]	Time 0.127 (0.163)	Data 1.30e-04 (2.76e-04)	Tok/s 81838 (87284)	Loss/tok 2.9208 (3.0440)	LR 1.250e-04
0: TRAIN [7][2300/3880]	Time 0.185 (0.163)	Data 1.50e-04 (2.75e-04)	Tok/s 91317 (87292)	Loss/tok 3.0213 (3.0440)	LR 1.250e-04
0: TRAIN [7][2310/3880]	Time 0.067 (0.163)	Data 1.13e-04 (2.74e-04)	Tok/s 78579 (87280)	Loss/tok 2.4762 (3.0437)	LR 1.250e-04
0: TRAIN [7][2320/3880]	Time 0.183 (0.163)	Data 1.37e-04 (2.74e-04)	Tok/s 92077 (87281)	Loss/tok 3.0662 (3.0435)	LR 1.250e-04
0: TRAIN [7][2330/3880]	Time 0.245 (0.163)	Data 1.27e-04 (2.73e-04)	Tok/s 95121 (87282)	Loss/tok 3.2152 (3.0437)	LR 1.250e-04
0: TRAIN [7][2340/3880]	Time 0.124 (0.163)	Data 1.13e-04 (2.73e-04)	Tok/s 82342 (87279)	Loss/tok 2.8315 (3.0434)	LR 1.250e-04
0: TRAIN [7][2350/3880]	Time 0.182 (0.163)	Data 1.19e-04 (2.72e-04)	Tok/s 91175 (87275)	Loss/tok 2.9646 (3.0433)	LR 1.250e-04
0: TRAIN [7][2360/3880]	Time 0.183 (0.163)	Data 1.39e-04 (2.71e-04)	Tok/s 92188 (87270)	Loss/tok 3.0633 (3.0428)	LR 1.250e-04
0: TRAIN [7][2370/3880]	Time 0.315 (0.163)	Data 1.34e-04 (2.71e-04)	Tok/s 94737 (87270)	Loss/tok 3.4311 (3.0430)	LR 1.250e-04
0: TRAIN [7][2380/3880]	Time 0.124 (0.163)	Data 1.44e-04 (2.70e-04)	Tok/s 82540 (87276)	Loss/tok 2.8523 (3.0429)	LR 1.250e-04
0: TRAIN [7][2390/3880]	Time 0.241 (0.163)	Data 1.29e-04 (2.70e-04)	Tok/s 97502 (87276)	Loss/tok 3.2476 (3.0429)	LR 1.250e-04
0: TRAIN [7][2400/3880]	Time 0.067 (0.163)	Data 1.28e-04 (2.69e-04)	Tok/s 79573 (87269)	Loss/tok 2.4047 (3.0428)	LR 1.250e-04
0: TRAIN [7][2410/3880]	Time 0.128 (0.163)	Data 1.35e-04 (2.68e-04)	Tok/s 80450 (87273)	Loss/tok 2.9131 (3.0430)	LR 1.250e-04
0: TRAIN [7][2420/3880]	Time 0.182 (0.163)	Data 1.17e-04 (2.68e-04)	Tok/s 92355 (87271)	Loss/tok 3.0616 (3.0427)	LR 1.250e-04
0: TRAIN [7][2430/3880]	Time 0.125 (0.162)	Data 1.35e-04 (2.67e-04)	Tok/s 82890 (87261)	Loss/tok 2.8547 (3.0424)	LR 1.250e-04
0: TRAIN [7][2440/3880]	Time 0.186 (0.162)	Data 1.39e-04 (2.67e-04)	Tok/s 90284 (87260)	Loss/tok 2.9057 (3.0422)	LR 1.250e-04
0: TRAIN [7][2450/3880]	Time 0.182 (0.162)	Data 1.13e-04 (2.66e-04)	Tok/s 92089 (87252)	Loss/tok 3.0236 (3.0419)	LR 1.250e-04
0: TRAIN [7][2460/3880]	Time 0.124 (0.162)	Data 1.26e-04 (2.65e-04)	Tok/s 83578 (87244)	Loss/tok 2.7913 (3.0417)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [7][2470/3880]	Time 0.181 (0.162)	Data 1.17e-04 (2.65e-04)	Tok/s 92487 (87245)	Loss/tok 2.9838 (3.0416)	LR 1.250e-04
0: TRAIN [7][2480/3880]	Time 0.068 (0.162)	Data 1.28e-04 (2.64e-04)	Tok/s 76783 (87241)	Loss/tok 2.5482 (3.0413)	LR 1.250e-04
0: TRAIN [7][2490/3880]	Time 0.244 (0.162)	Data 1.40e-04 (2.64e-04)	Tok/s 95251 (87252)	Loss/tok 3.2828 (3.0416)	LR 1.250e-04
0: TRAIN [7][2500/3880]	Time 0.314 (0.162)	Data 1.18e-04 (2.63e-04)	Tok/s 94623 (87260)	Loss/tok 3.3672 (3.0420)	LR 1.250e-04
0: TRAIN [7][2510/3880]	Time 0.125 (0.162)	Data 1.20e-04 (2.63e-04)	Tok/s 82589 (87249)	Loss/tok 2.9232 (3.0419)	LR 1.250e-04
0: TRAIN [7][2520/3880]	Time 0.125 (0.162)	Data 1.34e-04 (2.62e-04)	Tok/s 81709 (87248)	Loss/tok 2.8233 (3.0416)	LR 1.250e-04
0: TRAIN [7][2530/3880]	Time 0.067 (0.162)	Data 1.15e-04 (2.62e-04)	Tok/s 76566 (87257)	Loss/tok 2.6022 (3.0417)	LR 1.250e-04
0: TRAIN [7][2540/3880]	Time 0.123 (0.162)	Data 1.26e-04 (2.61e-04)	Tok/s 84568 (87251)	Loss/tok 2.8632 (3.0415)	LR 1.250e-04
0: TRAIN [7][2550/3880]	Time 0.247 (0.163)	Data 1.22e-04 (2.61e-04)	Tok/s 94294 (87264)	Loss/tok 3.2163 (3.0418)	LR 1.250e-04
0: TRAIN [7][2560/3880]	Time 0.181 (0.163)	Data 1.24e-04 (2.60e-04)	Tok/s 91908 (87272)	Loss/tok 2.9817 (3.0421)	LR 1.250e-04
0: TRAIN [7][2570/3880]	Time 0.183 (0.163)	Data 1.33e-04 (2.59e-04)	Tok/s 91797 (87274)	Loss/tok 3.0466 (3.0422)	LR 1.250e-04
0: TRAIN [7][2580/3880]	Time 0.123 (0.163)	Data 1.67e-04 (2.59e-04)	Tok/s 82124 (87270)	Loss/tok 2.8592 (3.0420)	LR 1.250e-04
0: TRAIN [7][2590/3880]	Time 0.242 (0.163)	Data 1.18e-04 (2.58e-04)	Tok/s 96376 (87286)	Loss/tok 3.1630 (3.0421)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [7][2600/3880]	Time 0.248 (0.163)	Data 1.16e-04 (2.58e-04)	Tok/s 93860 (87291)	Loss/tok 3.1455 (3.0423)	LR 1.250e-04
0: TRAIN [7][2610/3880]	Time 0.183 (0.163)	Data 1.35e-04 (2.57e-04)	Tok/s 93040 (87282)	Loss/tok 3.1104 (3.0420)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][2620/3880]	Time 0.244 (0.163)	Data 1.23e-04 (2.57e-04)	Tok/s 96921 (87293)	Loss/tok 3.2174 (3.0422)	LR 1.250e-04
0: TRAIN [7][2630/3880]	Time 0.180 (0.163)	Data 1.17e-04 (2.56e-04)	Tok/s 91975 (87301)	Loss/tok 3.0704 (3.0424)	LR 1.250e-04
0: TRAIN [7][2640/3880]	Time 0.184 (0.163)	Data 1.38e-04 (2.56e-04)	Tok/s 92530 (87310)	Loss/tok 3.0404 (3.0423)	LR 1.250e-04
0: TRAIN [7][2650/3880]	Time 0.067 (0.163)	Data 1.19e-04 (2.55e-04)	Tok/s 78417 (87309)	Loss/tok 2.3848 (3.0423)	LR 1.250e-04
0: TRAIN [7][2660/3880]	Time 0.246 (0.163)	Data 1.37e-04 (2.55e-04)	Tok/s 95104 (87313)	Loss/tok 3.1850 (3.0426)	LR 1.250e-04
0: TRAIN [7][2670/3880]	Time 0.182 (0.163)	Data 1.21e-04 (2.55e-04)	Tok/s 92688 (87329)	Loss/tok 2.9615 (3.0436)	LR 1.250e-04
0: TRAIN [7][2680/3880]	Time 0.124 (0.163)	Data 1.14e-04 (2.54e-04)	Tok/s 84455 (87326)	Loss/tok 2.8951 (3.0434)	LR 1.250e-04
0: TRAIN [7][2690/3880]	Time 0.123 (0.163)	Data 1.24e-04 (2.54e-04)	Tok/s 85475 (87337)	Loss/tok 2.7662 (3.0433)	LR 1.250e-04
0: TRAIN [7][2700/3880]	Time 0.182 (0.163)	Data 1.25e-04 (2.53e-04)	Tok/s 92806 (87340)	Loss/tok 2.9977 (3.0438)	LR 1.250e-04
0: TRAIN [7][2710/3880]	Time 0.185 (0.163)	Data 1.36e-04 (2.53e-04)	Tok/s 90276 (87347)	Loss/tok 3.0925 (3.0441)	LR 1.250e-04
0: TRAIN [7][2720/3880]	Time 0.067 (0.163)	Data 1.31e-04 (2.52e-04)	Tok/s 78486 (87336)	Loss/tok 2.4804 (3.0437)	LR 1.250e-04
0: TRAIN [7][2730/3880]	Time 0.182 (0.163)	Data 1.41e-04 (2.52e-04)	Tok/s 92200 (87344)	Loss/tok 3.0123 (3.0438)	LR 1.250e-04
0: TRAIN [7][2740/3880]	Time 0.244 (0.163)	Data 1.21e-04 (2.51e-04)	Tok/s 96319 (87343)	Loss/tok 3.0778 (3.0436)	LR 1.250e-04
0: TRAIN [7][2750/3880]	Time 0.183 (0.163)	Data 1.24e-04 (2.51e-04)	Tok/s 91744 (87355)	Loss/tok 3.0488 (3.0441)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][2760/3880]	Time 0.316 (0.163)	Data 1.13e-04 (2.50e-04)	Tok/s 93830 (87359)	Loss/tok 3.4022 (3.0444)	LR 1.250e-04
0: TRAIN [7][2770/3880]	Time 0.244 (0.163)	Data 1.11e-04 (2.50e-04)	Tok/s 96881 (87371)	Loss/tok 3.1412 (3.0445)	LR 1.250e-04
0: TRAIN [7][2780/3880]	Time 0.123 (0.163)	Data 1.18e-04 (2.49e-04)	Tok/s 85405 (87385)	Loss/tok 2.7499 (3.0446)	LR 1.250e-04
0: TRAIN [7][2790/3880]	Time 0.067 (0.163)	Data 1.12e-04 (2.49e-04)	Tok/s 78516 (87382)	Loss/tok 2.5067 (3.0447)	LR 1.250e-04
0: TRAIN [7][2800/3880]	Time 0.123 (0.163)	Data 1.11e-04 (2.49e-04)	Tok/s 82664 (87385)	Loss/tok 2.8695 (3.0446)	LR 1.250e-04
0: TRAIN [7][2810/3880]	Time 0.123 (0.163)	Data 1.08e-04 (2.48e-04)	Tok/s 81862 (87379)	Loss/tok 2.8613 (3.0444)	LR 1.250e-04
0: TRAIN [7][2820/3880]	Time 0.124 (0.163)	Data 1.27e-04 (2.48e-04)	Tok/s 82954 (87381)	Loss/tok 2.8891 (3.0442)	LR 1.250e-04
0: TRAIN [7][2830/3880]	Time 0.184 (0.163)	Data 1.24e-04 (2.47e-04)	Tok/s 91387 (87387)	Loss/tok 3.0917 (3.0445)	LR 1.250e-04
0: TRAIN [7][2840/3880]	Time 0.123 (0.163)	Data 1.19e-04 (2.47e-04)	Tok/s 84154 (87396)	Loss/tok 2.8137 (3.0445)	LR 1.250e-04
0: TRAIN [7][2850/3880]	Time 0.126 (0.163)	Data 1.12e-04 (2.46e-04)	Tok/s 80807 (87393)	Loss/tok 2.9086 (3.0444)	LR 1.250e-04
0: TRAIN [7][2860/3880]	Time 0.244 (0.163)	Data 1.09e-04 (2.46e-04)	Tok/s 94759 (87403)	Loss/tok 3.2021 (3.0446)	LR 1.250e-04
0: TRAIN [7][2870/3880]	Time 0.125 (0.163)	Data 1.33e-04 (2.45e-04)	Tok/s 82010 (87399)	Loss/tok 2.8871 (3.0446)	LR 1.250e-04
0: TRAIN [7][2880/3880]	Time 0.241 (0.163)	Data 1.08e-04 (2.45e-04)	Tok/s 95506 (87393)	Loss/tok 3.2923 (3.0446)	LR 1.250e-04
0: TRAIN [7][2890/3880]	Time 0.184 (0.163)	Data 1.16e-04 (2.45e-04)	Tok/s 91650 (87390)	Loss/tok 2.9968 (3.0445)	LR 1.250e-04
0: TRAIN [7][2900/3880]	Time 0.125 (0.163)	Data 1.08e-04 (2.44e-04)	Tok/s 83556 (87394)	Loss/tok 2.9004 (3.0448)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][2910/3880]	Time 0.184 (0.164)	Data 1.28e-04 (2.44e-04)	Tok/s 90096 (87401)	Loss/tok 2.9938 (3.0450)	LR 1.250e-04
0: TRAIN [7][2920/3880]	Time 0.318 (0.164)	Data 1.29e-04 (2.43e-04)	Tok/s 93857 (87403)	Loss/tok 3.3840 (3.0456)	LR 1.250e-04
0: TRAIN [7][2930/3880]	Time 0.066 (0.164)	Data 1.08e-04 (2.43e-04)	Tok/s 79451 (87390)	Loss/tok 2.5483 (3.0452)	LR 1.250e-04
0: TRAIN [7][2940/3880]	Time 0.124 (0.163)	Data 1.25e-04 (2.42e-04)	Tok/s 84537 (87385)	Loss/tok 2.8436 (3.0449)	LR 1.250e-04
0: TRAIN [7][2950/3880]	Time 0.125 (0.163)	Data 1.11e-04 (2.42e-04)	Tok/s 84657 (87386)	Loss/tok 2.8637 (3.0450)	LR 1.250e-04
0: TRAIN [7][2960/3880]	Time 0.069 (0.163)	Data 1.23e-04 (2.42e-04)	Tok/s 77180 (87386)	Loss/tok 2.3846 (3.0450)	LR 1.250e-04
0: TRAIN [7][2970/3880]	Time 0.184 (0.163)	Data 1.10e-04 (2.41e-04)	Tok/s 90201 (87395)	Loss/tok 2.9828 (3.0453)	LR 1.250e-04
0: TRAIN [7][2980/3880]	Time 0.125 (0.163)	Data 1.27e-04 (2.41e-04)	Tok/s 81597 (87393)	Loss/tok 2.9049 (3.0454)	LR 1.250e-04
0: TRAIN [7][2990/3880]	Time 0.244 (0.164)	Data 1.13e-04 (2.40e-04)	Tok/s 96039 (87400)	Loss/tok 3.1773 (3.0455)	LR 1.250e-04
0: TRAIN [7][3000/3880]	Time 0.125 (0.164)	Data 1.13e-04 (2.40e-04)	Tok/s 82463 (87392)	Loss/tok 2.8769 (3.0453)	LR 1.250e-04
0: TRAIN [7][3010/3880]	Time 0.242 (0.164)	Data 1.26e-04 (2.40e-04)	Tok/s 96236 (87401)	Loss/tok 3.2188 (3.0453)	LR 1.250e-04
0: TRAIN [7][3020/3880]	Time 0.125 (0.164)	Data 1.41e-04 (2.39e-04)	Tok/s 83356 (87396)	Loss/tok 2.8745 (3.0452)	LR 1.250e-04
0: TRAIN [7][3030/3880]	Time 0.126 (0.163)	Data 1.37e-04 (2.39e-04)	Tok/s 81371 (87387)	Loss/tok 2.9314 (3.0449)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][3040/3880]	Time 0.126 (0.163)	Data 1.33e-04 (2.39e-04)	Tok/s 80633 (87386)	Loss/tok 2.7739 (3.0451)	LR 1.250e-04
0: TRAIN [7][3050/3880]	Time 0.243 (0.163)	Data 1.13e-04 (2.38e-04)	Tok/s 95006 (87388)	Loss/tok 3.2333 (3.0451)	LR 1.250e-04
0: TRAIN [7][3060/3880]	Time 0.244 (0.163)	Data 1.11e-04 (2.38e-04)	Tok/s 95870 (87388)	Loss/tok 3.3563 (3.0451)	LR 1.250e-04
0: TRAIN [7][3070/3880]	Time 0.184 (0.163)	Data 1.15e-04 (2.37e-04)	Tok/s 90618 (87379)	Loss/tok 3.0065 (3.0449)	LR 1.250e-04
0: TRAIN [7][3080/3880]	Time 0.183 (0.163)	Data 1.22e-04 (2.37e-04)	Tok/s 91872 (87384)	Loss/tok 3.0649 (3.0451)	LR 1.250e-04
0: TRAIN [7][3090/3880]	Time 0.124 (0.163)	Data 1.13e-04 (2.37e-04)	Tok/s 81511 (87370)	Loss/tok 2.9308 (3.0449)	LR 1.250e-04
0: TRAIN [7][3100/3880]	Time 0.067 (0.163)	Data 1.29e-04 (2.36e-04)	Tok/s 77325 (87360)	Loss/tok 2.5778 (3.0445)	LR 1.250e-04
0: TRAIN [7][3110/3880]	Time 0.123 (0.163)	Data 1.09e-04 (2.36e-04)	Tok/s 82161 (87356)	Loss/tok 2.8309 (3.0443)	LR 1.250e-04
0: TRAIN [7][3120/3880]	Time 0.126 (0.163)	Data 1.11e-04 (2.36e-04)	Tok/s 81080 (87357)	Loss/tok 2.9312 (3.0443)	LR 1.250e-04
0: TRAIN [7][3130/3880]	Time 0.126 (0.163)	Data 1.30e-04 (2.35e-04)	Tok/s 83257 (87359)	Loss/tok 2.7986 (3.0442)	LR 1.250e-04
0: TRAIN [7][3140/3880]	Time 0.126 (0.163)	Data 1.35e-04 (2.35e-04)	Tok/s 82065 (87350)	Loss/tok 2.8975 (3.0440)	LR 1.250e-04
0: TRAIN [7][3150/3880]	Time 0.126 (0.163)	Data 1.40e-04 (2.34e-04)	Tok/s 81873 (87351)	Loss/tok 2.8185 (3.0440)	LR 1.250e-04
0: TRAIN [7][3160/3880]	Time 0.183 (0.163)	Data 1.18e-04 (2.34e-04)	Tok/s 91066 (87353)	Loss/tok 3.0214 (3.0441)	LR 1.250e-04
0: TRAIN [7][3170/3880]	Time 0.244 (0.163)	Data 1.35e-04 (2.34e-04)	Tok/s 96942 (87360)	Loss/tok 3.2032 (3.0443)	LR 1.250e-04
0: TRAIN [7][3180/3880]	Time 0.069 (0.163)	Data 1.37e-04 (2.33e-04)	Tok/s 76481 (87366)	Loss/tok 2.5177 (3.0444)	LR 1.250e-04
0: TRAIN [7][3190/3880]	Time 0.125 (0.163)	Data 1.34e-04 (2.33e-04)	Tok/s 83157 (87366)	Loss/tok 2.8260 (3.0444)	LR 1.250e-04
0: TRAIN [7][3200/3880]	Time 0.186 (0.163)	Data 1.31e-04 (2.33e-04)	Tok/s 90069 (87372)	Loss/tok 3.0158 (3.0444)	LR 1.250e-04
0: TRAIN [7][3210/3880]	Time 0.126 (0.163)	Data 1.29e-04 (2.32e-04)	Tok/s 81028 (87373)	Loss/tok 2.8475 (3.0443)	LR 1.250e-04
0: TRAIN [7][3220/3880]	Time 0.244 (0.163)	Data 1.34e-04 (2.32e-04)	Tok/s 95131 (87376)	Loss/tok 3.2775 (3.0444)	LR 1.250e-04
0: TRAIN [7][3230/3880]	Time 0.126 (0.163)	Data 1.29e-04 (2.32e-04)	Tok/s 82640 (87373)	Loss/tok 2.8914 (3.0445)	LR 1.250e-04
0: TRAIN [7][3240/3880]	Time 0.124 (0.163)	Data 1.21e-04 (2.32e-04)	Tok/s 82420 (87371)	Loss/tok 2.9580 (3.0444)	LR 1.250e-04
0: TRAIN [7][3250/3880]	Time 0.068 (0.163)	Data 1.43e-04 (2.31e-04)	Tok/s 77446 (87365)	Loss/tok 2.4457 (3.0443)	LR 1.250e-04
0: TRAIN [7][3260/3880]	Time 0.127 (0.163)	Data 1.24e-04 (2.31e-04)	Tok/s 82312 (87368)	Loss/tok 2.8037 (3.0444)	LR 1.250e-04
0: TRAIN [7][3270/3880]	Time 0.184 (0.163)	Data 1.27e-04 (2.31e-04)	Tok/s 91863 (87371)	Loss/tok 3.0069 (3.0444)	LR 1.250e-04
0: TRAIN [7][3280/3880]	Time 0.185 (0.163)	Data 1.11e-04 (2.30e-04)	Tok/s 90595 (87362)	Loss/tok 3.1220 (3.0441)	LR 1.250e-04
0: TRAIN [7][3290/3880]	Time 0.069 (0.163)	Data 1.58e-04 (2.30e-04)	Tok/s 77478 (87356)	Loss/tok 2.4745 (3.0442)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
0: TRAIN [7][3300/3880]	Time 0.182 (0.163)	Data 1.31e-04 (2.30e-04)	Tok/s 92244 (87351)	Loss/tok 3.0459 (3.0441)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][3310/3880]	Time 0.125 (0.163)	Data 1.24e-04 (2.29e-04)	Tok/s 81046 (87348)	Loss/tok 2.8792 (3.0440)	LR 1.250e-04
0: TRAIN [7][3320/3880]	Time 0.123 (0.163)	Data 1.36e-04 (2.29e-04)	Tok/s 83203 (87342)	Loss/tok 2.8867 (3.0440)	LR 1.250e-04
0: TRAIN [7][3330/3880]	Time 0.124 (0.163)	Data 1.25e-04 (2.29e-04)	Tok/s 83416 (87346)	Loss/tok 2.9353 (3.0441)	LR 1.250e-04
0: TRAIN [7][3340/3880]	Time 0.184 (0.163)	Data 1.49e-04 (2.28e-04)	Tok/s 90741 (87346)	Loss/tok 3.0855 (3.0441)	LR 1.250e-04
0: TRAIN [7][3350/3880]	Time 0.184 (0.163)	Data 1.24e-04 (2.28e-04)	Tok/s 92858 (87349)	Loss/tok 2.9317 (3.0444)	LR 1.250e-04
0: TRAIN [7][3360/3880]	Time 0.183 (0.163)	Data 1.42e-04 (2.28e-04)	Tok/s 91205 (87350)	Loss/tok 3.1174 (3.0446)	LR 1.250e-04
0: TRAIN [7][3370/3880]	Time 0.183 (0.163)	Data 1.23e-04 (2.28e-04)	Tok/s 91520 (87363)	Loss/tok 3.0083 (3.0449)	LR 1.250e-04
0: TRAIN [7][3380/3880]	Time 0.184 (0.163)	Data 1.37e-04 (2.27e-04)	Tok/s 90751 (87364)	Loss/tok 2.9621 (3.0447)	LR 1.250e-04
0: TRAIN [7][3390/3880]	Time 0.124 (0.163)	Data 1.41e-04 (2.27e-04)	Tok/s 81229 (87365)	Loss/tok 2.7478 (3.0446)	LR 1.250e-04
0: TRAIN [7][3400/3880]	Time 0.125 (0.163)	Data 1.42e-04 (2.27e-04)	Tok/s 83271 (87370)	Loss/tok 2.8059 (3.0446)	LR 1.250e-04
0: TRAIN [7][3410/3880]	Time 0.241 (0.163)	Data 1.55e-04 (2.26e-04)	Tok/s 97588 (87373)	Loss/tok 3.1846 (3.0447)	LR 1.250e-04
0: TRAIN [7][3420/3880]	Time 0.181 (0.163)	Data 1.48e-04 (2.26e-04)	Tok/s 93157 (87376)	Loss/tok 3.0615 (3.0446)	LR 1.250e-04
0: TRAIN [7][3430/3880]	Time 0.123 (0.163)	Data 1.78e-04 (2.26e-04)	Tok/s 85423 (87372)	Loss/tok 2.8247 (3.0445)	LR 1.250e-04
0: TRAIN [7][3440/3880]	Time 0.126 (0.163)	Data 1.19e-04 (2.26e-04)	Tok/s 82673 (87372)	Loss/tok 2.7871 (3.0445)	LR 1.250e-04
0: TRAIN [7][3450/3880]	Time 0.126 (0.163)	Data 1.27e-04 (2.25e-04)	Tok/s 82225 (87374)	Loss/tok 2.7845 (3.0444)	LR 1.250e-04
0: TRAIN [7][3460/3880]	Time 0.182 (0.163)	Data 1.30e-04 (2.25e-04)	Tok/s 92866 (87379)	Loss/tok 3.0244 (3.0446)	LR 1.250e-04
0: TRAIN [7][3470/3880]	Time 0.125 (0.163)	Data 1.29e-04 (2.25e-04)	Tok/s 81500 (87377)	Loss/tok 2.7471 (3.0445)	LR 1.250e-04
0: TRAIN [7][3480/3880]	Time 0.125 (0.163)	Data 1.21e-04 (2.25e-04)	Tok/s 82106 (87378)	Loss/tok 2.9242 (3.0448)	LR 1.250e-04
0: TRAIN [7][3490/3880]	Time 0.125 (0.163)	Data 1.38e-04 (2.24e-04)	Tok/s 83374 (87381)	Loss/tok 2.8433 (3.0447)	LR 1.250e-04
0: TRAIN [7][3500/3880]	Time 0.186 (0.164)	Data 1.42e-04 (2.24e-04)	Tok/s 91352 (87385)	Loss/tok 2.9948 (3.0446)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][3510/3880]	Time 0.068 (0.164)	Data 1.39e-04 (2.24e-04)	Tok/s 78307 (87389)	Loss/tok 2.5999 (3.0447)	LR 1.250e-04
0: TRAIN [7][3520/3880]	Time 0.125 (0.164)	Data 1.30e-04 (2.23e-04)	Tok/s 82835 (87388)	Loss/tok 2.9688 (3.0449)	LR 1.250e-04
0: TRAIN [7][3530/3880]	Time 0.247 (0.164)	Data 1.22e-04 (2.23e-04)	Tok/s 92389 (87387)	Loss/tok 3.2955 (3.0448)	LR 1.250e-04
0: TRAIN [7][3540/3880]	Time 0.317 (0.164)	Data 1.37e-04 (2.23e-04)	Tok/s 94234 (87394)	Loss/tok 3.4090 (3.0449)	LR 1.250e-04
0: TRAIN [7][3550/3880]	Time 0.124 (0.164)	Data 1.45e-04 (2.23e-04)	Tok/s 84671 (87391)	Loss/tok 2.9465 (3.0448)	LR 1.250e-04
0: TRAIN [7][3560/3880]	Time 0.068 (0.164)	Data 1.40e-04 (2.22e-04)	Tok/s 78409 (87393)	Loss/tok 2.5125 (3.0449)	LR 1.250e-04
0: TRAIN [7][3570/3880]	Time 0.184 (0.164)	Data 1.31e-04 (2.22e-04)	Tok/s 92025 (87394)	Loss/tok 3.0376 (3.0448)	LR 1.250e-04
0: TRAIN [7][3580/3880]	Time 0.067 (0.163)	Data 1.17e-04 (2.22e-04)	Tok/s 78357 (87387)	Loss/tok 2.5076 (3.0445)	LR 1.250e-04
0: TRAIN [7][3590/3880]	Time 0.124 (0.163)	Data 1.51e-04 (2.22e-04)	Tok/s 82937 (87381)	Loss/tok 2.8654 (3.0443)	LR 1.250e-04
0: TRAIN [7][3600/3880]	Time 0.246 (0.163)	Data 1.39e-04 (2.21e-04)	Tok/s 95248 (87384)	Loss/tok 3.2562 (3.0443)	LR 1.250e-04
0: TRAIN [7][3610/3880]	Time 0.123 (0.163)	Data 1.26e-04 (2.21e-04)	Tok/s 83328 (87381)	Loss/tok 2.7738 (3.0442)	LR 1.250e-04
0: TRAIN [7][3620/3880]	Time 0.244 (0.163)	Data 1.18e-04 (2.21e-04)	Tok/s 95450 (87388)	Loss/tok 3.1177 (3.0445)	LR 1.250e-04
0: TRAIN [7][3630/3880]	Time 0.183 (0.163)	Data 1.30e-04 (2.21e-04)	Tok/s 90745 (87386)	Loss/tok 3.0339 (3.0444)	LR 1.250e-04
0: TRAIN [7][3640/3880]	Time 0.182 (0.163)	Data 1.28e-04 (2.20e-04)	Tok/s 92836 (87395)	Loss/tok 3.0511 (3.0444)	LR 1.250e-04
0: TRAIN [7][3650/3880]	Time 0.183 (0.164)	Data 1.48e-04 (2.20e-04)	Tok/s 91778 (87406)	Loss/tok 3.0826 (3.0446)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [7][3660/3880]	Time 0.245 (0.164)	Data 1.45e-04 (2.20e-04)	Tok/s 95851 (87406)	Loss/tok 3.1733 (3.0447)	LR 1.250e-04
0: TRAIN [7][3670/3880]	Time 0.124 (0.164)	Data 1.39e-04 (2.20e-04)	Tok/s 84992 (87411)	Loss/tok 2.8453 (3.0448)	LR 1.250e-04
0: TRAIN [7][3680/3880]	Time 0.067 (0.163)	Data 1.35e-04 (2.19e-04)	Tok/s 78998 (87398)	Loss/tok 2.4700 (3.0444)	LR 1.250e-04
0: TRAIN [7][3690/3880]	Time 0.182 (0.164)	Data 1.52e-04 (2.19e-04)	Tok/s 91455 (87408)	Loss/tok 3.1602 (3.0450)	LR 1.250e-04
0: TRAIN [7][3700/3880]	Time 0.125 (0.163)	Data 1.33e-04 (2.19e-04)	Tok/s 82537 (87403)	Loss/tok 2.8457 (3.0448)	LR 1.250e-04
0: TRAIN [7][3710/3880]	Time 0.184 (0.163)	Data 1.30e-04 (2.19e-04)	Tok/s 90347 (87402)	Loss/tok 3.1027 (3.0447)	LR 1.250e-04
0: TRAIN [7][3720/3880]	Time 0.125 (0.163)	Data 1.40e-04 (2.19e-04)	Tok/s 82211 (87394)	Loss/tok 2.8816 (3.0447)	LR 1.250e-04
0: TRAIN [7][3730/3880]	Time 0.186 (0.163)	Data 1.42e-04 (2.18e-04)	Tok/s 90799 (87396)	Loss/tok 3.1184 (3.0448)	LR 1.250e-04
0: TRAIN [7][3740/3880]	Time 0.124 (0.163)	Data 1.35e-04 (2.18e-04)	Tok/s 83142 (87394)	Loss/tok 2.9902 (3.0447)	LR 1.250e-04
0: TRAIN [7][3750/3880]	Time 0.125 (0.164)	Data 1.48e-04 (2.18e-04)	Tok/s 82459 (87405)	Loss/tok 2.8385 (3.0449)	LR 1.250e-04
0: TRAIN [7][3760/3880]	Time 0.182 (0.163)	Data 1.36e-04 (2.18e-04)	Tok/s 91937 (87401)	Loss/tok 3.1215 (3.0448)	LR 1.250e-04
0: TRAIN [7][3770/3880]	Time 0.123 (0.164)	Data 1.36e-04 (2.17e-04)	Tok/s 84833 (87406)	Loss/tok 2.8370 (3.0449)	LR 1.250e-04
0: TRAIN [7][3780/3880]	Time 0.313 (0.164)	Data 1.16e-04 (2.17e-04)	Tok/s 94847 (87412)	Loss/tok 3.3868 (3.0451)	LR 1.250e-04
0: TRAIN [7][3790/3880]	Time 0.183 (0.164)	Data 1.20e-04 (2.17e-04)	Tok/s 92302 (87409)	Loss/tok 3.1671 (3.0451)	LR 1.250e-04
0: TRAIN [7][3800/3880]	Time 0.125 (0.164)	Data 1.35e-04 (2.17e-04)	Tok/s 83051 (87408)	Loss/tok 2.9009 (3.0452)	LR 1.250e-04
0: TRAIN [7][3810/3880]	Time 0.066 (0.164)	Data 1.23e-04 (2.16e-04)	Tok/s 81114 (87400)	Loss/tok 2.4503 (3.0451)	LR 1.250e-04
0: TRAIN [7][3820/3880]	Time 0.182 (0.164)	Data 1.27e-04 (2.16e-04)	Tok/s 92565 (87405)	Loss/tok 3.0291 (3.0452)	LR 1.250e-04
0: TRAIN [7][3830/3880]	Time 0.187 (0.164)	Data 1.33e-04 (2.16e-04)	Tok/s 89434 (87404)	Loss/tok 3.0905 (3.0452)	LR 1.250e-04
0: TRAIN [7][3840/3880]	Time 0.183 (0.164)	Data 1.32e-04 (2.16e-04)	Tok/s 90948 (87405)	Loss/tok 3.0495 (3.0450)	LR 1.250e-04
0: TRAIN [7][3850/3880]	Time 0.125 (0.163)	Data 1.22e-04 (2.16e-04)	Tok/s 82677 (87396)	Loss/tok 2.8372 (3.0450)	LR 1.250e-04
0: TRAIN [7][3860/3880]	Time 0.181 (0.164)	Data 1.55e-04 (2.15e-04)	Tok/s 93190 (87408)	Loss/tok 2.9407 (3.0453)	LR 1.250e-04
0: TRAIN [7][3870/3880]	Time 0.125 (0.164)	Data 1.32e-04 (2.15e-04)	Tok/s 82353 (87401)	Loss/tok 2.8364 (3.0450)	LR 1.250e-04
:::MLL 1586328409.147 epoch_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 524}}
:::MLL 1586328409.148 eval_start: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 529}}
0: Running evaluation on test set
0: TEST [7][0/6]	Time 0.728 (0.728)	Decoder iters 149.0 (149.0)	Tok/s 22470 (22470)
0: Running moses detokenizer
0: BLEU(score=23.978205139941505, counts=[37114, 18559, 10549, 6259], totals=[65498, 62495, 59492, 56495], precisions=[56.664325628263455, 29.696775742059366, 17.731795871713842, 11.078856535976636], bp=1.0, sys_len=65498, ref_len=64676)
0: Finished evaluation on test set
:::MLL 1586328412.163 eval_accuracy: {"value": 23.98, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 535}}
:::MLL 1586328412.164 eval_stop: {"value": null, "metadata": {"epoch_num": 8, "file": "train.py", "lineno": 538}}
0: Summary: Epoch: 7	Training Loss: 3.0443	Test BLEU: 23.98
0: Performance: Epoch: 7	Training: 349560 Tok/s
0: Finished epoch 7
:::MLL 1586328412.164 block_stop: {"value": null, "metadata": {"first_epoch_num": 8, "file": "train.py", "lineno": 557}}
0: Closing preprocessed data file
:::MLL 1586328412.165 run_stop: {"value": null, "metadata": {"status": "aborted", "file": "train.py", "lineno": 568}}
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-04-08 06:46:55 AM
RESULT,RNN_TRANSLATOR,,5118,nvidia,2020-04-08 05:21:37 AM
